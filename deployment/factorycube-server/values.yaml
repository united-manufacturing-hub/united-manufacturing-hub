# Default values for factorycube-server.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

### factoryinsight ###
factoryinsight:
  enabled: true
  image:
    repository: unitedmanufacturinghub/factoryinsight
    pullPolicy: IfNotPresent
  # Only specify tag if you want to use a specific version. If not specified the latest stable version is automatically selected
  # tag: 0.3.2
  replicas: 2
  user: "factoryinsight"
  # Password will be generated automatically
  db_host: "factorycube-server"
  db_database: "factoryinsight"
  db_user: "factoryinsight"
  db_password: "changeme"
  redis:
    URI1: factorycube-server-redis-node-0.factorycube-server-redis-headless:26379
    URI2: factorycube-server-redis-node-1.factorycube-server-redis-headless:26379
    URI3: factorycube-server-redis-node-2.factorycube-server-redis-headless:26379
  service:
    annotations: {}
  pdb:
    enabled: true
    annotations: {}
    minAvailable: 2
  hpa:
    enabled: true
    minReplicas: 2
    maxReplicas: 5
  ingress:
    enabled: false
    publicHost: ""
    publicHostSecretName: ""
  resources:
    limits:
       cpu: 1000m
    requests:
       cpu: 200m

### factoryinput ###
factoryinput:
  user: factoryinsight
  enabled: true
  service:
    annotations: {}
  image:
    repository: unitedmanufacturinghub/factoryinput
    pullPolicy: IfNotPresent
  replicas: 1
  storageRequest: 1Gi
  pdb:
    enabled: true
    minAvailable: 1

### mqtt-to-blob ###
mqtttoblob:
  # not enabeld by default
  enabled: true
  image:
    repository: unitedmanufacturinghub/mqtt-to-blob
    pullPolicy: Always
  replicas: 1
  pdb:
    enabled: true
    minAvailable: 1

### grafanaproxy ###
grafanaproxy:
  enabled: true
  image:
    repository: unitedmanufacturinghub/grafana-proxy
    pullPolicy: IfNotPresent
  replicas: 1
  service:
    annotations: {}
    type: LoadBalancer
    port: 2096
    targetPort: 80
    protocol: TCP
    labels: {}
    portName: service
    name: http
  resources:
    limits:
       cpu: 1000m
    requests:
       cpu: 200m

### mqtt-to-postresql ###
mqtttopostgresql:
  enabled: true
  image:
    repository: unitedmanufacturinghub/mqtt-to-postgresql
    pullPolicy: IfNotPresent
  # Only specify tag if you want to use a specific version. If not specified the latest stable version is automatically selected
  # tag: 0.3.2
  replicas: 1
  storageRequest: 1Gi
  pdb:
    enabled: true
    minAvailable: 1

minio-operator:
    enabled: false
    loadBalancerEnabled: true
    service:
        annotations: {}
    ## MinIO Tenant Definition
    tenants:
      # Tenant name
      - name: umhminio
        ## Registry location and Tag to download MinIO Server image
        image:
          repository: minio/minio
          tag: RELEASE.2021-06-17T00-10-46Z
          pullPolicy: IfNotPresent
        ## Customize namespace for tenant deployment
        namespace: default
        ## Customize any private registry image pull secret.
        ## currently only one secret registry is supported
        imagePullSecret: {}
        ## If a scheduler is specified here, Tenant pods will be dispatched by specified scheduler.
        ## If not specified, the Tenant pods will be dispatched by default scheduler.
        scheduler: {}
        ## Specification for MinIO Pool(s) in this Tenant.
        pools:
          ## Servers specifies the number of MinIO Tenant Pods / Servers in this pool.
          ## For standalone mode, supply 1. For distributed mode, supply 4 or more.
          ## Note that the operator does not support upgrading from standalone to distributed mode.
          - servers: 1
            ## volumesPerServer specifies the number of volumes attached per MinIO Tenant Pod / Server.
            volumesPerServer: 4
            ## size specifies the capacity per volume
            size: 10Gi
            ## storageClass specifies the storage class name to be used for this pool
            storageClassName: standard
            ## Used to specify a toleration for a pod
            tolerations: {}
            ## nodeSelector parameters for MinIO Pods. It specifies a map of key-value pairs. For the pod to be
            ## eligible to run on a node, the node must have each of the
            ## indicated key-value pairs as labels.
            ## Read more here: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
            nodeSelector: {}
            ## Affinity settings for MinIO pods. Read more about affinity
            ## here: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity.
            affinity: {}
            ## Configure resource requests and limits for MinIO containers
            resources: {}
            ## Configure security context
            securityContext: {}
        ## Mount path where PV will be mounted inside container(s).
        mountPath: /export
        ## Sub path inside Mount path where MinIO stores data.
        subPath: /data
        # pool secrets
        secrets:
          enabled: true
          name: umhminio-secret
          accessKey: minio
          secretKey: minio123
        # pool metrics to be read by Prometheus
        metrics:
          enabled: false
          port: 9000
        certificate:
          ## Use this field to provide one or more external CA certificates. This is used by MinIO
          ## to verify TLS connections with other applications:
          ## https://github.com/minio/minio/tree/master/docs/tls/kubernetes#2-create-kubernetes-secret
          externalCaCertSecret: {}
          ## Use this field to provide a list of Secrets with external certificates. This can be used to to configure
          ## TLS for MinIO Tenant pods. Create secrets as explained here:
          ## https://github.com/minio/minio/tree/master/docs/tls/kubernetes#2-create-kubernetes-secret
          externalCertSecret: {}
          ## Enable automatic Kubernetes based certificate generation and signing as explained in
          ## https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster
          requestAutoCert: true
          ## This field is used only when "requestAutoCert" is set to true. Use this field to set CommonName
          ## for the auto-generated certificate. Internal DNS name for the pod will be used if CommonName is
          ## not provided. DNS name format is *.minio.default.svc.cluster.local
          certConfig: {}
        ## Enable S3 specific features such as Bucket DNS which would allow `buckets` to be
        ## accessible as DNS entries of form `<bucketname>.minio.default.svc.cluster.local`
        s3:
          ## This feature is turned off by default
          bucketDNS: false
        ## PodManagement policy for MinIO Tenant Pods. Can be "OrderedReady" or "Parallel"
        ## Refer https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy
        ## for details.
        podManagementPolicy: Parallel
        ## serviceMetadata allows passing additional labels and annotations to MinIO and Console specific
        ## services created by the operator.
        serviceMetadata: {}
        ## Add environment variables to be set in MinIO container (https://github.com/minio/minio/tree/master/docs/config)
        env: {}
        ## PriorityClassName indicates the Pod priority and hence importance of a Pod relative to other Pods.
        ## This is applied to MinIO pods only.
        ## Refer Kubernetes documentation for details https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass/
        priorityClassName : ""
        ## Define configuration for Console (Graphical user interface for MinIO)
        ## Refer https://github.com/minio/console
        console:
          image:
            repository: minio/console
            tag: v0.7.5
            pullPolicy: IfNotPresent
          replicaCount: 1
          secrets:
            enabled: true
            name: console-secret
            passphrase: SECRET
            salt: THISISAVERYBADSALT
            accessKey: YOURCONSOLEACCESS
            secretKey: YOURCONSOLESECRET
          ## Used to specify a toleration for Tenant Console pods.
          tolerations: []
          ## nodeSelector parameters for Tenant Console Pods. It specifies a map of key-value pairs. For the pod to be
          ## eligible to run on a node, the node must have each of the
          ## indicated key-value pairs as labels.
          ## Read more here: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
          nodeSelector: {}
          ## Affinity settings for Tenant Console Pods. Read more about affinity
          ## here: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity.
          affinity: {}
          ## Configure resource requests and limits for Tenant Console containers
          resources: {}
          ## Configure security context
          securityContext: {}
          ## Add environment variables to be set in Tenant Console container (https://github.com/minio/minio/tree/master/docs/config)
          env: {}
          ## Use this field to provide one or more external CA certificates. This is used by Console
          ## to verify TLS connections with other applications:
          ## https://github.com/minio/minio/tree/master/docs/tls/kubernetes#2-create-kubernetes-secret
          externalCaCertSecret: {}
          ## Use this field to provide a list of Secrets with external certificates. This can be used to to configure
          ## TLS for MinIO Console pods. Create secrets as explained here:
          ## https://github.com/minio/minio/tree/master/docs/tls/kubernetes#2-create-kubernetes-secret
          externalCertSecret: {}

### timescaleDB ###
timescaledb-single:
  enabled: true
  # This file and its contents are licensed under the Apache License 2.0.
  # Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

  replicaCount: 1

  # To prevent very long names, we override the name, otherwise it would default to
  # timescaledb-single (the name of the chart)
  nameOverride: timescaledb

  # The default Patroni name of the cluster ("scope") is derived from the name of the release,
  # but you can override this behaviour here
  # https://patroni.readthedocs.io/en/latest/SETTINGS.html#global-universal
  clusterName:

  # The major PostgreSQL version to use, defaults to the default version of the Docker image
  # However, in pg_upgrade scenarios, you may need to specify an explicit version
  version:

  image:
    # Image was built from
    # https://github.com/timescale/timescaledb-docker-ha
    repository: timescaledev/timescaledb-ha
    tag: pg12-ts2.0-latest
    pullPolicy: IfNotPresent

  # These secrets should exist before the Helm is used to deploy this TimescaleDB.
  # You can use generate_kustomization.sh to help in creating these secrets, or have
  # a look at kustomize/example to see how you could install them.
  secretNames:
    # This secret should contain environment variables that influence Patroni,
    # for example PATRONI_SUPERUSER_PASSWORD or PATRONI_REPLICATION_PASSWORD
    # https://patroni.readthedocs.io/en/latest/ENVIRONMENT.html#postgresql
    credentials:  # defaults to RELEASE-credentials

    # This secret should be a Secret of type kubernetes.io/tls, containing
    # both a tls.key and a tls.crt
    certificate:  # defaults to RELEASE-certificate

    # This secret should contain environment variables that influence pgBackRest,
    # for example, PGBACKREST_REPO1_S3_KEY or PGBACKREST_REPO1_S3_KEY_SECRET
    pgbackrest:  # defaults to RELEASE-pgbackrest

  backup:
    enabled: false
    pgBackRest:
      # https://pgbackrest.org/configuration.html
      # Although not impossible, care should be taken not to include secrets
      # in these parameters. Use Kubernetes Secrets to specify S3 Keys, Secrets etc.
      compress-type: lz4
      process-max: 4
      start-fast: "y"
      repo1-retention-diff: 2
      repo1-retention-full: 2
      repo1-type: s3
      repo1-cipher-type: "none"
      repo1-s3-region: us-east-2
      repo1-s3-endpoint: s3.amazonaws.com

    # Overriding the archive-push/archive-get sections is most useful in
    # very high througput situations. Look at values/high_throuhgput_example.yaml for more details
    pgBackRest:archive-push: {}
    pgBackRest:archive-get: {}
    jobs:
        # name: needs to adhere to the kubernetes restrictions
        # type: can be full, incr or diff, see https://pgbackrest.org/user-guide.html
        # schedule: https://en.wikipedia.org/wiki/Cron#CRON_expression
      - name: full-weekly
        type: full
        schedule: "12 02 * * 0"
      - name: incremental-daily
        type: incr
        schedule: "12 02 * * 1-6"
    # Extra custom environment variables for the backup container.
    envFrom:
    # - secretRef:
    #     name: extra-pgbackrest-secrets

    # Alternatively, you can expose individual environment variables:
    # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#envvar-v1-core
    # Although not impossible, care should be taken not to include secrets
    # in these parameters. Use Kubernetes Secrets to specify S3 Keys, Secrets etc.
    env:
    # - name: PGBACKREST_REPO1_S3_BUCKET
    #   value: my_example_s3_bucket_for_backups
    # - name: PGBACKREST_REPO1_S3_KEY_SECRET
    #   valueFrom:
    #     secretKeyRef:
    #       name: pgbackrest-dev-secrets
    #       key: repo1-s3-key-secret

  # When creating a *new* deployment, the default is to initialize (using initdb) the database.
  # If however, you want to initialize the database using an existing backup, you can do so by
  # configuring this section.
  #
  # WARNING: You *should not* run 2 identically named deployments in separate Kubernetes
  #          clusters using the same S3 bucket for backups.
  bootstrapFromBackup:
    enabled: false
    # Setting the s3 path is mandatory to avoid overwriting an already existing backup,
    # and to be sure the restore is explicitly the one requested.
    repo1-path:
    # Here you can (optionally) provide a Secret to configure the restore process further.
    # For example, if you need to specify a different restore bucket, you should set
    # PGBACKREST_REPO1_S3_BUCKET: <base64 encoded value of the bucket> in these secrets
    secretName: pgbackrest-bootstrap


  # Extra custom environment variables.
  # These should be an EnvVar, as this allows you to inject secrets into the environment
  # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#envvar-v1-core
  env:
  #  - name: NOT_A_SECRET
  #    value: "test"
  #  - name: MYAPPLICATION_STANDBY_PASSWORDS
  #    valueFrom:
  #      secretKeyRef:
  #        name: myapplication-passwords
  #        key: standby

  # Externally created Kubernetes secrets will be injected into the pods by referencing them here. You
  # can also add more configuration options and secrets this way (see https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#configure-all-key-value-pairs-in-a-configmap-as-container-environment-variables)
  envFrom:
  #  - configMapRef:
  #      name: my-deployment-settings
  #      optional: true


  # This configuration will be passed on to Patroni directly, there are a few things that are
  # injected/changed, these are:
  #   - archive_command will be set to /bin/true if backup is disabled
  #   - any context sensitive parameter (scope, namespace, name) will be overridden by the Kubernetes context
  # https://patroni.readthedocs.io/en/latest/SETTINGS.html#settings
  patroni:
    log:
      level: WARNING
    # https://patroni.readthedocs.io/en/latest/replica_bootstrap.html#bootstrap
    bootstrap:
      method: restore_or_initdb
      restore_or_initdb:
        command: >
          /etc/timescaledb/scripts/restore_or_initdb.sh
          --encoding=UTF8
          --locale=C.UTF-8
        keep_existing_recovery_conf: true
      post_init: /etc/timescaledb/scripts/post_init.sh
      dcs:
        loop_wait: 10
        maximum_lag_on_failover: 33554432
        postgresql:
          parameters:
            archive_command: "/etc/timescaledb/scripts/pgbackrest_archive.sh %p"
            archive_mode: 'on'
            archive_timeout: 1800s
            #
            # Autovacuuming is very important to PostgreSQL. For TimescaleDB, in
            # most usecases the vacuuming part is of less importance (there are no deleted tuples to prune)
            # however, the autoanalyze bit (updating the statistics of the chunks) is important to help
            # in planning queries. Therefore we do some tuning of autovacuum to address these
            # TimescaleDB specific concerns.
            # We'd rather have autovacuum do things early, as this increases the changes that autovacuum
            # will find the buffers it needs in shared_buffers, instead of having to fetch them from disk.
            #
            autovacuum_analyze_scale_factor: 0.02
            # This allows us to auto-analyze at most 120 (pretty much empty) chunks every 5 seconds
            # This will ensure that we can have up-to-date statistics on inserts very, very quickly
            autovacuum_naptime: 5s
            autovacuum_max_workers: 10
            # We don't want vacuum work to be building up, therefore we increase
            # the cost limit so that the work to be done for vacuum will be done quickly.
            autovacuum_vacuum_cost_limit: 500
            autovacuum_vacuum_scale_factor: 0.05
            log_autovacuum_min_duration: 1min
            hot_standby: 'on'
            log_checkpoints: 'on'
            log_connections: 'on'
            log_disconnections: 'on'
            log_line_prefix: "%t [%p]: [%c-%l] %u@%d,app=%a [%e] "
            log_lock_waits: 'on'
            log_min_duration_statement: '1s'
            log_statement: ddl
            max_connections: 100
            max_prepared_transactions: 150
            shared_preload_libraries: timescaledb,pg_stat_statements
            ssl: 'on'
            ssl_cert_file: '/etc/certificate/tls.crt'
            ssl_key_file: '/etc/certificate/tls.key'
            tcp_keepalives_idle: 900
            tcp_keepalives_interval: 100
            temp_file_limit: 1GB
            timescaledb.passfile: '../.pgpass'
            unix_socket_directories: "/var/run/postgresql"
            unix_socket_permissions: '0750'
            wal_level: hot_standby
            wal_log_hints: 'on'
          use_pg_rewind: true
          use_slots: true
        retry_timeout: 10
        ttl: 30
    kubernetes:
      role_label: role
      scope_label: cluster-name
      use_endpoints: true
    postgresql:
      create_replica_methods:
      - basebackup
      pgbackrest:
        command: /etc/timescaledb/scripts/pgbackrest_restore.sh
        keep_data: true
        no_params: true
        no_master: true
      basebackup:
      - waldir: "/var/lib/postgresql/wal/pg_wal"
      recovery_conf:
        restore_command: /etc/timescaledb/scripts/pgbackrest_archive_get.sh %f "%p"
      callbacks:
        on_role_change: /etc/timescaledb/scripts/patroni_callback.sh
        on_start: /etc/timescaledb/scripts/patroni_callback.sh
        on_reload: /etc/timescaledb/scripts/patroni_callback.sh
        on_restart: /etc/timescaledb/scripts/patroni_callback.sh
        on_stop: /etc/timescaledb/scripts/patroni_callback.sh
      authentication:
        replication:
          username: standby
        superuser:
          username: postgres
      listen: 0.0.0.0:5432
      pg_hba:
      - local     all             postgres                              peer
      - local     all             all                                   md5
      - hostnossl all,replication all                all                reject
      - hostssl   all             all                127.0.0.1/32       md5
      - hostssl   all             all                ::1/128            md5
      - hostssl   replication     standby            all                md5
      - hostssl   all             all                all                md5
      use_unix_socket: true
    restapi:
      listen: 0.0.0.0:8008

  callbacks:
    # If set, this configMap will be used for the Patroni callbacks.
    configMap:  # example-patroni-callbacks

  postInit:
    # A list of sources, that contain post init scripts.
    # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#projectedvolumesource-v1-core
    # These scripts are all projected to the same directory and will be executed
    # in sorted order only once: After a cluster initialization
    # Some examples:
    - configMap:
        name: timescale-post-init
        optional: false
    - secret:
        name: timescale-post-init-pw
        optional: false

  loadBalancer:
    # If not enabled, we still expose the primary using a so called Headless Service
    # https://kubernetes.io/docs/concepts/services-networking/service/#headless-services
    enabled: true
    port: 5432
    # Read more about the AWS annotations here:
    # https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#aws
    # https://docs.aws.amazon.com/eks/latest/userguide/load-balancing.html
    annotations:
      # Setting idle-timeout to the maximum allowed value, as in general
      # database connections are long lived
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "4000"

      # service.beta.kubernetes.io/aws-load-balancer-type: nlb            # Use an NLB instead of ELB
      # service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0  # Internal Load Balancer
    # Define extra things that should go in the service spec
    # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#servicespec-v1-core
    spec:
    # loadBalancerSourceRanges:
    # - "0.0.0.0/0"

  replicaLoadBalancer:
    # If not enabled, we still expose the replica's using a so called Headless Service
    # https://kubernetes.io/docs/concepts/services-networking/service/#headless-services
    enabled: false
    port: 5433
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "4000"
    # Define extra things that should go in the service spec
    # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#servicespec-v1-core
    spec:
    # loadBalancerSourceRanges:
    # - "0.0.0.0/0"

  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1

  persistentVolumes:
    # For sanity reasons, the actual PGDATA and wal directory will be subdirectories of the Volume mounts,
    # this allows Patroni/a human/an automated operator to move directories during bootstrap, which cannot
    # be done if we did not use subdirectories
    # https://www.postgresql.org/docs/current/creating-cluster.html#CREATING-CLUSTER-MOUNT-POINTS
    data:
      enabled: true
      size: 2Gi
      ## database data Persistent Volume Storage Class
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      # storageClass: "-"
      subPath: ""
      mountPath: "/var/lib/postgresql"
      annotations: {}
      accessModes:
        - ReadWriteOnce
    # WAL will be a subdirectory of the data volume, which means enabling a separate
    # volume for the WAL files should just work for new pods.
    wal:
      enabled: true
      size: 1Gi
      subPath: ""
      storageClass:
      # When changing this mountPath ensure you also change the following key to reflect this:
      # patroni.postgresql.basebackup.[].waldir
      mountPath: "/var/lib/postgresql/wal"
      annotations: {}
      accessModes:
        - ReadWriteOnce
    # Any tablespace mentioned here requires a volume that will be associated with it.
    # tablespaces:
      # example1:
      #   size: 5Gi
      #   storageClass: gp2
      # example2:
      #   size: 5Gi
      #   storageClass: gp2

  # EXPERIMENTAL, please do *not* enable on production environments
  # if enabled, fullWalPrevention will switch the default transaction mode from read write
  # to read only if thresholds are breached.
  fullWalPrevention:
    enabled: false
    checkFrequency: 30
    # To prevent the default transaction mode from switching constantly, we have separate
    # thresholds for switching to read-only and read-write
    thresholds:
      readOnlyFreePercent: 5
      readOnlyFreeMB: 64
      readWriteFreePercent: 8
      readWriteFreeMB: 128

  resources: {}
    # If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  sharedMemory:
    # By default Kubernetes only provides 64MB to /dev/shm
    # /dev/shm is only used by PostgreSQL for work_mem for parallel workers,
    # so most will not run into this issue.
    # https://github.com/kubernetes/kubernetes/issues/28272
    #
    # If you do however run into:
    #
    #   SQLSTATE 53100
    #   ERROR:  could not resize shared memory segment "/PostgreSQL.12345" to 4194304 bytes:
    #   No space left on device
    #
    # you may wish to use a mount to Memory, by setting useMount to true
    useMount: false

  # timescaledb-tune will be run with the Pod resources requests or - if not set - its limits.
  # This should give a reasonably tuned PostgreSQL instance.
  # Any PostgreSQL parameter that is explicitly set in the Patroni configuration will override
  # the auto-tuned variables.
  timescaledbTune:
    enabled: true
    # For full flexibility, we allow you to override any timescaledb-tune parameter below.
    # However, these parameters only take effect on newly scheduled pods and their settings are
    # only visibible inside those new pods.
    # Therefore you probably want to set explicit overrides in patroni.bootstrap.dcs.postgresql.parameters,
    # as those will take effect as soon as possible.
    # https://github.com/timescale/timescaledb-tune
    args: {}
      # max-conns: 120
      # cpus: 5
      # memory: 4GB

  # pgBouncer does connection pooling for PostgreSQL
  # https://www.pgbouncer.org/
  # enabling pgBouncer will run an extra container in every Pod, serving a pgBouncer
  # pass-through instance
  pgBouncer:
    enabled: false
    port: 6432
    config:
    # DANGER: The below settings are considered to be safe to set, and we recommend
    # you do set these to appropriate values for you.
    # However, for flexibility, we do allow the override of any pg_bouncer setting
    # many of which are vital to the operation of this helm chart.
    # The values we do not suggest altering are set in the template
    # https://github.com/timescale/timescaledb-kubernetes/blob/master/charts/timescaledb-single/templates/configmap-pgbouncer.yaml#L35-L50
    # Only override these settings if you are confident of  what you are doing.
      server_reset_query: DISCARD ALL
      max_client_conn: 500
      default_pool_size: 12
      pool_mode: transaction
    pg_hba:
    - local     all postgres                   peer
    - host      all postgres,standby 0.0.0.0/0 reject
    - host      all postgres,standby ::0/0     reject
    - hostssl   all all              0.0.0.0/0 md5
    - hostssl   all all              ::0/0     md5
    - hostnossl all all              0.0.0.0/0 reject
    - hostnossl all all              ::0/0     reject
    # Secret should contain user/password pairs in the format expected by pgbouncer
    # https://www.pgbouncer.org/config.html#authentication-file-format
    # example:
    # userlist.txt: |
    #   "username" "hashedpassword"
    #   "username2" "hashedpassword2"
    userListSecretName:

  networkPolicy:
    enabled: false
    prometheusApp: prometheus
    # Below you can specify a whitelist of Ingress rules, for more information:
    # https://kubernetes.io/docs/concepts/services-networking/network-policies/#the-networkpolicy-resource
    ingress:
    # - from:
    #   - podSelector:
    #       matchLabels:
    #         app: foo
    #   ports:
    #   - protocol: TCP
    #       port: 11111

  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
  nodeSelector: {}

  # Prometheus exporter for PostgreSQL server metrics.
  # https://github.com/wrouesnel/postgres_exporter
  prometheus:
    enabled: false
    image:
      repository: wrouesnel/postgres_exporter
      tag: v0.7.0
      pullPolicy: Always
    # Extra custom environment variables for prometheus.
    # These should be an EnvVar, as this allows you to inject secrets into the environment
    # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#envvar-v1-core
    env:
    # - name: NOT_A_SECRET
    #   value: "test"
    # - name: MYAPPLICATION_STANDBY_PASSWORDS
    #   valueFrom:
    #     secretKeyRef:
    #       name: myapplication-passwords
    #       key: standby
    # Additional volumes for prometheus, e.g., to support additional queries.
    # These should be a Volume, as this allows you to inject any kind of Volume
    # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#volume-v1-core
    volumes:
    # - name: exporter-config
    #   configMap:
    #     name: exporter-prometheus
    #     items:
    #       - key: metrics_queries
    #         path: queries.yaml
    # Additional volume mounts, to be used in conjunction with the above variable.
    # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#volumemount-v1-core
    volumeMounts:
    # - name: exporter-config
    #   mountPath: /var/exporter

  # For new deployments, we would advise Parallel here, however as that change breaks previous
  # deployments, it is set to OrderedReady here
  podManagementPolicy: OrderedReady

  # Annotations that are applied to each pod in the stateful set
  # https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  podAnnotations: {}

  # https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  affinityTemplate: |
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          topologyKey: "kubernetes.io/hostname"
          labelSelector:
            matchLabels:
              app: {{ template "timescaledb.fullname" . }}
              release: {{ .Release.Name | quote }}
              cluster-name: {{ template "clusterName" . }}
      - weight: 50
        podAffinityTerm:
          topologyKey: failure-domain.beta.kubernetes.io/zone
          labelSelector:
            matchLabels:
              app: {{ template "timescaledb.fullname" . }}
              release: {{ .Release.Name | quote }}
              cluster-name: {{ template "clusterName" . }}
  affinity: {}

  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName:

  rbac:
    # Specifies whether RBAC resources should be created
    create: true

  serviceAccount:
    # Specifies whether a ServiceAccount should be created
    create: true
    # The name of the ServiceAccount to use.
    # If not set and create is true, a name is generated using the fullname template
    name:

  # Setting unsafe to true will generate some random credentials. This is meant
  # for development or first evaluation of the Helm Charts. It should *not* be
  # used for anything beyong the evaluation phase.
  unsafe: false

  debug:
    # This setting is mainly for during development, debugging or troubleshooting.
    # This command will be executed *before* the main container starts. In the
    # example below, we can mimick a slow restore by sleeping for 5 minutes before starting
    execStartPre:  # sleep 300

### grafana ###
grafana:
  enabled: true

  rbac:
    create: true
    ## Use an existing ClusterRole/Role (depending on rbac.namespaced false/true)
    # useExistingRole: name-of-some-(cluster)role
    pspEnabled: true
    pspUseAppArmor: true
    namespaced: false
    extraRoleRules: []
    # - apiGroups: []
    #   resources: []
    #   verbs: []
    extraClusterRoleRules: []
    # - apiGroups: []
    #   resources: []
    #   verbs: []
  serviceAccount:
    create: true
    name:
    nameTest:
  #  annotations:
  #    eks.amazonaws.com/role-arn: arn:aws:iam::123456789000:role/iam-role-name-here

  replicas: 1

  ## See `kubectl explain poddisruptionbudget.spec` for more
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
  podDisruptionBudget:
    minAvailable: 1
  #  maxUnavailable: 1

  ## See `kubectl explain deployment.spec.strategy` for more
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  deploymentStrategy:
    type: RollingUpdate

  readinessProbe:
    httpGet:
      path: /api/health
      port: 3000

  livenessProbe:
    httpGet:
      path: /api/health
      port: 3000
    initialDelaySeconds: 60
    timeoutSeconds: 30
    failureThreshold: 10

  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName: "default-scheduler"

  image:
    repository: grafana/grafana
    tag: 8.1.3
    sha: ""
    pullPolicy: IfNotPresent

    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ##
    # pullSecrets:
    #   - myRegistrKeySecretName

  testFramework:
    enabled: false
    image: "bats/bats"
    tag: "v1.1.0"
    imagePullPolicy: IfNotPresent
    securityContext: {}

  securityContext:
    runAsUser: 472
    runAsGroup: 472
    fsGroup: 472

  containerSecurityContext:
    {}

  extraConfigmapMounts: []
    # - name: certs-configmap
    #   mountPath: /etc/grafana/ssl/
    #   subPath: certificates.crt # (optional)
    #   configMap: certs-configmap
    #   readOnly: true


  extraEmptyDirMounts: []
    # - name: provisioning-notifiers
    #   mountPath: /etc/grafana/provisioning/notifiers


  # Apply extra labels to common labels.
  extraLabels: {}

  ## Assign a PriorityClassName to pods if set
  # priorityClassName:

  downloadDashboardsImage:
    repository: curlimages/curl
    tag: 7.73.0
    sha: ""
    pullPolicy: IfNotPresent

  downloadDashboards:
    env: {}
    envFromSecret: ""
    resources: {}

  ## Pod Annotations
  # podAnnotations: {}

  ## Pod Labels
  # podLabels: {}

  podPortName: grafana

  ## Deployment annotations
  # annotations: {}

  ## Expose the grafana service to be accessed from outside the cluster (LoadBalancer service).
  ## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it.
  ## ref: http://kubernetes.io/docs/user-guide/services/
  ##
  service:
    type: LoadBalancer
    port: 8080
    targetPort: 3000
      # targetPort: 4181 To be used with a proxy extraContainer
    annotations: {}
    labels: {}
    portName: service

  serviceMonitor:
    ## If true, a ServiceMonitor CRD is created for a prometheus operator
    ## https://github.com/coreos/prometheus-operator
    ##
    enabled: false
    path: /metrics
    #  namespace: monitoring  (defaults to use the namespace this chart is deployed to)
    labels: {}
    interval: 1m
    scheme: http
    tlsConfig: {}
    scrapeTimeout: 30s
    relabelings: []

  extraExposePorts: []
   # - name: keycloak
   #   port: 8080
   #   targetPort: 8080
   #   type: ClusterIP

  # overrides pod.spec.hostAliases in the grafana deployment's pods
  hostAliases: []
    # - ip: "1.2.3.4"
    #   hostnames:
    #     - "my.host.com"

  ingress:
    enabled: false
    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx
    # Values can be templated
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    labels: {}
    path: /

    # pathType is only for k8s > 1.19
    pathType: Prefix

    hosts:
      - chart-example.local
    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
    extraPaths: []
    # - path: /*
    #   backend:
    #     serviceName: ssl-redirect
    #     servicePort: use-annotation
    ## Or for k8s > 1.19
    # - path: /*
    #   pathType: Prefix
    #   backend:
    #     service:
    #       name: ssl-redirect
    #       port:
    #         name: service


    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  resources: {}
  #  limits:
  #    cpu: 100m
  #    memory: 128Mi
  #  requests:
  #    cpu: 100m
  #    memory: 128Mi

  ## Node labels for pod assignment
  ## ref: https://kubernetes.io/docs/user-guide/node-selection/
  #
  nodeSelector: {}

  ## Tolerations for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []

  ## Affinity for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ##
  affinity: {}

  extraInitContainers:
  - args:
    - mkdir -p /var/lib/grafana/plugins && wget -O /var/lib/grafana/umh-datasource.zip "https://github.com/united-manufacturing-hub/united-manufacturing-hub-datasource/releases/latest/download/umh-datasource.zip" && unzip -n /var/lib/grafana/umh-datasource.zip -d /var/lib/grafana/plugins/ && wget -O /var/lib/grafana/umh-factoryinput-panel.zip "https://github.com/united-manufacturing-hub/umh-factoryinput-panel/releases/latest/download/umh-factoryinput-panel.zip" && unzip -n /var/lib/grafana/umh-factoryinput-panel.zip -d /var/lib/grafana/plugins/
    command:
    - /bin/sh
    - -c
    image: busybox
    name: init-umh-datasource
    volumeMounts:
    - mountPath: /var/lib/grafana
      name: storage

      # subPath: ia-factoryinsight-datasource.zip
  ## Enable an Specify container in extraContainers. This is meant to allow adding an authentication proxy to a grafana pod
  extraContainers: |
  # - name: proxy
  #   image: quay.io/gambol99/keycloak-proxy:latest
  #   args:
  #   - -provider=github
  #   - -client-id=
  #   - -client-secret=
  #   - -github-org=<ORG_NAME>
  #   - -email-domain=*
  #   - -cookie-secret=
  #   - -http-address=http://0.0.0.0:4181
  #   - -upstream-url=http://127.0.0.1:3000
  #   ports:
  #     - name: proxy-web
  #       containerPort: 4181


  ## Enable persistence using Persistent Volume Claims0
  ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
  ##
  persistence:
    type: pvc
    enabled: false
    # storageClassName: default
    accessModes:
      - ReadWriteOnce
    size: 10Gi
    # annotations: {}
    finalizers:
      - kubernetes.io/pvc-protection
    # selectorLabels: {}
    # subPath: ""
    # existingClaim:

    ## If persistence is not enabled, this allows to mount the
    ## local storage in-memory to improve performance
    ##
    inMemory:
      enabled: false
      ## The maximum usage on memory medium EmptyDir would be
      ## the minimum value between the SizeLimit specified
      ## here and the sum of memory limits of all containers in a pod
      ##
      # sizeLimit: 300Mi

  initChownData:
    ## If false, data ownership will not be reset at startup
    ## This allows the prometheus-server to be run with an arbitrary user
    ##
    enabled: true

    ## initChownData container image
    ##
    image:
      repository: busybox
      tag: "1.31.1"
      sha: ""
      pullPolicy: IfNotPresent

    ## initChownData resource requests and limits
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources: {}
    #  limits:
    #    cpu: 100m
    #    memory: 128Mi
    #  requests:
    #    cpu: 100m
    #    memory: 128Mi


  # Administrator credentials when not using an existing secret (see below)
  adminUser: admin
  # adminPassword: strongpassword

  # Use an existing secret for the admin user.
  admin:
    existingSecret: "grafana-secret"
    userKey: adminuser
    passwordKey: adminpassword

  ## Define command to be executed at startup by grafana container
  ## Needed if using `vault-env` to manage secrets (ref: https://banzaicloud.com/blog/inject-secrets-into-pods-vault/)
  ## Default is "run.sh" as defined in grafana's Dockerfile
  # command:
  # - "sh"
  # - "/run.sh"

  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName:

  ## Extra environment variables that will be pass onto deployment pods
  ##
  ## to provide grafana with access to CloudWatch on AWS EKS:
  ## 1. create an iam role of type "Web identity" with provider oidc.eks.* (note the provider for later)
  ## 2. edit the "Trust relationships" of the role, add a line inside the StringEquals clause using the
  ## same oidc eks provider as noted before (same as the existing line)
  ## also, replace NAMESPACE and prometheus-operator-grafana with the service account namespace and name
  ##
  ##  "oidc.eks.us-east-1.amazonaws.com/id/XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX:sub": "system:serviceaccount:NAMESPACE:prometheus-operator-grafana",
  ##
  ## 3. attach a policy to the role, you can use a built in policy called CloudWatchReadOnlyAccess
  ## 4. use the following env: (replace 123456789000 and iam-role-name-here with your aws account number and role name)
  ##
  ## env:
  ##   AWS_ROLE_ARN: arn:aws:iam::123456789000:role/iam-role-name-here
  ##   AWS_WEB_IDENTITY_TOKEN_FILE: /var/run/secrets/eks.amazonaws.com/serviceaccount/token
  ##   AWS_REGION: us-east-1
  ##
  ## 5. uncomment the EKS section in extraSecretMounts: below
  ## 6. uncomment the annotation section in the serviceAccount: above
  ## make sure to replace arn:aws:iam::123456789000:role/iam-role-name-here with your role arn

  env:
    GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS: umh-datasource,umh-factoryinput-panel

  ## "valueFrom" environment variable references that will be added to deployment pods
  ## ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#envvarsource-v1-core
  ## Renders in container spec as:
  ##   env:
  ##     ...
  ##     - name: <key>
  ##       valueFrom:
  ##         <value rendered as YAML>
  envValueFrom:
    FACTORYINSIGHT_BASEURL:
        secretKeyRef:
          name: factoryinsight-secret
          key: baseURL
    FACTORYINSIGHT_APIKEY:
        secretKeyRef:
          name: factoryinsight-secret
          key: apiKey
    FACTORYINSIGHT_CUSTOMERID:
        secretKeyRef:
          name: factoryinsight-secret
          key: customerID
    FACTORYINSIGHT_PASSWORD:
        secretKeyRef:
          name: factoryinsight-secret
          key: password


  ## The name of a secret in the same kubernetes namespace which contain values to be added to the environment
  ## This can be useful for auth tokens, etc. Value is templated.
  envFromSecret: ""

  ## Sensible environment variables that will be rendered as new secret object
  ## This can be useful for auth tokens, etc
  envRenderSecret: {}

  ## Additional grafana server secret mounts
  # Defines additional mounts with secrets. Secrets must be manually created in the namespace.
  extraSecretMounts: []
    # - name: secret-files
    #   mountPath: /etc/secrets
    #   secretName: grafana-secret-files
    #   readOnly: true
    #   subPath: ""
    #
    # for AWS EKS (cloudwatch) use the following (see also instruction in env: above)
    # - name: aws-iam-token
    #   mountPath: /var/run/secrets/eks.amazonaws.com/serviceaccount
    #   readOnly: true
    #   projected:
    #     defaultMode: 420
    #     sources:
    #       - serviceAccountToken:
    #           audience: sts.amazonaws.com
    #           expirationSeconds: 86400
    #           path: token
    #
    # for CSI e.g. Azure Key Vault use the following
    # - name: secrets-store-inline
    #  mountPath: /run/secrets
    #  readOnly: true
    #  csi:
    #    driver: secrets-store.csi.k8s.io
    #    readOnly: true
    #    volumeAttributes:
    #      secretProviderClass: "akv-grafana-spc"
    #    nodePublishSecretRef:                       # Only required when using service principal mode
    #       name: grafana-akv-creds                  # Only required when using service principal mode

  ## Additional grafana server volume mounts
  # Defines additional volume mounts.
  extraVolumeMounts: []
    # - name: extra-volume-0
    #   mountPath: /mnt/volume0
    #   readOnly: true
    #   existingClaim: volume-claim
    # - name: extra-volume-1
    #   mountPath: /mnt/volume1
    #   readOnly: true
    #   hostPath: /usr/shared/

  ## Pass the plugins you want installed as a list.
  ##
  plugins:
      - grafana-worldmap-panel
      - grafana-piechart-panel
      - aceiot-svg-panel
      - grafana-worldmap-panel
      - natel-discrete-panel
      - isaozler-paretochart-panel
      - williamvenner-timepickerbuttons-panel
      - agenty-flowcharting-panel
      - marcusolsson-dynamictext-panel
      - factry-untimely-panel
      - cloudspout-button-panel

  ## Configure grafana datasources
  ## ref: http://docs.grafana.org/administration/provisioning/#datasources
  ##
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        # <string, required> name of the datasource. Required
        - name: umh-datasource
          # <string, required> datasource type. Required
          type: umh-datasource
          # <string, required> access mode. proxy or direct (Server or Browser in the UI). Required
          access: proxy
          # <int> org id. will default to orgId 1 if not specified
          orgId: 1
          url: "http://factorycube-server-factoryinsight-service/"
          # <string> custom UID which can be used to reference this datasource in other parts of the configuration, if not specified will be generated automatically
          jsonData:
            customerId: $FACTORYINSIGHT_CUSTOMERID
            apiKey: $FACTORYINSIGHT_PASSWORD
            serverURL: "http://factorycube-server-factoryinsight-service/"
            apiKeyConfigured: true
          version: 1
          # <bool> allow users to edit datasources from the UI.
          isDefault: true
          editable: false

  ## Configure notifiers
  ## ref: http://docs.grafana.org/administration/provisioning/#alert-notification-channels
  ##
  notifiers: {}
  #  notifiers.yaml:
  #    notifiers:
  #    - name: email-notifier
  #      type: email
  #      uid: email1
  #      # either:
  #      org_id: 1
  #      # or
  #      org_name: Main Org.
  #      is_default: true
  #      settings:
  #        addresses: an_email_address@example.com
  #    delete_notifiers:

  ## Configure grafana dashboard providers
  ## ref: http://docs.grafana.org/administration/provisioning/#dashboards
  ##
  ## `path` must be /var/lib/grafana/dashboards/<provider_name>
  ##
  dashboardProviders: {}
  #  dashboardproviders.yaml:
  #    apiVersion: 1
  #    providers:
  #    - name: 'default'
  #      orgId: 1
  #      folder: ''
  #      type: file
  #      disableDeletion: false
  #      editable: true
  #      options:
  #        path: /var/lib/grafana/dashboards/default

  ## Configure grafana dashboard to import
  ## NOTE: To use dashboards you must also enable/configure dashboardProviders
  ## ref: https://grafana.com/dashboards
  ##
  ## dashboards per provider, use provider name as key.
  ##
  dashboards: {}
    # default:
    #   some-dashboard:
    #     json: |
    #       $RAW_JSON
    #   custom-dashboard:
    #     file: dashboards/custom-dashboard.json
    #   prometheus-stats:
    #     gnetId: 2
    #     revision: 2
    #     datasource: Prometheus
    #   local-dashboard:
    #     url: https://example.com/repository/test.json
    #     token: ''
    #   local-dashboard-base64:
    #     url: https://example.com/repository/test-b64.json
    #     token: ''
    #     b64content: true

  ## Reference to external ConfigMap per provider. Use provider name as key and ConfigMap name as value.
  ## A provider dashboards must be defined either by external ConfigMaps or in values.yaml, not in both.
  ## ConfigMap data example:
  ##
  ## data:
  ##   example-dashboard.json: |
  ##     RAW_JSON
  ##
  dashboardsConfigMaps: {}
  #  default: ""

  ## Grafana's primary configuration
  ## NOTE: values in map will be converted to ini format
  ## ref: http://docs.grafana.org/installation/configuration/
  ##
  grafana.ini:
    paths:
      data: /var/lib/grafana/data
      logs: /var/log/grafana
      plugins: /var/lib/grafana/plugins
      provisioning: /etc/grafana/provisioning
    analytics:
      check_for_updates: true
    log:
      mode: console
    grafana_net:
      url: https://grafana.net
    database:
      host: factorycube-server
      user: "grafana"
      name: "grafana"
      password: "changeme"
      ssl_mode: require
      type: postgres
  ## grafana Authentication can be enabled with the following values on grafana.ini
   # server:
        # The full public facing url you use in browser, used for redirects and emails
   #    root_url:
   # https://grafana.com/docs/grafana/latest/auth/github/#enable-github-in-grafana
   # auth.github:
   #    enabled: false
   #    allow_sign_up: false
   #    scopes: user:email,read:org
   #    auth_url: https://github.com/login/oauth/authorize
   #    token_url: https://github.com/login/oauth/access_token
   #    api_url: https://api.github.com/user
   #    team_ids:
   #    allowed_organizations:
   #    client_id:
   #    client_secret:
  ## LDAP Authentication can be enabled with the following values on grafana.ini
  ## NOTE: Grafana will fail to start if the value for ldap.toml is invalid
    # auth.ldap:
    #   enabled: true
    #   allow_sign_up: true
    #   config_file: /etc/grafana/ldap.toml

  ## Grafana's LDAP configuration
  ## Templated by the template in _helpers.tpl
  ## NOTE: To enable the grafana.ini must be configured with auth.ldap.enabled
  ## ref: http://docs.grafana.org/installation/configuration/#auth-ldap
  ## ref: http://docs.grafana.org/installation/ldap/#configuration
  ldap:
    enabled: false
    # `existingSecret` is a reference to an existing secret containing the ldap configuration
    # for Grafana in a key `ldap-toml`.
    existingSecret: ""
    # `config` is the content of `ldap.toml` that will be stored in the created secret
    config: ""
    # config: |-
    #   verbose_logging = true

    #   [[servers]]
    #   host = "my-ldap-server"
    #   port = 636
    #   use_ssl = true
    #   start_tls = false
    #   ssl_skip_verify = false
    #   bind_dn = "uid=%s,ou=users,dc=myorg,dc=com"

  ## Grafana's SMTP configuration
  ## NOTE: To enable, grafana.ini must be configured with smtp.enabled
  ## ref: http://docs.grafana.org/installation/configuration/#smtp
  smtp:
    # `existingSecret` is a reference to an existing secret containing the smtp configuration
    # for Grafana.
    existingSecret: ""
    userKey: "user"
    passwordKey: "password"

  ## Sidecars that collect the configmaps with specified label and stores the included files them into the respective folders
  ## Requires at least Grafana 5 to work and can't be used together with parameters dashboardProviders, datasources and dashboards
  sidecar:
    image:
      repository: quay.io/kiwigrid/k8s-sidecar
      tag: 1.10.7
      sha: ""
    imagePullPolicy: IfNotPresent
    resources: {}
  #   limits:
  #     cpu: 100m
  #     memory: 100Mi
  #   requests:
  #     cpu: 50m
  #     memory: 50Mi
    # skipTlsVerify Set to true to skip tls verification for kube api calls
    # skipTlsVerify: true
    enableUniqueFilenames: false
    dashboards:
      enabled: false
      SCProvider: true
      # label that the configmaps with dashboards are marked with
      label: grafana_dashboard
      # value of label that the configmaps with dashboards are set to
      labelValue: null
      # folder in the pod that should hold the collected dashboards (unless `defaultFolderName` is set)
      folder: /tmp/dashboards
      # The default folder name, it will create a subfolder under the `folder` and put dashboards in there instead
      defaultFolderName: null
      # If specified, the sidecar will search for dashboard config-maps inside this namespace.
      # Otherwise the namespace in which the sidecar is running will be used.
      # It's also possible to specify ALL to search in all namespaces
      searchNamespace: null
      # If specified, the sidecar will look for annotation with this name to create folder and put graph here.
      # You can use this parameter together with `provider.foldersFromFilesStructure`to annotate configmaps and create folder structure.
      folderAnnotation: null
      # provider configuration that lets grafana manage the dashboards
      provider:
        # name of the provider, should be unique
        name: sidecarProvider
        # orgid as configured in grafana
        orgid: 1
        # folder in which the dashboards should be imported in grafana
        folder: ''
        # type of the provider
        type: file
        # disableDelete to activate a import-only behaviour
        disableDelete: false
        # allow updating provisioned dashboards from the UI
        allowUiUpdates: false
        # allow Grafana to replicate dashboard structure from filesystem
        foldersFromFilesStructure: false
    datasources:
      enabled: false
      # label that the configmaps with datasources are marked with
      label: grafana_datasource
      # value of label that the configmaps with datasources are set to
      labelValue: null
      # If specified, the sidecar will search for datasource config-maps inside this namespace.
      # Otherwise the namespace in which the sidecar is running will be used.
      # It's also possible to specify ALL to search in all namespaces
      searchNamespace: null
    notifiers:
      enabled: false
      # label that the configmaps with notifiers are marked with
      label: grafana_notifier
      # If specified, the sidecar will search for notifier config-maps inside this namespace.
      # Otherwise the namespace in which the sidecar is running will be used.
      # It's also possible to specify ALL to search in all namespaces
      searchNamespace: null

  ## Override the deployment namespace
  ##
  namespaceOverride: ""

  ## Number of old ReplicaSets to retain
  ##
  revisionHistoryLimit: 10

  ## Add a seperate remote image renderer deployment/service
  imageRenderer:
    # Enable the image-renderer deployment & service
    enabled: false
    replicas: 1
    image:
      # image-renderer Image repository
      repository: grafana/grafana-image-renderer
      # image-renderer Image tag
      tag: latest
      # image-renderer Image sha (optional)
      sha: ""
      # image-renderer ImagePullPolicy
      pullPolicy: Always
    # extra environment variables
    env:
      HTTP_HOST: "0.0.0.0"
      # RENDERING_ARGS: --disable-gpu,--window-size=1280x758
      # RENDERING_MODE: clustered
    # image-renderer deployment serviceAccount
    serviceAccountName: ""
    # image-renderer deployment securityContext
    securityContext: {}
    # image-renderer deployment Host Aliases
    hostAliases: []
    # image-renderer deployment priority class
    priorityClassName: ''
    service:
      # image-renderer service port name
      portName: 'http'
      # image-renderer service port used by both service and deployment
      port: 8081
      targetPort: 8081
    # In case a sub_path is used this needs to be added to the image renderer callback
    grafanaSubPath: ""
    # name of the image-renderer port on the pod
    podPortName: http
    # number of image-renderer replica sets to keep
    revisionHistoryLimit: 10
    networkPolicy:
      # Enable a NetworkPolicy to limit inbound traffic to only the created grafana pods
      limitIngress: true
      # Enable a NetworkPolicy to limit outbound traffic to only the created grafana pods
      limitEgress: false
    resources: {}
  #   limits:
  #     cpu: 100m
  #     memory: 100Mi
  #   requests:
  #     cpu: 50m
  #     memory: 50Mi
### nodered ###
nodered:
  enabled: false
  pluginInstallEnabled: true
  pluginInstallLines: |-
      npm list node-red-contrib-cron-plus || npm install node-red-contrib-cron-plus;
  tag: 2.0.6
  port: 1880
  storageRequest: 1Gi
  env:
      NODE_RED_ENABLE_SAFE_MODE: false
  timezone: Berlin/Europe
  serviceType: LoadBalancer
  ingress:
    enabled: false
    publicHost: ""
    publicHostSecretName: ""
  settings: |-
    module.exports = {
        // the tcp port that the Node-RED web server is listening on
        uiPort: process.env.PORT || 1880,
        // By default, the Node-RED UI accepts connections on all IPv4 interfaces.
        // To listen on all IPv6 addresses, set uiHost to "::",
        // The following property can be used to listen on a specific interface. For
        // example, the following would only allow connections from the local machine.
        //uiHost: "127.0.0.1",
        // Retry time in milliseconds for MQTT connections
        mqttReconnectTime: 15000,
        // Retry time in milliseconds for Serial port connections
        serialReconnectTime: 15000,
        // Retry time in milliseconds for TCP socket connections
        //socketReconnectTime: 10000,
        // Timeout in milliseconds for TCP server socket connections
        //  defaults to no timeout
        //socketTimeout: 120000,
        // Maximum number of messages to wait in queue while attempting to connect to TCP socket
        //  defaults to 1000
        //tcpMsgQueueSize: 2000,
        // Timeout in milliseconds for HTTP request connections
        //  defaults to 120 seconds
        //httpRequestTimeout: 120000,
        // The maximum length, in characters, of any message sent to the debug sidebar tab
        debugMaxLength: 1000,
        // The maximum number of messages nodes will buffer internally as part of their
        // operation. This applies across a range of nodes that operate on message sequences.
        //  defaults to no limit. A value of 0 also means no limit is applied.
        //nodeMessageBufferMaxLength: 0,
        // To disable the option for using local files for storing keys and certificates in the TLS configuration
        //  node, set this to true
        //tlsConfigDisableLocalFiles: true,
        // Colourise the console output of the debug node
        //debugUseColors: true,
        // The file containing the flows. If not set, it defaults to flows_<hostname>.json
        //flowFile: 'flows.json',
        // To enabled pretty-printing of the flow within the flow file, set the following
        //  property to true:
        //flowFilePretty: true,
        // By default, credentials are encrypted in storage using a generated key. To
        // specify your own secret, set the following property.
        // If you want to disable encryption of credentials, set this property to false.
        // Note: once you set this property, do not change it - doing so will prevent
        // node-red from being able to decrypt your existing credentials and they will be
        // lost.
        //credentialSecret: "a-secret-key",
        // By default, all user data is stored in a directory called `.node-red` under
        // the user's home directory. To use a different location, the following
        // property can be used
        //userDir: '/home/nol/.node-red/',
        // Node-RED scans the `nodes` directory in the userDir to find local node files.
        // The following property can be used to specify an additional directory to scan.
        //nodesDir: '/home/nol/.node-red/nodes',
        // By default, the Node-RED UI is available at http://localhost:1880/
        // The following property can be used to specify a different root path.
        // If set to false, this is disabled.
        //httpAdminRoot: '/admin',
        // Some nodes, such as HTTP In, can be used to listen for incoming http requests.
        // By default, these are served relative to '/'. The following property
        // can be used to specifiy a different root path. If set to false, this is
        // disabled.
        //httpNodeRoot: '/red-nodes',
        // The following property can be used in place of 'httpAdminRoot' and 'httpNodeRoot',
        // to apply the same root to both parts.
        httpRoot: '/nodered',
        // When httpAdminRoot is used to move the UI to a different root path, the
        // following property can be used to identify a directory of static content
        // that should be served at http://localhost:1880/.
        //httpStatic: '/home/nol/node-red-static/',
        // The maximum size of HTTP request that will be accepted by the runtime api.
        // Default: 5mb
        //apiMaxLength: '5mb',
        // If you installed the optional node-red-dashboard you can set it's path
        // relative to httpRoot
        ui: { path: "ui" },
        // Securing Node-RED
        // -----------------
        // To password protect the Node-RED editor and admin API, the following
        // property can be used. See http://nodered.org/docs/security.html for details.
        //adminAuth: {},
        // To password protect the node-defined HTTP endpoints (httpNodeRoot), or
        // the static content (httpStatic), the following properties can be used.
        // The pass field is a bcrypt hash of the password.
        // See http://nodered.org/docs/security.html#generating-the-password-hash
        //httpNodeAuth: {user:"user",pass:"$2a$08$zZWtXTja0fB1pzD4sHCMyOCMYz2Z6dNbM6tl8sJogENOMcxWV9DN."},
        //httpStaticAuth: {user:"user",pass:"$2a$08$zZWtXTja0fB1pzD4sHCMyOCMYz2Z6dNbM6tl8sJogENOMcxWV9DN."},
        // The following property can be used to enable HTTPS
        // See http://nodejs.org/api/https.html#https_https_createserver_options_requestlistener
        // for details on its contents.
        // This property can be either an object, containing both a (private) key and a (public) certificate,
        // or a function that returns such an object:
        //// https object:
        //https: {
        //  key: require("fs").readFileSync('privkey.pem'),
        //  cert: require("fs").readFileSync('cert.pem')
        //},
        ////https function:
        // https: function() {
        //     // This function should return the options object, or a Promise
        //     // that resolves to the options object
        //     return {
        //         key: require("fs").readFileSync('privkey.pem'),
        //         cert: require("fs").readFileSync('cert.pem')
        //     }
        // },
        // The following property can be used to refresh the https settings at a
        // regular time interval in hours.
        // This requires:
        //   - the `https` setting to be a function that can be called to get
        //     the refreshed settings.
        //   - Node.js 11 or later.
        //httpsRefreshInterval : 12,
        // The following property can be used to cause insecure HTTP connections to
        // be redirected to HTTPS.
        //requireHttps: true,
        // The following property can be used to disable the editor. The admin API
        // is not affected by this option. To disable both the editor and the admin
        // API, use either the httpRoot or httpAdminRoot properties
        //disableEditor: false,
        // The following property can be used to configure cross-origin resource sharing
        // in the HTTP nodes.
        // See https://github.com/troygoode/node-cors#configuration-options for
        // details on its contents. The following is a basic permissive set of options:
        //httpNodeCors: {
        //    origin: "*",
        //    methods: "GET,PUT,POST,DELETE"
        //},
        // If you need to set an http proxy please set an environment variable
        // called http_proxy (or HTTP_PROXY) outside of Node-RED in the operating system.
        // For example - http_proxy=http://myproxy.com:8080
        // (Setting it here will have no effect)
        // You may also specify no_proxy (or NO_PROXY) to supply a comma separated
        // list of domains to not proxy, eg - no_proxy=.acme.co,.acme.co.uk
        // The following property can be used to add a custom middleware function
        // in front of all http in nodes. This allows custom authentication to be
        // applied to all http in nodes, or any other sort of common request processing.
        //httpNodeMiddleware: function(req,res,next) {
        //    // Handle/reject the request, or pass it on to the http in node by calling next();
        //    // Optionally skip our rawBodyParser by setting this to true;
        //    //req.skipRawBodyParser = true;
        //    next();
        //},
        // The following property can be used to add a custom middleware function
        // in front of all admin http routes. For example, to set custom http
        // headers
        // httpAdminMiddleware: function(req,res,next) {
        //    // Set the X-Frame-Options header to limit where the editor
        //    // can be embedded
        //    //res.set('X-Frame-Options', 'sameorigin');
        //    next();
        // },
        // The following property can be used to pass custom options to the Express.js
        // server used by Node-RED. For a full list of available options, refer
        // to http://expressjs.com/en/api.html#app.settings.table
        //httpServerOptions: { },
        // The following property can be used to verify websocket connection attempts.
        // This allows, for example, the HTTP request headers to be checked to ensure
        // they include valid authentication information.
        //webSocketNodeVerifyClient: function(info) {
        //    // 'info' has three properties:
        //    //   - origin : the value in the Origin header
        //    //   - req : the HTTP request
        //    //   - secure : true if req.connection.authorized or req.connection.encrypted is set
        //    //
        //    // The function should return true if the connection should be accepted, false otherwise.
        //    //
        //    // Alternatively, if this function is defined to accept a second argument, callback,
        //    // it can be used to verify the client asynchronously.
        //    // The callback takes three arguments:
        //    //   - result : boolean, whether to accept the connection or not
        //    //   - code : if result is false, the HTTP error status to return
        //    //   - reason: if result is false, the HTTP reason string to return
        //},
        // The following property can be used to seed Global Context with predefined
        // values. This allows extra node modules to be made available with the
        // Function node.
        // For example,
        //    functionGlobalContext: { os:require('os') }
        // can be accessed in a function block as:
        //    global.get("os")
        functionGlobalContext: {
            // os:require('os'),
            // jfive:require("johnny-five"),
            // j5board:require("johnny-five").Board({repl:false})
        },
        // `global.keys()` returns a list of all properties set in global context.
        // This allows them to be displayed in the Context Sidebar within the editor.
        // In some circumstances it is not desirable to expose them to the editor. The
        // following property can be used to hide any property set in `functionGlobalContext`
        // from being list by `global.keys()`.
        // By default, the property is set to false to avoid accidental exposure of
        // their values. Setting this to true will cause the keys to be listed.
        exportGlobalContextKeys: false,
        // Context Storage
        // The following property can be used to enable context storage. The configuration
        // provided here will enable file-based context that flushes to disk every 30 seconds.
        // Refer to the documentation for further options: https://nodered.org/docs/api/context/
        //
        //contextStorage: {
        //    default: {
        //        module:"localfilesystem"
        //    },
        //},
        // The following property can be used to order the categories in the editor
        // palette. If a node's category is not in the list, the category will get
        // added to the end of the palette.
        // If not set, the following default order is used:
        //paletteCategories: ['subflows', 'common', 'function', 'network', 'sequence', 'parser', 'storage'],
        // Configure the logging output
        logging: {
            // Only console logging is currently supported
            console: {
                // Level of logging to be recorded. Options are:
                // fatal - only those errors which make the application unusable should be recorded
                // error - record errors which are deemed fatal for a particular request + fatal errors
                // warn - record problems which are non fatal + errors + fatal errors
                // info - record information about the general running of the application + warn + error + fatal errors
                // debug - record information which is more verbose than info + info + warn + error + fatal errors
                // trace - record very detailed logging + debug + info + warn + error + fatal errors
                // off - turn off all logging (doesn't affect metrics or audit)
                level: "info",
                // Whether or not to include metric events in the log output
                metrics: false,
                // Whether or not to include audit events in the log output
                audit: false
            }
        },
        // Customising the editor
        editorTheme: {
            projects: {
                // To enable the Projects feature, set this value to true
                enabled: false
            }
        }
    }


##### CONFIG FOR REDIS #####
redis:
  enabled: true
  cluster:
    enabled: true
    slaveCount: 1
  clusterDomain: cluster.local
  common:
    exampleValue: common-chart
    global:
      redis: {}
  configmap: |-
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
  containerSecurityContext:
    enabled: true
    runAsUser: 1001
  global:
    redis: {}
  image:
    pullPolicy: IfNotPresent
    registry: docker.io
    repository: bitnami/redis
    tag: 6.0.9-debian-10-r13
  master:
    affinity: {}
    command: /run.sh
    customLivenessProbe: {}
    customReadinessProbe: {}
    disableCommands:
    - FLUSHDB
    - FLUSHALL
    extraEnvVars: []
    extraEnvVarsCM: []
    extraEnvVarsSecret: []
    extraFlags:
    - --maxmemory 4gb
    hostAliases: []
    livenessProbe:
      enabled: true
      failureThreshold: 5
      initialDelaySeconds: 5
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 5
    persistence:
      accessModes:
      - ReadWriteOnce
      enabled: true
      matchExpressions: {}
      matchLabels: {}
      path: /data
      size: 8Gi
      subPath: ""
      volumes: null
    podAnnotations: {}
    podLabels: {}
    preExecCmds: ""
    priorityClassName: ""
    readinessProbe:
      enabled: true
      failureThreshold: 5
      initialDelaySeconds: 120
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 1
    resources:
      limits:
        memory: 4Gi
      requests:
        cpu: 100m
        memory: 1Gi
    service:
      annotations: {}
      externalTrafficPolicy: Cluster
      labels: {}
      port: 6379
      type: ClusterIP
    shareProcessNamespace: false
    statefulset:
      annotations: {}
      labels: {}
      updateStrategy: RollingUpdate
      volumeClaimTemplates:
        annotations: {}
        labels: {}
  metrics:
    enabled: true
    image:
      pullPolicy: IfNotPresent
      registry: docker.io
      repository: bitnami/redis-exporter
      tag: 1.13.1-debian-10-r6
    podAnnotations:
      prometheus.io/port: "9121"
      prometheus.io/scrape: "true"
    priorityClassName: {}
    prometheusRule:
      additionalLabels: {}
      enabled: false
      namespace: ""
      rules: []
    service:
      annotations: {}
      externalTrafficPolicy: Cluster
      labels: {}
      type: ClusterIP
    serviceMonitor:
      enabled: false
      metricRelabelings: []
      relabelings: []
      selector:
        prometheus: kube-prometheus
  networkPolicy:
    enabled: false
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  #password: changeme
  existingSecret: redis-secret
  existingSecretPasswordKey: redispassword
  persistence: {}
  podDisruptionBudget:
    enabled: true
    minAvailable: 2
  podSecurityPolicy:
    create: false
  rbac:
    create: false
    role:
      rules: []
  redisPort: 6379
  securityContext:
    enabled: true
    fsGroup: 1001
  sentinel:
    customLivenessProbe: {}
    customReadinessProbe: {}
    downAfterMilliseconds: 1000
    enabled: true
    extraEnvVars: []
    extraEnvVarsCM: []
    extraEnvVarsSecret: []
    failoverTimeout: 18000
    image:
      pullPolicy: IfNotPresent
      registry: docker.io
      repository: bitnami/redis-sentinel
      tag: 6.0.9-debian-10-r14
    initialCheckTimeout: 5
    livenessProbe:
      enabled: true
      failureThreshold: 5
      initialDelaySeconds: 5
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 5
    masterSet: mymaster
    parallelSyncs: 1
    port: 26379
    preExecCmds: ""
    # quorum: 2
    quorum: 1
    readinessProbe:
      enabled: true
      failureThreshold: 5
      initialDelaySeconds: 120
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 1
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
    service:
      annotations: {}
      externalTrafficPolicy: Cluster
      labels: {}
      redisPort: 6379
      sentinelPort: 26379
      type: ClusterIP
    staticID: false
    usePassword: true
  serviceAccount:
    create: false
  slave:
    affinity: {}
    command: /run.sh
    customLivenessProbe: {}
    customReadinessProbe: {}
    disableCommands:
    - FLUSHDB
    - FLUSHALL
    extraEnvVars: []
    extraEnvVarsCM: []
    extraEnvVarsSecret: []
    extraFlags: []
    hostAliases: []
    livenessProbe:
      enabled: true
      failureThreshold: 5
      initialDelaySeconds: 30
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 5
    persistence:
      accessModes:
      - ReadWriteOnce
      enabled: true
      matchExpressions: {}
      matchLabels: {}
      path: /data
      size: 8Gi
      subPath: ""
    podAnnotations: {}
    podLabels: {}
    port: 6379
    preExecCmds: ""
    priorityClassName: {}
    readinessProbe:
      enabled: true
      failureThreshold: 5
      initialDelaySeconds: 120
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 10
    resources:
      limits:
        memory: 4Gi
      requests:
        cpu: 100m
        memory: 1Gi
    service:
      annotations: {}
      externalTrafficPolicy: Cluster
      labels: {}
      port: 6379
      type: ClusterIP
    shareProcessNamespace: false
    spreadConstraints: {}
    statefulset:
      annotations: {}
      labels: {}
      updateStrategy: RollingUpdate
      volumeClaimTemplates:
        annotations: {}
        labels: {}
  sysctlImage:
    command: []
    enabled: false
    mountHostSys: false
    pullPolicy: Always
    registry: docker.io
    repository: bitnami/minideb
    resources: {}
    tag: buster
  tls:
    authClients: true
    enabled: false
  usePassword: true
  usePasswordFile: false
  volumePermissions:
    enabled: false
    image:
      pullPolicy: Always
      registry: docker.io
      repository: bitnami/minideb
      tag: buster
    resources: {}
    securityContext:
      runAsUser: 0

##### CONFIG FOR VERNEMQ #####

vernemq:
  enabled: true
  AclConfig: |-
     pattern write ia/raw/%u/#
     pattern write ia/%u/#
     pattern $SYS/broker/connection/%c/state

     user TESTING
     topic ia/#
     topic $SYS/#
     topic read $share/TESTING/ia/#

     user ia_nodered
     topic ia/#

  CACert: |-
    -----BEGIN CERTIFICATE-----
    MIIDSzCCAjOgAwIBAgIUI2id6KFXw/nEv9bRhVsDhzM/ozkwDQYJKoZIhvcNAQEL
    BQAwFjEUMBIGA1UEAwwLRWFzeS1SU0EgQ0EwHhcNMjEwNDA2MDkyNDM1WhcNMzEw
    NDA0MDkyNDM1WjAWMRQwEgYDVQQDDAtFYXN5LVJTQSBDQTCCASIwDQYJKoZIhvcN
    AQEBBQADggEPADCCAQoCggEBALh3vJroi6hcvcm3OmpXs26MsXmRiUhBgdrNSOzZ
    KjWpW+R9aMONu1KxoXAuj/pAwt5CkLAMQ5EI5H05OArYOKekLfMe3FRTEOUSvFU/
    qO0hBymJRJZRhECwWu01gL1OS4AXaxdjrXwx8H4URAiKa4FhJCWrSPCikNvzX4K2
    SSSZMbMXEddO9uoyBU83HcZdo8NX0QRUKXy6w/zVMsjyLSZCtk4BaLCmlidhcH91
    Hggl6JDk4CuYRfkmemyblzw2bKAlWqsppv/m1nkI3hEFvwN68cpfsNnQynfYpZkL
    ODuNnia+PIhl3XyDXkJCM2ylPObcmo0D9GV2/J0JZgu9y3ECAwEAAaOBkDCBjTAd
    BgNVHQ4EFgQUvW9fQDy1ACN7knRxApaEh3LxW78wUQYDVR0jBEowSIAUvW9fQDy1
    ACN7knRxApaEh3LxW7+hGqQYMBYxFDASBgNVBAMMC0Vhc3ktUlNBIENBghQjaJ3o
    oVfD+cS/1tGFWwOHMz+jOTAMBgNVHRMEBTADAQH/MAsGA1UdDwQEAwIBBjANBgkq
    hkiG9w0BAQsFAAOCAQEAfDsmd4FaA4R0rS84hGqileIaUdwhGE5IAlj3W8Wd2O63
    xqThR+qN7TzO5Nki7tXNC2qCpuJmbPqrbB0CgwiobkwV+3vwF+VW8lCjSfYSpXbs
    nUN9CXMkQVWwbx8cAc7LlmpxU9evD21cBGOvD9AjngJwcofUUYXIuwWDdd1DVOaK
    AJ2Xc2MEjeVXdQnFZLF2FEPy/znJ0EIW/PhGgFHyQWn1inJU00FNfGQsdO8Ruloi
    VNVYOyMINZIG0V91l29XCViuSUrR1BSvK+YG2wiFqHV0bUytbY7uuNCh3S/HZEhE
    zC5KEddzKtihzW3yP4baMAmvu7atS0Gy7MLXWG+SRw==
    -----END CERTIFICATE-----
  Cert: |-
    -----BEGIN CERTIFICATE-----
    MIIDjzCCAnegAwIBAgIQB6OLOw2bXBVXZD3ArV2i+jANBgkqhkiG9w0BAQsFADAW
    MRQwEgYDVQQDDAtFYXN5LVJTQSBDQTAeFw0yMTA0MDYxMTMxMDdaFw0yNDAzMjEx
    MTMxMDdaMCUxIzAhBgNVBAMMGmZhY3RvcnljdWJlLXNlcnZlci12ZXJuZW1xMIIB
    IjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4EGKWn5ZSNtC5t3bsMjTMow3
    usMfM/1NzM9MYAvUMy71WHdCt/10bnIzRwYLUMtjAQURvyq+dXaj1DMz7452uzCC
    GE6r6SojReH7snAWMgrbDhE3psokW4LY6/tH5ihOBu1xfUtI1ac6rkHz4JsURpHj
    5HAGnPkfOiQZc10AxBxPz9i558a7J58Y4FZ0M51rgKaWAr+qDMzjfdNI/MUs1JTd
    Yn8QM0Sv2XjP4CziqWOa0Xf4MsoLgOqItZMLaYQwaUIkx0CV231D1cREZB2xbYSn
    DyK9DbXumK0tsVwgXh8Q+EY0ofW+M/noukJUxIbL8E1OKwbnS+2smpz5clc+BQID
    AQABo4HJMIHGMAkGA1UdEwQCMAAwHQYDVR0OBBYEFEPOUbp9SDAizKwHuiFhBPVH
    OKaTMFEGA1UdIwRKMEiAFL1vX0A8tQAje5J0cQKWhIdy8Vu/oRqkGDAWMRQwEgYD
    VQQDDAtFYXN5LVJTQSBDQYIUI2id6KFXw/nEv9bRhVsDhzM/ozkwEwYDVR0lBAww
    CgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMCUGA1UdEQQeMByCGmZhY3RvcnljdWJl
    LXNlcnZlci12ZXJuZW1xMA0GCSqGSIb3DQEBCwUAA4IBAQANaEiAIR+nADiGNeOk
    SSCRG5VeelFfO3X4wUiHrEKE6NkVVF+w6Wemogmtb+KB6NmjWAGiZep8p/R/7F75
    iAGUk4LEAcXFMw6NWZhODlv5ryo6V2X1EuqMwXOfccbUwu7l9md+j2UxY45vp1Z2
    +1eP/pjVmEt29HPUnWUT/rFeqlz605xYLauoSTsTxkGl5T8EoLwtvZOFnBIy7oGs
    QzW42IlfFKNlYKHMYJ8I6AQ89+cUUko/zqUPeiFl+fl1WpXCOofEiRUd/4Z6R0Rw
    moz/sNxvZ44GG47X2Gvc1Ewfc9rpgvSmyMTUgS4EP2p5LBQEDuFJz1DSW43QGOEx
    6OoQ
    -----END CERTIFICATE-----
  Privkey: |-
    -----BEGIN PRIVATE KEY-----
    MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDgQYpafllI20Lm
    3duwyNMyjDe6wx8z/U3Mz0xgC9QzLvVYd0K3/XRucjNHBgtQy2MBBRG/Kr51dqPU
    MzPvjna7MIIYTqvpKiNF4fuycBYyCtsOETemyiRbgtjr+0fmKE4G7XF9S0jVpzqu
    QfPgmxRGkePkcAac+R86JBlzXQDEHE/P2LnnxrsnnxjgVnQznWuAppYCv6oMzON9
    00j8xSzUlN1ifxAzRK/ZeM/gLOKpY5rRd/gyyguA6oi1kwtphDBpQiTHQJXbfUPV
    xERkHbFthKcPIr0Nte6YrS2xXCBeHxD4RjSh9b4z+ei6QlTEhsvwTU4rBudL7aya
    nPlyVz4FAgMBAAECggEAGr62SWSYMWA08mOzN7o7teOTIerzZbfn920hprLCc/g9
    unc+tcK8iA+GngnSf6hB/nUkULBAVrH3h+8zWnabImf+m/eF+SDVZBg3dGh5vS4e
    Qec7w+CHxYItbLdjM8fKxni6+D5qSVDQZsfm1fqoyGHN1AioQUBUwRJdFq0Y0kp0
    6BwZG2Kk43JQByeKX4/D2QgZEjFCgsw2Wihy8sQkX96aXbAdUO5Um7kzf7iIMXlI
    877uarOuCuIEYJSWjwrwu4I7CQ68lEA1pGSlEMKU9ju8KtoqaQ65ELx9PgK8df3+
    zIM8WnZzT/US0TPI+cWvsB6yYYGsohE25oGNYMnKAQKBgQD5RGOeAR4GhBm2tRR/
    6XzLWmru0sGif/7A/rPDjzWSlnkaqkTSK29xQZkDK9WJVLG3SWCV2XnwysLgVHyF
    J2DlPZVDnKvS5EXwGSenzpWMm6yHNZhsA1nxvSiZBXj8MPhF5NbOrnuZJVR9+xmA
    xQTspByMa9gwD8sXkWS+aq8+kQKBgQDmUDTfY8jtCXm9kXZPPgESGtYuDmBlvyf4
    40Gt79FmOt3ZYBr2do0BeUnyJOFwleQeUqRIneUsUHyUHj1VI/SpBuUAPWOT9w8s
    zlrLkmYnrOArKcZpCpWQZWyKXtomxdUlZJjFJGJfipJ1zu1G2IJZNjuuFpEJY4Jz
    xI3JO9OqNQKBgQC28UV3enRx7dP9b0sbUd7gVHz0yMOKowvy0zxWA64a5sr9Eggu
    g+hno/BMDojLINjJn1X+xL0lR3rdL8Sv6SXJOXhqRfspbZYS0DN1ij3iE4L9PQ2g
    AJmrvNul6wlPdhVRHtG0oQRfcrFfQMIH6xFhKFU3jhL9uymgI3pf8mapsQKBgQCV
    6ETR1mA0sQM1eVW5dFE8GI2qAp+J+mcpH7pjXYI9eTlnQJ+sJjPXbrvLyWeXYWjO
    t9nIjkVZl7XOrvQa08Fut1cc1dhXuep9rn74m/yz/FkYnyNOwZ3iE7IXSm/R5ti6
    cwTg5LLHHyn/VkWo5EnLiXpE8NlMdCG3+oqTHcbCtQKBgBoQrLzRsruiha6QP4OZ
    eCvOVIaxuH60A85QW5pgb0IlORXfX9zjA+M3v/G+tj6y5vxPOiVJaRd6iBqEDpMd
    337jNZKFr5EaVmF45PrcM060cbEcbdq8zCTEnQ6Y+bQiDTkfdgcE5CDJDJ7ol2I2
    MIzsw5gq+PcsyNV5g/IUX0PJ
    -----END PRIVATE KEY-----
  additionalEnv:
  - name: DOCKER_VERNEMQ_ALLOW_REGISTER_DURING_NETSPLIT
    value: "on"
  - name: DOCKER_VERNEMQ_ALLOW_PUBLISH_DURING_NETSPLIT
    value: "on"
  - name: DOCKER_VERNEMQ_ALLOW_SUBSCRIBE_DURING_NETSPLIT
    value: "on"
  - name: DOCKER_VERNEMQ_ALLOW_UNSUBSCRIBE_DURING_NETSPLIT
    value: "on"
  - name: DOCKER_VERNEMQ_ALLOW_ANONYMOUS
    value: "on"
  - name: DOCKER_VERNEMQ_MAX_ONLINE_MESSAGES
    value: "-1"
  - name: DOCKER_VERNEMQ_MAX_OFFLINE_MESSAGES
    value: "-1"
  - name: DOCKER_VERNEMQ_LISTENER__SSL__CAFILE
    value: /etc/ssl/vernemq/ca.crt
  - name: DOCKER_VERNEMQ_LISTENER__SSL__CERTFILE
    value: /etc/ssl/vernemq/tls.crt
  - name: DOCKER_VERNEMQ_LISTENER__SSL__KEYFILE
    value: /etc/ssl/vernemq/tls.key
  - name: DOCKER_VERNEMQ_ACCEPT_EULA
    value: "yes"
  - name: DOCKER_VERNEMQ_LISTENER__SSL__DEFAULT__USE_IDENTITY_AS_USERNAME
    value: "on"
  - name: DOCKER_VERNEMQ_LISTENER__SSL__DEFAULT__REQUIRE_CERTIFICATE
    value: "on"
  extraVolumeMounts:
  - mountPath: /vernemq/etc/vmq.acl
    name: vernemq-acl
    readOnly: true
    subPath: vernemq-acl
  extraVolumes:
  - configMap:
      name: vernemq-acl
    name: vernemq-acl
  fullnameOverride: ""
  image:
    pullPolicy: IfNotPresent
    repository: vernemq/vernemq
    tag: 1.11.0
  ingress:
    annotations: {}
    enabled: false
    hosts: []
    labels: {}
    paths:
    - /
    tls: []
  nameOverride: ""
  nodeSelector: {}
  pdb:
    enabled: true
    minAvailable: 1
  persistentVolume:
    accessModes:
    - ReadWriteOnce
    annotations: {}
    enabled: false
    size: 5Gi
  podAntiAffinity: soft
  rbac:
    create: true
    serviceAccount:
      create: true
  replicaCount: 1
  resources: {}
  secretMounts:
   - name: vernemq-certificates
     path: /etc/ssl/vernemq
     secretName: vernemq-certificates-secret
  securityContext:
    fsGroup: 10000
    runAsGroup: 10000
    runAsUser: 10000
  service:
    annotations:
      prometheus.io/port: "8888"
      prometheus.io/scrape: "true"
    labels: {}
    mqtt:
      enabled: true
      nodePort: 1883
      port: 1883
    mqtts:
      enabled: true
      nodePort: 8883
      port: 8883
    type: LoadBalancer
    ws:
      enabled: false
      nodePort: 8080
      port: 8080
    wss:
      enabled: false
      nodePort: 8443
      port: 8443
  serviceMonitor:
    create: false
  statefulset:
    annotations: {}
    labels: {}
    lifecycle: {}
    livenessProbe:
      failureThreshold: 3
      initialDelaySeconds: 90
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 5
    podAnnotations: {}
    podManagementPolicy: OrderedReady
    prometheus.io/port: "8888"
    prometheus.io/scrape: "true"
    readinessProbe:
      failureThreshold: 3
      initialDelaySeconds: 90
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 5
    terminationGracePeriodSeconds: 60
    updateStrategy: RollingUpdate
  tolerations: []

