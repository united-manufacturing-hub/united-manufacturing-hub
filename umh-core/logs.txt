go vet -tags=test ./...
nilaway -include-pkgs="github.com/united-manufacturing-hub/united-manufacturing-hub/umh-core" -tags=test ./...
ginkgo -r -v --tags=test --label-filter='integration' --fail-fast ./...
[38;5;9m[1mGinkgo detected a version mismatch between the Ginkgo CLI and the version of Ginkgo imported by your packages:[0m
  Ginkgo CLI Version:
    [1m2.22.0[0m
  Mismatched package versions found:
    [1m2.23.3[0m used by integration, fsm, backoff, api, v2, http, pull, push, encoding, hash, subscriber, tracing, watchdog, config, benthosserviceconfig, redpandaserviceconfig, control, ctxutil, benthos, container, s6, portmanager, sentry, benthos, container_monitor, nmap, redpanda, s6, starvationchecker, benthos, s6

  [38;5;243mGinkgo will continue to attempt to run but you may see errors (including flag
  parsing errors) and should either update your go.mod or your version of the
  Ginkgo CLI to match.

  To install the matching version of the CLI run
    [1mgo install github.com/onsi/ginkgo/v2/ginkgo[0m[38;5;243m
  from a path that contains a go.mod file.  Alternatively you can use
    [1mgo run github.com/onsi/ginkgo/v2/ginkgo[0m[38;5;243m
  from a path that contains a go.mod file to invoke the matching version of the
  Ginkgo CLI.

  If you are attempting to test multiple packages that each have a different
  version of the Ginkgo library with a single Ginkgo CLI that is currently
  unsupported.
  [0m
Running Suite: UMH Integration Suite - /home/scarjit/Git/united-manufacturing-hub/umh-core/integration
======================================================================================================
Random Seed: [1m1743421614[0m

Will run [1m1[0m of [1m12[0m specs
[38;5;14mS[0m[38;5;14mS[0m[38;5;14mS[0m[38;5;14mS[0m[38;5;14mS[0m[38;5;14mS[0m[38;5;14mS[0m[38;5;14mS[0m[38;5;14mS[0m
[38;5;243m------------------------------[0m
[0mUMH Container Integration [38;5;243mwith service scaling test [0m[1mshould scale up to multiple services while maintaining healthy metrics[0m [38;5;204m[integration, scaling][0m
[38;5;243m/home/scarjit/Git/united-manufacturing-hub/umh-core/integration/integration_test.go:209[0m
  [1mSTEP:[0m Starting with an empty configuration [38;5;243m@ 03/31/25 13:46:55.529[0m

=== STARTING CONTAINER BUILD AND RUN ===
Container name: umh-core-fa6e64b9
Memory limit: 1024m
Port mappings - Host:Container
- Metrics: 8818:8080
- Golden: 8819:8082
Cleaning up any previous containers with the same name...
Note: Container stop returned: exit status 125, output: Error: no container with name or ID "umh-core-fa6e64b9" found: no such container
 (this may be normal)
Building Docker image...
Core directory: /home/scarjit/Git/united-manufacturing-hub/umh-core
Dockerfile path: /home/scarjit/Git/united-manufacturing-hub/umh-core/Dockerfile
Docker build successful
Writing local config file...
Starting container...
Container started with ID: a13f5c90d4a16a75a3ac9d8dddc97daa8de7c185fa1581a41e6072382742a164
Writing config directly to container umh-core-fa6e64b9...
Container state: running
Waiting for metrics endpoint to become available...
Starting to wait for metrics at 2025-03-31T13:48:08+02:00
Container name: umh-core-fa6e64b9
Using localhost URL with host port: http://localhost:8818/metrics
Attempt 1: Connecting to metrics...
Using localhost URL with host port: http://localhost:8818/metrics
Attempt 2: Connecting to metrics...
Successfully connected to metrics endpoint (http://localhost:8818/metrics) after 1.004684567s (2 attempts)
Starting to wait for metrics at 2025-03-31T13:48:09+02:00
Container name: umh-core-fa6e64b9
Using localhost URL with host port: http://localhost:8818/metrics
Attempt 1: Connecting to metrics...
Successfully connected to metrics endpoint (http://localhost:8818/metrics) after 1.237201ms (1 attempts)
  [1mSTEP:[0m Adding the golden service as a baseline [38;5;243m@ 03/31/25 13:48:09.835[0m
Writing config directly to container umh-core-fa6e64b9...
  [1mSTEP:[0m Waiting for the golden service to become responsive [38;5;243m@ 03/31/25 13:48:10.309[0m
Using localhost URL with host port: http://localhost:8819/health
Using localhost URL with host port: http://localhost:8819/health
  [1mSTEP:[0m Scaling up by adding 10 sleep services [38;5;243m@ 03/31/25 13:48:11.33[0m
  Added service sleepy-0
Writing config directly to container umh-core-fa6e64b9...
  Added service sleepy-1
Writing config directly to container umh-core-fa6e64b9...
  Added service sleepy-2
Writing config directly to container umh-core-fa6e64b9...
  Added service sleepy-3
Writing config directly to container umh-core-fa6e64b9...
  Added service sleepy-4
Writing config directly to container umh-core-fa6e64b9...
  Added service sleepy-5
Writing config directly to container umh-core-fa6e64b9...
  Added service sleepy-6
Writing config directly to container umh-core-fa6e64b9...
  Added service sleepy-7
Writing config directly to container umh-core-fa6e64b9...
  Added service sleepy-8
Writing config directly to container umh-core-fa6e64b9...
  Added service sleepy-9
Writing config directly to container umh-core-fa6e64b9...
  [1mSTEP:[0m Simulating random stop/start actions on sleep services (chaos monkey) [38;5;243m@ 03/31/25 13:48:18.527[0m
  Chaos monkey: stoping service sleepy-5
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 15.32 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 2.00 ms
    0.9 quantile: 14.00 ms
    0.95 quantile: 21.00 ms
    0.99 quantile: 30.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 22
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 30
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-0
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 20.46 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 2.00 ms
    0.9 quantile: 16.00 ms
    0.95 quantile: 26.00 ms
    0.99 quantile: 50.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 50
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-7
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 17.16 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 2.00 ms
    0.9 quantile: 15.00 ms
    0.95 quantile: 26.00 ms
    0.99 quantile: 50.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 50
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-3
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 20.93 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 2.00 ms
    0.9 quantile: 21.00 ms
    0.95 quantile: 28.00 ms
    0.99 quantile: 50.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 32
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 32
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 32
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 22
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 50
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 32
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 30
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-5
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 29.90 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 2.00 ms
    0.9 quantile: 23.00 ms
    0.95 quantile: 30.00 ms
    0.99 quantile: 50.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 32
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 32
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 32
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 22
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 50
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 32
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 30
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-3
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 21.02 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 2.00 ms
    0.9 quantile: 22.00 ms
    0.95 quantile: 28.00 ms
    0.99 quantile: 50.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 32
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 32
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 32
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 22
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 50
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 32
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 30
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-5
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 22.65 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 26.00 ms
    0.95 quantile: 34.00 ms
    0.99 quantile: 57.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 31
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 57
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 30
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-4
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 28.98 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 26.00 ms
    0.95 quantile: 32.00 ms
    0.99 quantile: 57.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 31
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 57
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 30
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-4
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 16.74 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 26.00 ms
    0.95 quantile: 34.00 ms
    0.99 quantile: 57.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 31
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 57
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-9
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 23.27 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 26.00 ms
    0.95 quantile: 34.00 ms
    0.99 quantile: 50.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 50
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-2
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 31.88 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 26.00 ms
    0.95 quantile: 34.00 ms
    0.99 quantile: 50.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 50
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-0
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 17.41 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 26.00 ms
    0.95 quantile: 34.00 ms
    0.99 quantile: 55.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 55
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-4
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 21.96 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 26.00 ms
    0.95 quantile: 34.00 ms
    0.99 quantile: 55.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 55
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-7
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 18.77 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 25.00 ms
    0.95 quantile: 32.00 ms
    0.99 quantile: 55.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 55
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-4
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 17.33 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 26.00 ms
    0.95 quantile: 34.00 ms
    0.99 quantile: 55.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 55
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-2
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 23.58 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 26.00 ms
    0.95 quantile: 34.00 ms
    0.99 quantile: 55.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 55
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-9
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 25.94 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 26.00 ms
    0.95 quantile: 34.00 ms
    0.99 quantile: 55.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 55
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-2
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 23.12 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 26.00 ms
    0.95 quantile: 34.00 ms
    0.99 quantile: 55.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 31
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 55
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-2
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 17.68 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 26.00 ms
    0.95 quantile: 34.00 ms
    0.99 quantile: 55.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 31
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 55
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-1
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 28.92 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 26.00 ms
    0.95 quantile: 34.00 ms
    0.99 quantile: 50.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 50
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 30
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-4
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 31.90 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 26.00 ms
    0.95 quantile: 34.00 ms
    0.99 quantile: 51.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 51
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 30
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-6
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 18.62 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 26.00 ms
    0.95 quantile: 34.00 ms
    0.99 quantile: 51.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 28
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 51
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 30
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-6
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 25.36 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 27.00 ms
    0.95 quantile: 34.00 ms
    0.99 quantile: 51.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 51
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 30
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-5
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 25.31 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 27.00 ms
    0.95 quantile: 34.00 ms
    0.99 quantile: 51.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 51
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 30
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-0
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 23.79 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 28.00 ms
    0.95 quantile: 38.00 ms
    0.99 quantile: 53.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 31
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 53
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 30
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-4
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 24.16 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 27.00 ms
    0.95 quantile: 37.00 ms
    0.99 quantile: 53.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 31
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 53
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 25
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-3
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 30.72 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 28.00 ms
    0.95 quantile: 38.00 ms
    0.99 quantile: 53.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 31
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 53
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 25
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-3
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 20.16 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 28.00 ms
    0.95 quantile: 37.00 ms
    0.99 quantile: 51.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 51
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 25
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-6
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 31.41 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 28.00 ms
    0.95 quantile: 37.00 ms
    0.99 quantile: 51.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 51
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 25
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-6
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 21.46 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 40.00 ms
    0.99 quantile: 51.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 51
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 25
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-2
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 25.59 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 29.00 ms
    0.95 quantile: 38.00 ms
    0.99 quantile: 51.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 51
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 25
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-7
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 29.09 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 29.00 ms
    0.95 quantile: 38.00 ms
    0.99 quantile: 51.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 51
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 25
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-0
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 28.43 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 38.00 ms
    0.99 quantile: 51.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 51
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 25
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-2
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 20.45 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 29.00 ms
    0.95 quantile: 38.00 ms
    0.99 quantile: 51.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 51
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 25
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-8
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 31.45 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 29.00 ms
    0.95 quantile: 38.00 ms
    0.99 quantile: 51.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 51
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 25
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-8
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 27.24 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 40.00 ms
    0.99 quantile: 55.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 55
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 25
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-0
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 19.85 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 40.00 ms
    0.99 quantile: 55.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 26
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 55
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 25
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-1
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 24.19 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 59.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 59
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 25
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-9
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 19.95 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 59.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 59
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 23
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-3
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 34.50 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 59.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 59
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 23
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 23
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-3
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 22.27 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 59.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 59
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 23
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-8
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 27.05 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 59.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 59
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 23
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-5
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 30.16 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 59.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 59
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 23
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-4
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 23.10 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 59.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 59
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 23
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-4
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 26.49 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 59.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 59
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 23
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-7
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 35.70 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 59.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 59
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 23
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-3
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 24.51 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 59.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 59
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 23
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-5
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 24.73 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 61.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 61
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 23
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-1
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 23.49 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 61.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 61
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 23
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-3
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 26.40 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 61.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 61
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 23
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-9
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 34.22 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 61.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 61
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 23
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-7
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 32.01 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 61.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 61
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 23
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-2
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 34.31 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 61.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 61
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-3
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 31.33 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 61.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 61
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-2
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 28.54 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 61.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 61
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-6
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 28.65 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 61.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 61
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-9
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 39.00 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 61.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 61
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-3
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 29.52 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 30.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 61.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 61
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-7
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 44.63 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 31.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 61.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 61
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-8
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 29.50 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 31.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 61.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 61
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-9
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 26.80 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 31.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 61.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 61
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-0
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 35.78 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 31.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 61.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 30
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 61
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-9
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 43.95 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-0
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 25.43 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-0
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 37.11 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 31.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-7
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 28.87 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-6
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 29.33 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-0
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 25.24 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-8
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 34.40 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 4.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-0
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 38.46 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 4.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-7
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 42.59 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-1
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 30.36 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 4.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-6
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 36.74 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 4.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-4
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 42.08 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-2
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 31.89 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 4.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-5
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 44.48 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 4.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-5
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 29.27 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 3.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 41.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-0
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 25.72 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 4.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-7
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 40.08 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 4.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-3
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 33.46 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 4.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-5
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 26.50 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 4.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 41.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-7
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 32.34 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 4.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 41.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-8
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 37.54 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 4.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 41.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-8
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 28.51 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 4.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 41.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-0
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 36.74 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 4.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 41.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-9
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 25.85 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 4.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 41.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: starting service sleepy-0
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 33.08 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 4.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 41.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-2
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 44.24 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 4.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 41.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-5
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 47.88 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  âœ“ No errors found above limit

  Control loop reconcile time quantiles:
    0.5 quantile: 4.00 ms
    0.9 quantile: 32.00 ms
    0.95 quantile: 43.00 ms
    0.99 quantile: 75.00 ms

  Reconcile p99 durations over 20.0 ms:
    umh_core_reconcile_duration_milliseconds{component="ConfigManager",instance="get_config",quantile="0.99"} 40
    umh_core_reconcile_duration_milliseconds{component="RedpandaManager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="base_fsm_manager",instance="S6ManagerCore",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="control_loop",instance="main",quantile="0.99"} 75
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda",quantile="0.99"} 42
    umh_core_reconcile_duration_milliseconds{component="redpanda_instance",instance="redpanda.reconcileExternalChanges",quantile="0.99"} 42
  âœ… Metrics are healthy
Using localhost URL with host port: http://localhost:8819/health
  âœ… Golden service is running
  Chaos monkey: stoping service sleepy-7
Writing config directly to container umh-core-fa6e64b9...
Using localhost URL with host port: http://localhost:8818/metrics
  âœ“ Memory: 34.09 MB (limit: 512.00 MB)
  âœ“ Starved seconds: 0.00 (limit: 0)
  [38;5;9m[FAILED][0m in [It] - /home/scarjit/Git/united-manufacturing-hub/umh-core/integration/metrics_parsing.go:155 [38;5;243m@ 03/31/25 13:49:51.354[0m
  [1mSTEP:[0m Stopping the container after the scaling test [38;5;243m@ 03/31/25 13:49:51.354[0m
Test failed, printing container logs:
Test failed, printing container logs:

=== DOCKER CONTAINER LOGS ===
s6-rc: info: service s6rc-fdholder: starting
s6-rc: info: service s6rc-oneshot-runner: starting
s6-rc: info: service s6rc-oneshot-runner successfully started
s6-rc: info: service fix-attrs: starting
s6-rc: info: service s6rc-fdholder successfully started
s6-rc: info: service fix-attrs successfully started
s6-rc: info: service legacy-cont-init: starting
s6-rc: info: service legacy-cont-init successfully started
s6-rc: info: service umh-core-log-prepare: starting
s6-rc: info: service syslogd-prepare: starting
s6-rc: info: service umh-core-log-prepare successfully started
s6-rc: info: service umh-core-log: starting
s6-rc: info: service umh-core-log successfully started
s6-rc: info: service umh-core: starting
s6-rc: info: service umh-core successfully started
s6-rc: info: service syslogd-prepare successfully started
s6-rc: info: service syslogd-log: starting
s6-rc: info: service syslogd-log successfully started
s6-rc: info: service syslogd: starting
s6-rc: info: service syslogd successfully started
s6-rc: info: service legacy-services: starting
s6-rc: info: service legacy-services successfully started


=== UMH CORE INTERNAL LOGS ===
2025-03-31 11:49:50.842021374  3,597 [shard  0:main] main - application.cc:849 - redpanda.space_management_max_segment_concurrency:10	- Maximum parallel segments inspected during space management process.} {Timestamp:2025-03-31 11:49:43.597754403 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.storage_compaction_index_memory:134217728	- Maximum number of bytes that may be used on each shard by compaction index writers.} {Timestamp:2025-03-31 11:49:43.597755725 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.storage_compaction_key_map_memory:134217728	- Maximum number of bytes that may be used on each shard by compaction key-offset maps. Only applies when `log_compaction_use_sliding_window` is set to `true`.} {Timestamp:2025-03-31 11:49:43.597757619 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.storage_compaction_key_map_memory_limit_percent:12	- Limit on `storage_compaction_key_map_memory`, expressed as a percentage of memory per shard, that bounds the amount of memory used by compaction key-offset maps. Memory per shard is computed after `data_transforms_per_core_memory_reservation`, and only applies when `log_compaction_use_sliding_window` is set to `true`.} {Timestamp:2025-03-31 11:49:43.597758641 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.storage_ignore_cstore_hints:0	- When set, cstore hints are ignored and not used for data access (but are otherwise generated).} {Timestamp:2025-03-31 11:49:43.597798255 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.storage_ignore_timestamps_in_future_sec:{nullopt}	- The maximum number of seconds that a record's timestamp can be ahead of a Redpanda broker's clock and still be used when deciding whether to clean up the record for data retention. This property makes possible the timely cleanup of records from clients with clocks that are drastically unsynchronized relative to Redpanda. When determining whether to clean up a record with timestamp more than `storage_ignore_timestamps_in_future_sec` seconds ahead of the broker, Redpanda ignores the record's timestamp and instead uses a valid timestamp of another record in the same segment, or (if another record's valid timestamp is unavailable) the timestamp of when the segment file was last modified (mtime). By default, `storage_ignore_timestamps_in_future_sec` is disabled (null). To figure out whether to set `storage_ignore_timestamps_in_future_sec` for your system: . Look for logs with segments that are unexpectedly large and not being cleaned up. . In the logs, search for records with unsynchronized timestamps that are further into the future than tolerable by your data retention and storage settings. For example, timestamps 60 seconds or more into the future can be considered to be too unsynchronized. . If you find unsynchronized timestamps throughout your logs, determine the number of seconds that the timestamps are ahead of their actual time, and set `storage_ignore_timestamps_in_future_sec` to that value so data retention can proceed. . If you only find unsynchronized timestamps that are the result of transient behavior, you can disable `storage_ignore_timestamps_in_future_sec`.} {Timestamp:2025-03-31 11:49:43.59780082 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.storage_max_concurrent_replay:1024	- Maximum number of partitions' logs that will be replayed concurrently at startup, or flushed concurrently on shutdown.} {Timestamp:2025-03-31 11:49:43.597802834 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.storage_min_free_bytes:5368709120	- Threshold of minimum bytes free space before rejecting producers.} {Timestamp:2025-03-31 11:49:43.597804777 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.storage_read_buffer_size:131072	- Size of each read buffer (one per in-flight read, per log segment).} {Timestamp:2025-03-31 11:49:43.597806741 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.storage_read_readahead_count:10	- How many additional reads to issue ahead of current read location.} {Timestamp:2025-03-31 11:49:43.597810408 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.storage_reserve_min_segments:2	- The number of segments per partition that the system will attempt to reserve disk capacity for. For example, if the maximum segment size is configured to be 100 MB, and the value of this option is 2, then in a system with 10 partitions Redpanda will attempt to reserve at least 2 GB of disk space.} {Timestamp:2025-03-31 11:49:43.597812492 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.storage_space_alert_free_threshold_bytes:0	- Threshold of minimum bytes free space before setting storage space alert.} {Timestamp:2025-03-31 11:49:43.597814596 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.storage_space_alert_free_threshold_percent:5	- Threshold of minimum percent free space before setting storage space alert.} {Timestamp:2025-03-31 11:49:43.597817531 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.storage_strict_data_init:0	- Requires that an empty file named `.redpanda_data_dir` be present in the broker configuration `data_directory`. If set to `true`, Redpanda will refuse to start if the file is not found in the data directory.} {Timestamp:2025-03-31 11:49:43.597819515 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.storage_target_replay_bytes:10737418240	- Target bytes to replay from disk on startup after clean shutdown: controls frequency of snapshots and checkpoints.} {Timestamp:2025-03-31 11:49:43.597820717 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.superusers:{}	- List of superuser usernames.} {Timestamp:2025-03-31 11:49:43.597822401 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.target_fetch_quota_byte_rate:{nullopt}	- Target fetch size quota byte rate (bytes per second) - disabled default} {Timestamp:2025-03-31 11:49:43.597824064 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.target_quota_byte_rate:0	- Target request size quota byte rate (bytes per second)} {Timestamp:2025-03-31 11:49:43.59782746 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.tls_enable_renegotiation:0	- TLS client-initiated renegotiation is considered unsafe and is by default disabled.  Only re-enable it if you are experiencing issues with your TLS-enabled client.  This option has no effect on TLSv1.3 connections as client-initiated renegotiation was removed.} {Timestamp:2025-03-31 11:49:43.597830185 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.tls_min_version:v1.2	- The minimum TLS version that Redpanda clusters support. This property prevents client applications from negotiating a downgrade to the TLS version when they make a connection to a Redpanda cluster.} {Timestamp:2025-03-31 11:49:43.597832289 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.tm_sync_timeout_ms:10000	- Transaction manager's synchronization timeout. Maximum time to wait for internal state machine to catch up before rejecting a request.} {Timestamp:2025-03-31 11:49:43.597833571 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.tm_violation_recovery_policy:	- } {Timestamp:2025-03-31 11:49:43.597836287 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.tombstone_retention_
2025-03-31 11:49:50.842114529  ms:{nullopt}	- The retention time for tombstone records in a compacted topic. Cannot be enabled at the same time as any of `cloud_storage_enabled`, `cloud_storage_enable_remote_read`, or `cloud_storage_enable_remote_write`.} {Timestamp:2025-03-31 11:49:43.59783789 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.topic_fds_per_partition:{5}	- Required file handles per partition when creating topics.} {Timestamp:2025-03-31 11:49:43.597839453 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.topic_memory_per_partition:{4194304}	- Required memory per partition when creating topics.} {Timestamp:2025-03-31 11:49:43.597841276 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.topic_partitions_per_shard:1000	- Maximum number of partitions which may be allocated to one shard (CPU core).} {Timestamp:2025-03-31 11:49:43.59784364 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.topic_partitions_reserve_shard0:0	- Reserved partition slots on shard (CPU core) 0 on each node.  If this is >= topic_partitions_per_shard, no data partitions will be scheduled on shard 0} {Timestamp:2025-03-31 11:49:43.597845855 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.transaction_coordinator_cleanup_policy:delete	- Cleanup policy for a transaction coordinator topic. Accepted Values: `compact`, `delete`, `["compact","delete"]`, `none`} {Timestamp:2025-03-31 11:49:43.59784869 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.transaction_coordinator_delete_retention_ms:604800000	- Delete segments older than this age. To ensure transaction state is retained as long as the longest-running transaction, make sure this is no less than `transactional_id_expiration_ms`.} {Timestamp:2025-03-31 11:49:43.597850423 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.transaction_coordinator_log_segment_size:1073741824	- The size (in bytes) each log segment should be.} {Timestamp:2025-03-31 11:49:43.597852006 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.transaction_coordinator_partitions:50	- Number of partitions for transactions coordinator.} {Timestamp:2025-03-31 11:49:43.597853298 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.transaction_coordinator_replication:	- } {Timestamp:2025-03-31 11:49:43.597856535 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.transaction_max_timeout_ms:900000	- The maximum allowed timeout for transactions. If a client-requested transaction timeout exceeds this configuration, the broker returns an error during transactional producer initialization. This guardrail prevents hanging transactions from blocking consumer progress.} {Timestamp:2025-03-31 11:49:43.59787523 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.transactional_id_expiration_ms:604800000	- Expiration time of producer IDs. Measured starting from the time of the last write until now for a given ID.} {Timestamp:2025-03-31 11:49:43.597877013 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.tx_log_stats_interval_s:10000	- How often to log per partition tx stats, works only with debug logging enabled.} {Timestamp:2025-03-31 11:49:43.597878285 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.tx_registry_log_capacity:	- } {Timestamp:2025-03-31 11:49:43.597879488 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.tx_registry_sync_timeout_ms:	- } {Timestamp:2025-03-31 11:49:43.597881281 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.tx_timeout_delay_ms:1000	- Delay before scheduling the next check for timed out transactions.} {Timestamp:2025-03-31 11:49:43.597884066 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.unsafe_enable_consumer_offsets_delete_retention:0	- Enables delete retention of consumer offsets topic. This is an internal-only configuration and should be enabled only after consulting with Redpanda support.} {Timestamp:2025-03-31 11:49:43.59788606 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.usage_disk_persistance_interval_sec:300000	- The interval in which all usage stats are written to disk.} {Timestamp:2025-03-31 11:49:43.597887823 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.usage_num_windows:24	- The number of windows to persist in memory and disk.} {Timestamp:2025-03-31 11:49:43.597890057 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.usage_window_width_interval_sec:3600000	- The width of a usage window, tracking cloud and kafka ingress/egress traffic each interval.} {Timestamp:2025-03-31 11:49:43.597891881 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.use_fetch_scheduler_group:1	- Use a separate scheduler group for fetch processing.} {Timestamp:2025-03-31 11:49:43.597893223 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.use_scheduling_groups:	- } {Timestamp:2025-03-31 11:49:43.597895237 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.virtual_cluster_min_producer_ids:18446744073709551615	- Minimum number of active producers per virtual cluster.} {Timestamp:2025-03-31 11:49:43.597897071 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.wait_for_leader_timeout_ms:5000	- Timeout to wait for leadership in metadata cache.} {Timestamp:2025-03-31 11:49:43.597903583 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.write_caching_default:false	- The default write caching mode to apply to user topics. Write caching acknowledges a message as soon as it is received and acknowledged on a majority of brokers, without waiting for it to be written to disk. With `acks=all`, this provides lower latency while still ensuring that a majority of brokers acknowledge the write. Fsyncs follow `raft_replica_max_pending_flush_bytes` and `raft_replica_max_flush_delay_ms`, whichever is reached first. The `write_caching_default` cluster property can be overridden with the `write.caching` topic property. Accepted values: * `true` * `false` * `disabled`: This takes precedence over topic overrides and disables write caching for the entire cluster.} {Timestamp:2025-03-31 11:49:43.597905416 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.zstd_decompress_workspace_bytes:8388608	- Size of the zstd decompression workspace.} {Timestamp:2025-03-31 11:49:43.597906769 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:900 - Node configuration properties:} {Timestamp:2025-03-31 11:49:43.597908262 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:901 - (use `rpk redpanda config set <cfg> <value>` to change)} {Timestamp:2025-03-31 11:49:43.597923771 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.admin:{{:{host: 0.0.0.0, port: 9644}}}	- Network address for the Admin API[] server.} {Timestamp:2025-03-31 11:49:43.597925805 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.admin_api_doc_dir:/usr/share/redpanda/admin-api-doc	- Path to the API specifications for the Admin API.} {Timestamp:2025-03-31 11:49:43.597927548 +0000 UTC Content:INFO  2025-03-31 11:49:43,59
2025-03-31 11:49:50.842178640  7 [shard  0:main] main - application.cc:849 - redpanda.admin_api_tls:{}	- Specifies the TLS configuration for the HTTP Admin API.} {Timestamp:2025-03-31 11:49:43.597929521 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.advertised_kafka_api:{{:{host: 127.0.0.1, port: 9092}}}	- Address of Kafka API published to the clients} {Timestamp:2025-03-31 11:49:43.597931565 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.advertised_rpc_api:{{host: 127.0.0.1, port: 33145}}	- Address of RPC endpoint published to other cluster members} {Timestamp:2025-03-31 11:49:43.597933699 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_directory:{nullopt}	- Directory for archival cache. Should be present when `cloud_storage_enabled` is present} {Timestamp:2025-03-31 11:49:43.597935903 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_hash_path_directory:{nullopt}	- Directory to store inventory report hashes for use by cloud storage scrubber} {Timestamp:2025-03-31 11:49:43.597937286 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.coproc_supervisor_server:	- } {Timestamp:2025-03-31 11:49:43.597941033 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.crash_loop_limit:{5}	- A limit on the number of consecutive times a broker can crash within one hour before its crash-tracking logic is reset. This limit prevents a broker from getting stuck in an infinite cycle of crashes. For more information see https://docs.redpanda.com/current/reference/properties/broker-properties/#crash_loop_limit.} {Timestamp:2025-03-31 11:49:43.597944229 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.crash_loop_sleep_sec:{nullopt}	- The amount of time the broker sleeps before terminating the process when it reaches the number of consecutive times a broker can crash. For more information, see https://docs.redpanda.com/current/reference/properties/broker-properties/#crash_loop_limit.} {Timestamp:2025-03-31 11:49:43.597945471 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.dashboard_dir:	- } {Timestamp:2025-03-31 11:49:43.597947495 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.data_directory:{data_directory="/data/redpanda"}	- Path to the directory for storing Redpanda's streaming data files.} {Timestamp:2025-03-31 11:49:43.597949409 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.developer_mode:1	- Skips most of the checks performed at startup, not recomended for production use} {Timestamp:2025-03-31 11:49:43.597951833 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.emergency_disable_data_transforms:0	- Override the cluster property `data_transforms_enabled` and disable Wasm-powered data transforms. This is an emergency shutoff button.} {Timestamp:2025-03-31 11:49:43.59795552 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.empty_seed_starts_cluster:1	- Controls how a new cluster is formed. All brokers in a cluster must have the same value. See how the `empty_seed_starts_cluster` setting works with the `seed_servers` setting to form a cluster. For backward compatibility, `true` is the default. Redpanda recommends using `false` in production environments to prevent accidental cluster formation.} {Timestamp:2025-03-31 11:49:43.597956743 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.enable_central_config:	- } {Timestamp:2025-03-31 11:49:43.597962694 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.fips_mode:disabled	- Controls whether Redpanda starts in FIPS mode.  This property allows for three values: `disabled` - Redpanda does not start in FIPS mode. `permissive` - Redpanda performs the same check as enabled, but a warning is logged, and Redpanda continues to run. Redpanda loads the OpenSSL FIPS provider into the OpenSSL library. After this completes, Redpanda is operating in FIPS mode, which means that the TLS cipher suites available to users are limited to the TLSv1.2 and TLSv1.3 NIST-approved cryptographic methods. `enabled` - Redpanda verifies that the operating system is enabled for FIPS by checking `/proc/sys/crypto/fips_enabled`. If the file does not exist or does not return `1`, Redpanda immediately exits.} {Timestamp:2025-03-31 11:49:43.597964567 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.kafka_api:{{:{host: 0.0.0.0, port: 9092}:{nullopt}}}	- IP address and port of the Kafka API endpoint that handles requests.} {Timestamp:2025-03-31 11:49:43.597979946 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.kafka_api_tls:{}	- Transport Layer Security (TLS) configuration for the Kafka API endpoint.} {Timestamp:2025-03-31 11:49:43.59798223 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.memory_allocation_warning_threshold:{131073}	- Threshold for log messages that contain a larger memory allocation than specified.} {Timestamp:2025-03-31 11:49:43.597985416 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.node_id:{nullopt}	- A number that uniquely identifies the broker within the cluster. If `null` (the default value), Redpanda automatically assigns an ID. If set, it must be non-negative value. The `node_id` property must not be changed after a broker joins the cluster.} {Timestamp:2025-03-31 11:49:43.597988482 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.node_id_overrides:{}	- List of node ID and UUID overrides to be applied at broker startup. Each entry includes the current UUID and desired ID and UUID. Each entry applies to a given node if and only if 'current' matches that node's current UUID.} {Timestamp:2025-03-31 11:49:43.597990666 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.openssl_config_file:{nullopt}	- Path to the configuration file used by OpenSSL to properly load the FIPS-compliant module.} {Timestamp:2025-03-31 11:49:43.597993031 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.openssl_module_directory:{nullopt}	- Path to the directory that contains the OpenSSL FIPS-compliant module. The filename that Redpanda looks for is `fips.so`.} {Timestamp:2025-03-31 11:49:43.597996106 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.rack:{nullopt}	- A label that identifies a failure zone. Apply the same label to all brokers in the same failure zone. When `enable_rack_awareness` is set to `true` at the cluster level, the system uses the rack labels to spread partition replicas across different failure zones.} {Timestamp:2025-03-31 11:49:43.59799818 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.recovery_mode_enabled:0	- If `true`, start Redpanda in recovery mode, where user partitions are not loaded and only administrative operations are allowed.} {Timestamp:2025-03-31 11:49:43.597999974 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.rpc_server:{host: 0.0.0.0, port: 33145}	- IP address and port for the Remote Procedure Call (RPC) server.} {Timestamp:2025-03-31 11:49:43.598002248 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.rpc_server_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} crl file: {nullopt} client_auth_required: 0 }	- 
2025-03-31 11:49:50.842240656  TLS configuration for the RPC server.} {Timestamp:2025-03-31 11:49:43.598013239 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.seed_servers:{}	- List of the seed servers used to join current cluster. If the `seed_servers` list is empty the node will be a cluster root and it will form a new cluster. When `empty_seed_starts_cluster` is `true`, Redpanda enables one broker with an empty `seed_servers` list to initiate a new cluster. The broker with an empty `seed_servers` becomes the cluster root, to which other brokers must connect to join the cluster.  Brokers looking to join the cluster should have their `seed_servers` populated with the cluster root's address, facilitating their connection to the cluster. Only one broker, the designated cluster root, should have an empty `seed_servers` list during the initial cluster bootstrapping. This ensures a single initiation point for cluster formation. When `empty_seed_starts_cluster` is `false`, Redpanda requires all brokers to start with a known set of brokers listed in `seed_servers`. The `seed_servers` list must not be empty and should be identical across these initial seed brokers, containing the addresses of all seed brokers. Brokers not included in the `seed_servers` list use it to discover and join the cluster, allowing for expansion beyond the foundational members. The `seed_servers` list must be consistent across all seed brokers to prevent cluster fragmentation and ensure stable cluster formation.} {Timestamp:2025-03-31 11:49:43.598015443 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.storage_failure_injection_config_path:{nullopt}	- Path to the configuration file used for low level storage failure injection.} {Timestamp:2025-03-31 11:49:43.598017677 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.storage_failure_injection_enabled:0	- If `true`, inject low level storage failures on the write path. Do _not_ use for production instances.} {Timestamp:2025-03-31 11:49:43.598019881 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.upgrade_override_checks:0	- Whether to violate safety checks when starting a Redpanda version newer than the cluster's consensus version.} {Timestamp:2025-03-31 11:49:43.598023037 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - redpanda.verbose_logging_timeout_sec_max:{nullopt}	- Maximum duration in seconds for verbose (`TRACE` or `DEBUG`) logging. Values configured above this will be clamped. If null (the default) there is no limit. Can be overridden in the Admin API on a per-request basis.} {Timestamp:2025-03-31 11:49:43.598025011 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - pandaproxy.advertised_pandaproxy_api:{}	- Network address for the HTTP Proxy API server to publish to clients.} {Timestamp:2025-03-31 11:49:43.5980409 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - pandaproxy.api_doc_dir:/usr/share/redpanda/proxy-api-doc	- Path to the API specifications for the HTTP Proxy API.} {Timestamp:2025-03-31 11:49:43.598044617 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - pandaproxy.client_cache_max_size:10	- The maximum number of Kafka client connections that Redpanda can cache in the LRU (least recently used) cache. The LRU cache helps optimize resource utilization by keeping the most recently used clients in memory, facilitating quicker reconnections for frequent clients while limiting memory usage.} {Timestamp:2025-03-31 11:49:43.598046541 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - pandaproxy.client_keep_alive:300000	- Time, in milliseconds, that an idle client connection may remain open to the HTTP Proxy API.} {Timestamp:2025-03-31 11:49:43.598048695 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - pandaproxy.consumer_instance_timeout_ms:300000	- How long to wait for an idle consumer before removing it. A consumer is considered idle when it's not making requests or heartbeats.} {Timestamp:2025-03-31 11:49:43.598050328 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - pandaproxy.pandaproxy_api:{{:{host: 0.0.0.0, port: 8082}:<nullopt>}}	- Rest API listen address and port} {Timestamp:2025-03-31 11:49:43.598051831 +0000 UTC Content:INFO  2025-03-31 11:49:43,597 [shard  0:main] main - application.cc:849 - pandaproxy.pandaproxy_api_tls:{}	- TLS configuration for Pandaproxy api.} {Timestamp:2025-03-31 11:49:43.598054366 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.broker_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} crl file: {nullopt} client_auth_required: 0 }	- TLS configuration for the Kafka API servers to which the HTTP Proxy client should connect.} {Timestamp:2025-03-31 11:49:43.59805653 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.brokers:{{host: 0.0.0.0, port: 9092}}	- Network addresses of the Kafka API servers to which the HTTP Proxy client should connect.} {Timestamp:2025-03-31 11:49:43.598059175 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.client_identifier:{pandaproxy_client}	- Custom identifier to include in the Kafka request header for the HTTP Proxy client. This identifier can help debug or monitor client activities.} {Timestamp:2025-03-31 11:49:43.598061148 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.consumer_heartbeat_interval_ms:500	- Interval (in milliseconds) for consumer heartbeats.} {Timestamp:2025-03-31 11:49:43.598063092 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.consumer_rebalance_timeout_ms:2000	- Timeout (in milliseconds) for consumer rebalance.} {Timestamp:2025-03-31 11:49:43.598064895 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.consumer_request_max_bytes:1048576	- Maximum bytes to fetch per request.} {Timestamp:2025-03-31 11:49:43.598066639 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.consumer_request_min_bytes:1	- Minimum bytes to fetch per request.} {Timestamp:2025-03-31 11:49:43.598068572 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.consumer_request_timeout_ms:100	- Interval (in milliseconds) for consumer request timeout.} {Timestamp:2025-03-31 11:49:43.598070426 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.consumer_session_timeout_ms:300000	- Timeout (in milliseconds) for consumer session.} {Timestamp:2025-03-31 11:49:43.59807263 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.produce_ack_level:-1	- Number of acknowledgments the producer requires the leader to have received before considering a request complete.} {Timestamp:2025-03-31 11:49:43.598074403 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.produce_batch_delay_ms:100	- Delay (in milliseconds) to wait before sending batch.} {Timestamp:2025-03-31 11:49:43.598076217 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.produce_batch_record_count:1000	- Number of records to batch before sending to broker.} {Timestamp:2025-03-31 11:49:43.59807805 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.produce_batch_size_bytes:1048576	- Number of bytes to batch before sending to broker.} {Timestamp:2025-03-31 11:49:43.598080635 +0000 UTC Conte
2025-03-31 11:49:50.842325425  nt:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.produce_compression_type:none	- Enable or disable compression by the Kafka client. Specify `none` to disable compression or one of the supported types [gzip, snappy, lz4, zstd].} {Timestamp:2025-03-31 11:49:43.598082759 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.produce_shutdown_delay_ms:0	- Delay (in milliseconds) to allow for final flush of buffers before shutting down.} {Timestamp:2025-03-31 11:49:43.598084432 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.retries:5	- Number of times to retry a request to a broker.} {Timestamp:2025-03-31 11:49:43.598117614 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.retry_base_backoff_ms:100	- Delay (in milliseconds) for initial retry backoff.} {Timestamp:2025-03-31 11:49:43.598119628 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.sasl_mechanism:	- The SASL mechanism to use when connecting.} {Timestamp:2025-03-31 11:49:43.598121412 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.scram_password:	- Password to use for SCRAM authentication mechanisms.} {Timestamp:2025-03-31 11:49:43.598123195 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - pandaproxy_client.scram_username:	- Username to use for SCRAM authentication mechanisms.} {Timestamp:2025-03-31 11:49:43.598124928 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry.api_doc_dir:/usr/share/redpanda/proxy-api-doc	- API doc directory} {Timestamp:2025-03-31 11:49:43.598128485 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry.mode_mutability:1	- Enable modifications to the read-only `mode` of the Schema Registry.When set to `true`, the entire Schema Registry or its subjects can be switched to `READONLY` or `READWRITE`. This property is useful for preventing unwanted changes to the entire Schema Registry or specific subjects.} {Timestamp:2025-03-31 11:49:43.598130589 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry.schema_registry_api:{{:{host: 0.0.0.0, port: 8081}:<nullopt>}}	- Schema Registry API listener address and port} {Timestamp:2025-03-31 11:49:43.598132382 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry.schema_registry_api_tls:{}	- TLS configuration for Schema Registry API.} {Timestamp:2025-03-31 11:49:43.598134576 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry.schema_registry_replication_factor:{nullopt}	- Replication factor for internal `_schemas` topic.  If unset, defaults to `default_topic_replication`.} {Timestamp:2025-03-31 11:49:43.598137291 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.broker_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} crl file: {nullopt} client_auth_required: 0 }	- TLS configuration for the Kafka API servers to which the HTTP Proxy client should connect.} {Timestamp:2025-03-31 11:49:43.598139405 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.brokers:{{host: 0.0.0.0, port: 9092}}	- Network addresses of the Kafka API servers to which the HTTP Proxy client should connect.} {Timestamp:2025-03-31 11:49:43.59814209 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.client_identifier:{schema_registry_client}	- Custom identifier to include in the Kafka request header for the HTTP Proxy client. This identifier can help debug or monitor client activities.} {Timestamp:2025-03-31 11:49:43.598143984 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.consumer_heartbeat_interval_ms:500	- Interval (in milliseconds) for consumer heartbeats.} {Timestamp:2025-03-31 11:49:43.598145857 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.consumer_rebalance_timeout_ms:2000	- Timeout (in milliseconds) for consumer rebalance.} {Timestamp:2025-03-31 11:49:43.598147671 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.consumer_request_max_bytes:1048576	- Maximum bytes to fetch per request.} {Timestamp:2025-03-31 11:49:43.598149444 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.consumer_request_min_bytes:1	- Minimum bytes to fetch per request.} {Timestamp:2025-03-31 11:49:43.598151448 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.consumer_request_timeout_ms:100	- Interval (in milliseconds) for consumer request timeout.} {Timestamp:2025-03-31 11:49:43.598153381 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.consumer_session_timeout_ms:10000	- Timeout (in milliseconds) for consumer session.} {Timestamp:2025-03-31 11:49:43.598155706 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.produce_ack_level:-1	- Number of acknowledgments the producer requires the leader to have received before considering a request complete.} {Timestamp:2025-03-31 11:49:43.598157609 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.produce_batch_delay_ms:0	- Delay (in milliseconds) to wait before sending batch.} {Timestamp:2025-03-31 11:49:43.598174231 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.produce_batch_record_count:0	- Number of records to batch before sending to broker.} {Timestamp:2025-03-31 11:49:43.598176104 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.produce_batch_size_bytes:0	- Number of bytes to batch before sending to broker.} {Timestamp:2025-03-31 11:49:43.598178739 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.produce_compression_type:none	- Enable or disable compression by the Kafka client. Specify `none` to disable compression or one of the supported types [gzip, snappy, lz4, zstd].} {Timestamp:2025-03-31 11:49:43.598180863 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.produce_shutdown_delay_ms:0	- Delay (in milliseconds) to allow for final flush of buffers before shutting down.} {Timestamp:2025-03-31 11:49:43.598182596 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.retries:5	- Number of times to retry a request to a broker.} {Timestamp:2025-03-31 11:49:43.59818443 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.retry_base_backoff_ms:100	- Delay (in milliseconds) for initial retry backoff.} {Timestamp:2025-03-31 11:49:43.598186173 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.sasl_mechanism:	- The SASL mechanism to use when connecting.} {Timestamp:2025-03-31 11:49:43.598187966 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client.scram_password:	- Password to use for SCRAM authentication mechanisms.} {Timestamp:2025-03-31 11:49:43.598189489 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - schema_registry_client
2025-03-31 11:49:50.842387081  .scram_username:	- Username to use for SCRAM authentication mechanisms.} {Timestamp:2025-03-31 11:49:43.598191794 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.broker_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} crl file: {nullopt} client_auth_required: 0 }	- TLS configuration for the Kafka API servers to which the HTTP Proxy client should connect.} {Timestamp:2025-03-31 11:49:43.598193577 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.brokers:{{host: 0.0.0.0, port: 9092}}	- Network addresses of the Kafka API servers to which the HTTP Proxy client should connect.} {Timestamp:2025-03-31 11:49:43.598195681 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.client_identifier:{audit_log_client}	- Custom identifier to include in the Kafka request header for the HTTP Proxy client. This identifier can help debug or monitor client activities.} {Timestamp:2025-03-31 11:49:43.598197234 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.consumer_heartbeat_interval_ms:500	- Interval (in milliseconds) for consumer heartbeats.} {Timestamp:2025-03-31 11:49:43.598198757 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.consumer_rebalance_timeout_ms:2000	- Timeout (in milliseconds) for consumer rebalance.} {Timestamp:2025-03-31 11:49:43.598200179 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.consumer_request_max_bytes:1048576	- Maximum bytes to fetch per request.} {Timestamp:2025-03-31 11:49:43.598201612 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.consumer_request_min_bytes:1	- Minimum bytes to fetch per request.} {Timestamp:2025-03-31 11:49:43.598203345 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.consumer_request_timeout_ms:100	- Interval (in milliseconds) for consumer request timeout.} {Timestamp:2025-03-31 11:49:43.598204878 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.consumer_session_timeout_ms:10000	- Timeout (in milliseconds) for consumer session.} {Timestamp:2025-03-31 11:49:43.598207012 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.produce_ack_level:1	- Number of acknowledgments the producer requires the leader to have received before considering a request complete.} {Timestamp:2025-03-31 11:49:43.598208675 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.produce_batch_delay_ms:0	- Delay (in milliseconds) to wait before sending batch.} {Timestamp:2025-03-31 11:49:43.598255663 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.produce_batch_record_count:0	- Number of records to batch before sending to broker.} {Timestamp:2025-03-31 11:49:43.59825914 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.produce_batch_size_bytes:0	- Number of bytes to batch before sending to broker.} {Timestamp:2025-03-31 11:49:43.598261985 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.produce_compression_type:zstd	- Enable or disable compression by the Kafka client. Specify `none` to disable compression or one of the supported types [gzip, snappy, lz4, zstd].} {Timestamp:2025-03-31 11:49:43.59826431 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.produce_shutdown_delay_ms:3000	- Delay (in milliseconds) to allow for final flush of buffers before shutting down.} {Timestamp:2025-03-31 11:49:43.598266303 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.retries:5	- Number of times to retry a request to a broker.} {Timestamp:2025-03-31 11:49:43.598268307 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.retry_base_backoff_ms:100	- Delay (in milliseconds) for initial retry backoff.} {Timestamp:2025-03-31 11:49:43.598270231 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.sasl_mechanism:	- The SASL mechanism to use when connecting.} {Timestamp:2025-03-31 11:49:43.598272174 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.scram_password:	- Password to use for SCRAM authentication mechanisms.} {Timestamp:2025-03-31 11:49:43.598274138 +0000 UTC Content:INFO  2025-03-31 11:49:43,598 [shard  0:main] main - application.cc:849 - audit_log_client.scram_username:	- Username to use for SCRAM authentication mechanisms.} {Timestamp:2025-03-31 11:49:43.599177042 +0000 UTC Content:INFO  2025-03-31 11:49:43,599 [shard  0:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.601533904 +0000 UTC Content:INFO  2025-03-31 11:49:43,601 [shard  1:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.601535717 +0000 UTC Content:INFO  2025-03-31 11:49:43,601 [shard  6:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.60153695 +0000 UTC Content:INFO  2025-03-31 11:49:43,601 [shard  7:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.601818859 +0000 UTC Content:INFO  2025-03-31 11:49:43,601 [shard  9:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.602320339 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard 21:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.602324878 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard 20:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.60232586 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard 23:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.602326681 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard 22:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.602327523 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard 15:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.602328274 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard 14:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.602329046 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard  3:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.602329747 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard  4:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.602330448 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard 19:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.60233111 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard 17:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.602331851 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard 16:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.602332572 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard  2:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.602333244 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard 18:
2025-03-31 11:49:50.842447805  main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.602333905 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard  5:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.602334666 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard 10:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.602335428 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard 13:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.602365775 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard 12:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.602436858 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard 11:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.602501509 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard  8:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:43.602509935 +0000 UTC Content:INFO  2025-03-31 11:49:43,602 [shard  0:main] main - application.cc:563 - Setting abort_on_allocation_failure (abort on OOM): true} {Timestamp:2025-03-31 11:49:43.61298075 +0000 UTC Content:INFO  2025-03-31 11:49:43,612 [shard  0:main] syschecks - Writing pid file "/data/redpanda/pid.lock"} {Timestamp:2025-03-31 11:49:43.626248202 +0000 UTC Content:INFO  2025-03-31 11:49:43,626 [shard  0:main] storage - api.cc:70 - Checking `/data/redpanda` for supported filesystems} {Timestamp:2025-03-31 11:49:43.626389267 +0000 UTC Content:INFO  2025-03-31 11:49:43,626 [shard  0:main] syschecks - Detected file system type is btrfs} {Timestamp:2025-03-31 11:49:43.626391982 +0000 UTC Content:ERROR 2025-03-31 11:49:43,626 [shard  0:main] syschecks - Path: `/data/redpanda' uses btrfs filesystem which is not XFS or ext4. This is a unsupported configuration. You may experience poor performance or instability.} {Timestamp:2025-03-31 11:49:43.630979619 +0000 UTC Content:INFO  2025-03-31 11:49:43,630 [shard  0:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631037648 +0000 UTC Content:INFO  2025-03-31 11:49:43,630 [shard  3:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631174885 +0000 UTC Content:INFO  2025-03-31 11:49:43,630 [shard 10:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631180345 +0000 UTC Content:INFO  2025-03-31 11:49:43,630 [shard  8:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631181397 +0000 UTC Content:INFO  2025-03-31 11:49:43,630 [shard 13:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631182279 +0000 UTC Content:INFO  2025-03-31 11:49:43,630 [shard  6:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631183091 +0000 UTC Content:INFO  2025-03-31 11:49:43,630 [shard 11:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631237292 +0000 UTC Content:INFO  2025-03-31 11:49:43,630 [shard  4:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631238755 +0000 UTC Content:INFO  2025-03-31 11:49:43,630 [shard  7:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631239757 +0000 UTC Content:INFO  2025-03-31 11:49:43,630 [shard  1:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631421628 +0000 UTC Content:INFO  2025-03-31 11:49:43,631 [shard  9:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631423482 +0000 UTC Content:INFO  2025-03-31 11:49:43,631 [shard 18:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631424854 +0000 UTC Content:INFO  2025-03-31 11:49:43,631 [shard 17:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631436857 +0000 UTC Content:INFO  2025-03-31 11:49:43,631 [shard 21:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.63143849 +0000 UTC Content:INFO  2025-03-31 11:49:43,631 [shard 14:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631439872 +0000 UTC Content:INFO  2025-03-31 11:49:43,631 [shard 22:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631441195 +0000 UTC Content:INFO  2025-03-31 11:49:43,631 [shard 15:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631442477 +0000 UTC Content:INFO  2025-03-31 11:49:43,631 [shard 16:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.63144379 +0000 UTC Content:INFO  2025-03-31 11:49:43,631 [shard 20:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631452466 +0000 UTC Content:INFO  2025-03-31 11:49:43,631 [shard 19:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631453929 +0000 UTC Content:INFO  2025-03-31 11:49:43,631 [shard 12:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631538978 +0000 UTC Content:INFO  2025-03-31 11:49:43,631 [shard 23:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.631562693 +0000 UTC Content:INFO  2025-03-31 11:49:43,631 [shard  2:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:43.632063923 +0000 UTC Content:INFO  2025-03-31 11:49:43,632 [shard  5:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:44.85088593 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard  0:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.850928309 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard  2:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.850952785 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard  1:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.850956091 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard  3:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33
2025-03-31 11:49:50.842532293  145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.850959157 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard  5:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.850961752 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard  8:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.850964257 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard  4:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.850985176 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard 11:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.850987791 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard 18:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.850990235 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard 21:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.85099273 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard 19:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.851002959 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard  6:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.851005554 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard 20:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.851008009 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard 16:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.851010473 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard 12:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.851012928 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard 14:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.851026874 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard 23:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.851029369 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard  7:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.851031753 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard 15:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.851034188 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard 13:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.851045699 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard 22:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.851220628 +0000 UTC Content:INFO  2025-03-31 11:49:44,851 [shard 10:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.851223693 +0000 UTC Content:INFO  2025-03-31 11:49:44,850 [shard  9:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.851370429 +0000 UTC Content:INFO  2025-03-31 11:49:44,851 [shard 17:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.853891969 +0000 UTC Content:ERROR 2025-03-31 11:49:44,853 [shard  0:main] cluster - storage space alert: free space at 3.571% on /data/redpanda: 930.512GiB total, 33.231GiB free, min. free for alert 0.000bytes, min. free for degraded 5.000GiB. Please adjust retention policies as needed to avoid running out of space} {Timestamp:2025-03-31 11:49:44.862496323 +0000 UTC Content:INFO  2025-03-31 11:49:44,862 [shard  0:main] storage - segment_set.cc:347 - Recovered: {offset_tracker:{term:0, base_offset:259, committed_offset:265, dirty_offset
2025-03-31 11:49:50.842593187  :265}, compacted_segment=0, finished_self_compaction=0, finished_windowed_compaction=0, generation=1, reader={/data/redpanda/redpanda/kvstore/0_0/259-0-v1.log, (4134 bytes)}, writer=nullptr, cache=nullptr, compaction_index:nullopt, closed=0, tombstone=0, index={file:/data/redpanda/redpanda/kvstore/0_0/259-0-v1.base_index, offsets:259, index:{header_bitflags:0, base_offset:259, max_offset:265, base_timestamp:{timestamp: 1743421782036}, max_timestamp:{timestamp: 1743421782114}, batch_timestamps_are_monotonic:1, with_offset:false, non_data_timestamps:0, broker_timestamp:{nullopt}, num_compactible_records_appended:{7}, clean_compact_timestamp:{nullopt}, may_have_tombstone_records:1, index(1, 1, 1)}, step:32768, needs_persistence:0}}} {Timestamp:2025-03-31 11:49:44.862498347 +0000 UTC Content:INFO  2025-03-31 11:49:44,862 [shard  0:main] kvstore - kvstore.cc:565 - Replaying segment with base offset 259} {Timestamp:2025-03-31 11:49:44.870072287 +0000 UTC Content:INFO  2025-03-31 11:49:44,870 [shard  0:main] main - application.cc:2648 - Loaded stored node ID for node: 0} {Timestamp:2025-03-31 11:49:44.870713299 +0000 UTC Content:INFO  2025-03-31 11:49:44,870 [shard  0:main] main - application.cc:2675 - Loaded existing UUID for node: 9cf6cb6c-d630-4b06-be29-1b0a8da51615} {Timestamp:2025-03-31 11:49:44.873487464 +0000 UTC Content:INFO  2025-03-31 11:49:44,873 [shard  0:main] main - application.cc:2727 - Started RPC server listening at {host: 0.0.0.0, port: 33145}} {Timestamp:2025-03-31 11:49:44.873489758 +0000 UTC Content:INFO  2025-03-31 11:49:44,873 [shard  0:main] main - application.cc:2758 - Running with already-established node ID {0}} {Timestamp:2025-03-31 11:49:44.873504526 +0000 UTC Content:INFO  2025-03-31 11:49:44,873 [shard  0:main] main - application.cc:2833 - Starting Redpanda with node_id 0, cluster UUID {d68df077-1f89-425b-8663-9b6f92a567cc}} {Timestamp:2025-03-31 11:49:44.875722697 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard  0:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.875737866 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard  1:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.8757456 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard  8:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.875753685 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard  4:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.875755339 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard  9:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.875776378 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard 10:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.875778041 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard  6:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.875779564 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard  3:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.875787779 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard 13:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.875789032 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard 22:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.875796325 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard 12:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.875797818 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard 14:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.875805142 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard 17:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.875812586 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard 15:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.8758203 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard 18:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.87583599 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard 16:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.876141353 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard 11:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.876142956 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard 20:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.876144318 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard  2:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.876145661 +0000 UTC Content:INFO  2025-03-31 11:49:44,876 [shard  5:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.876147023 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard 19:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.876178022 +0000 UTC Content:INFO  2025-03-31 11:49:44,876 [shard  7:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.876205563 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard 23:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.87630523 +0000 UTC Content:INFO  2025-03-31 11:49:44,875 [shard 21:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:44.878191209 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard  0:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878205896 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard  1:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.8784639 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard  7:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878465734 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard 11:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878467116 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard  3:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878468419 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard 12:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878469631 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard  5:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878471064 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard 10:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878472456 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard 17:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:20
2025-03-31 11:49:50.842657458  25-03-31 11:49:44.878473829 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard 20:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878475051 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard 22:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878476374 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard  9:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878477596 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard 21:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878478858 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard 14:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878480091 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard 13:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878481483 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard 15:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878482605 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard 23:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878595988 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard 18:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878638819 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard 16:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878640211 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard  2:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.8786505 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard 19:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878738355 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard  4:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.878808186 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard  8:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.87881001 +0000 UTC Content:INFO  2025-03-31 11:49:44,878 [shard  6:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:44.879355473 +0000 UTC Content:INFO  2025-03-31 11:49:44,879 [shard  0:main] main - application.cc:1730 - Partition manager started} {Timestamp:2025-03-31 11:49:44.888975792 +0000 UTC Content:INFO  2025-03-31 11:49:44,888 [shard  0:main] main - application.cc:1818 - Archiver service setup, cloud_storage_enabled: false, legacy_upload_mode_enabled: true} {Timestamp:2025-03-31 11:49:44.898208455 +0000 UTC Content:INFO  2025-03-31 11:49:44,898 [shard  0:main] resource_mgmt - storage.cc:182 - Setting new target log data size 558.307GiB. Disk size 930.512GiB reservation percent 25 target percent {80} bytes {nullopt}} {Timestamp:2025-03-31 11:49:44.907303448 +0000 UTC Content:INFO  2025-03-31 11:49:44,907 [shard  0:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908263569 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard  1:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908267577 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard  4:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908270633 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard  6:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908273398 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard  2:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908276373 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard  3:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908279489 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard 11:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908282114 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard  8:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.90828512 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard 19:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908288025 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard 15:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908290951 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard 20:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908293756 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard 16:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908296641 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard 18:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908299657 +0000 UTC Content:INFO  2025-03-31 11:49:
2025-03-31 11:49:50.842720927  44,908 [shard 14:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908302422 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard 12:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908304877 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard 21:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908307382 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard 17:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908310187 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard 10:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908345132 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard 23:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908704557 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard 13:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908707883 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard 22:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908812149 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard  9:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.908828459 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard  5:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.909007515 +0000 UTC Content:INFO  2025-03-31 11:49:44,908 [shard  7:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:44.913357887 +0000 UTC Content:INFO  2025-03-31 11:49:44,913 [shard  0:main] cluster - cluster_discovery.cc:100 - Controller directory /data/redpanda/redpanda/controller/0_0 not empty; assuming existing cluster exists} {Timestamp:2025-03-31 11:49:44.932720414 +0000 UTC Content:INFO  2025-03-31 11:49:44,932 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1676 - Recovered persistent state from kvstore: voted for: {id: 0, revision: 0}, term: 31} {Timestamp:2025-03-31 11:49:44.932722258 +0000 UTC Content:INFO  2025-03-31 11:49:44,932 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1407 - Starting with voted_for {id: 0, revision: 0} term 31 initial_state false} {Timestamp:2025-03-31 11:49:44.932831713 +0000 UTC Content:INFO  2025-03-31 11:49:44,932 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1451 - Current log offsets: {start_offset:0, committed_offset:40, committed_offset_term:31, dirty_offset:40, dirty_offset_term:31}, read bootstrap state: data_seen 0 config_seen 0 eol false commit 0 term 0 prev_idx 0 prev_term 0 config_tracker -9223372036854775808 commit_base_tracker -9223372036854775808 configurations []} {Timestamp:2025-03-31 11:49:44.932833236 +0000 UTC Content:INFO  2025-03-31 11:49:44,932 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1478 - Truncating configurations at 40} {Timestamp:2025-03-31 11:49:44.942885776 +0000 UTC Content:INFO  2025-03-31 11:49:44,942 [shard  0:main] storage - segment.cc:817 - Creating new segment /data/redpanda/redpanda/kvstore/0_0/266-0-v1.log} {Timestamp:2025-03-31 11:49:44.959467296 +0000 UTC Content:INFO  2025-03-31 11:49:44,959 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1589 - started raft, log offsets: {start_offset:0, committed_offset:40, committed_offset_term:31, dirty_offset:40, dirty_offset_term:31}, term: 31, configuration: {current: {voters: {{id: 0, revision: 0}}, learners: {}}, old:{nullopt}, revision: 0, update: {nullopt}, version: 6}}} {Timestamp:2025-03-31 11:49:44.959908083 +0000 UTC Content:INFO  2025-03-31 11:49:44,959 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1008 - current node priority 1 is lower than target 4294967295, however the node is the only voter, continue with dispatching vote} {Timestamp:2025-03-31 11:49:44.95991146 +0000 UTC Content:INFO  2025-03-31 11:49:44,959 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:940 - starting pre-vote leader election, current term: 31, leadership transfer: false} {Timestamp:2025-03-31 11:49:44.969616277 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard  0:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.969660009 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard 11:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.969677412 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard  5:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.969678844 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard 10:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.969680147 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard  7:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.969681379 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard  6:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.969689494 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard 19:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.969690757 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard 16:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.969691909 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard 15:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.969693061 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard 23:main] cluster - drain_manage
2025-03-31 11:49:50.842770119  r.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.969694213 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard 21:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.969701737 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard  9:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.96970298 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard 18:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.969704182 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard 22:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.969710444 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard 12:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.969717226 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard 14:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.969968999 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard  8:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.969970712 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard 13:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.969971904 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard  3:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.970037848 +0000 UTC Content:INFO  2025-03-31 11:49:44,970 [shard 17:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.970041084 +0000 UTC Content:INFO  2025-03-31 11:49:44,969 [shard 20:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.970042477 +0000 UTC Content:INFO  2025-03-31 11:49:44,970 [shard  2:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.970304408 +0000 UTC Content:INFO  2025-03-31 11:49:44,970 [shard  1:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.970797964 +0000 UTC Content:INFO  2025-03-31 11:49:44,970 [shard  4:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:44.970808754 +0000 UTC Content:INFO  2025-03-31 11:49:44,970 [shard  0:main] cluster - members_manager.cc:98 - starting  members manager with founding brokers: {}} {Timestamp:2025-03-31 11:49:44.970909133 +0000 UTC Content:INFO  2025-03-31 11:49:44,970 [shard  0:main] cluster - controller.cc:572 - Controller log replay starting (to offset 36)} {Timestamp:2025-03-31 11:49:44.973117155 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard  0:main] cluster - bootstrap_backend.cc:92 - Applying update to bootstrap_manager} {Timestamp:2025-03-31 11:49:44.973177519 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard  0:main] cluster - members_manager.cc:855 - initializing cluster state with initial brokers [{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}], and node UUID map: {{9cf6cb6c-d630-4b06-be29-1b0a8da51615 -> 0}} at offset: 1} {Timestamp:2025-03-31 11:49:44.973189902 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard  0:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973264422 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard  1:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973267848 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard  4:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973270934 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard  5:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973273909 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard  3:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973276825 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard  7:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973294117 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard  6:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973297053 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard 13:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973306801 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard 15:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973309576 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard 17:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973312241 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard  2:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973322621 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard 19:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973544197 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard 22:main] cluster - members_table.cc:114 - setting ini
2025-03-31 11:49:50.842819372  tial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973546972 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard 12:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973549346 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard 18:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973552021 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard 16:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973554997 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard 23:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973557782 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard  8:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973560918 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard 21:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973563633 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard 11:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973643272 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard 14:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.97365811 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard 20:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973707042 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard 10:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.973725246 +0000 UTC Content:INFO  2025-03-31 11:49:44,973 [shard  9:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:44.974858092 +0000 UTC Content:INFO  2025-03-31 11:49:44,974 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] vote_stm.cc:421 - becoming the leader term:32} {Timestamp:2025-03-31 11:49:44.97491044 +0000 UTC Content:INFO  2025-03-31 11:49:44,974 [shard  0:main] storage - segment.cc:817 - Creating new segment /data/redpanda/redpanda/controller/0_0/41-32-v1.log} {Timestamp:2025-03-31 11:49:44.991286664 +0000 UTC Content:INFO  2025-03-31 11:49:44,991 [shard  0:main] cluster - controller.cc:583 - Controller log replay complete.} {Timestamp:2025-03-31 11:49:44.991298316 +0000 UTC Content:INFO  2025-03-31 11:49:44,991 [shard  0:main] cluster - controller.cc:1032 - Member of cluster UUID d68df077-1f89-425b-8663-9b6f92a567cc} {Timestamp:2025-03-31 11:49:44.992305015 +0000 UTC Content:INFO  2025-03-31 11:49:44,992 [shard  0:main] cluster - controller_backend.cc:833 - Cleaning up orphan topic files. bootstrap_revision: -9223372036854775808} {Timestamp:2025-03-31 11:49:44.99824018 +0000 UTC Content:INFO  2025-03-31 11:49:44,998 [shard  0:main] cluster - feature_manager.cc:100 - Starting...} {Timestamp:2025-03-31 11:49:44.998294953 +0000 UTC Content:INFO  2025-03-31 11:49:44,998 [shard  0:main] cluster - partition_balancer_backend.cc:108 - partition balancer started} {Timestamp:2025-03-31 11:49:44.998957296 +0000 UTC Content:INFO  2025-03-31 11:49:44,998 [shard  0:main] cluster - metrics_reporter.cc:390 - Generated cluster metrics ID 3953b6f3-7800-4aef-aad3-9890c911f179} {Timestamp:2025-03-31 11:49:44.998958728 +0000 UTC Content:INFO  2025-03-31 11:49:44,998 [shard  0:main] data-migrate - data_migration_backend.cc:107 - backend starting} {Timestamp:2025-03-31 11:49:44.99895979 +0000 UTC Content:INFO  2025-03-31 11:49:44,998 [shard  0:main] data-migrate - data_migration_backend.cc:160 - backend not started as cloud_storage_api is not available} {Timestamp:2025-03-31 11:49:44.999454899 +0000 UTC Content:INFO  2025-03-31 11:49:44,999 [shard  0:main] cluster - leader_balancer.cc:112 - Leader balancer: controller leadership detected. Starting rebalancer in 30 seconds} {Timestamp:2025-03-31 11:49:45.004158053 +0000 UTC Content:ERROR 2025-03-31 11:49:45,004 [shard  0:main] debug-bundle-service - Current specified RPK location /usr/bin/rpk does not exist!  Debug bundle creation is not available until this is fixed!} {Timestamp:2025-03-31 11:49:45.004442918 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard  3:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004445362 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard 20:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004447296 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard  6:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004449109 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard 22:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004450852 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard  4:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004452536 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard 10:main] admin_api_s
2025-03-31 11:49:50.842879565  erver - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004466031 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard 19:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004467694 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard  7:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004469297 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard  9:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.00447096 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard 23:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004472293 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard  1:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004488653 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard 11:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004490357 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard  1:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.00449196 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard 23:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004493683 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard 14:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004495446 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard 12:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004497189 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard  8:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004498562 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard 10:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004499924 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard 16:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004501618 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard  0:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.0045031 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard 13:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004516756 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard 16:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004518379 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard 19:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004519952 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard 22:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004521545 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard 21:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004523058 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard 13:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004524701 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard  9:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004526384 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard  3:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004527937 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard  4:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.00452949 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard 15:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004531023 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard  0:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004543997 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard 17:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.00454569 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard 14:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004547273 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard 15:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004548836 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard 12:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004550379 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard  7:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004551912 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard  8:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004553325 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard 11:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004554697 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard 21:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004566199 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard 20:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004567672 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard 17:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004817781 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard  2:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004819714 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard  2:main] admin_api_server - server.cc:353 - S
2025-03-31 11:49:50.842934628  tarted HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004821047 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] vote_stm.cc:436 - became the leader term: 32} {Timestamp:2025-03-31 11:49:45.00482254 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard  5:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004823942 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard  5:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004857576 +0000 UTC Content:WARN  2025-03-31 11:49:45,004 [shard 18:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:45.004859148 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard  6:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.004879497 +0000 UTC Content:INFO  2025-03-31 11:49:45,004 [shard 18:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:45.005079412 +0000 UTC Content:INFO  2025-03-31 11:49:45,005 [shard  0:main] resource_mgmt - storage.cc:73 - Starting disk space manager service (enabled)} {Timestamp:2025-03-31 11:49:45.01225268 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  0:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.012255596 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  0:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012256507 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  1:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.012257279 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 10:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.01225798 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  2:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.012258681 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  9:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.012259363 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 13:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.012260074 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 22:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.012260795 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  8:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.012261497 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  1:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012262178 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 15:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.012262889 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 11:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.01226357 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  6:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.012264292 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  6:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012264943 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 21:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.0122844 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 21:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012285151 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  4:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.012285842 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 17:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.012296222 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 17:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012296933 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  5:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.012302303 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  5:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012302974 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  7:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.012308365 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  7:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012314386 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  8:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012315638 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 23:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.012322942 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 23:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012324405 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  3:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.01233264 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  3:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012334083 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 22:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012340655 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 13:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012363728 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 15:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.01236454 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 18:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.012373587 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 19:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.01237499 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 12:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.012376252 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 16:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.012408893 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  4:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012410506 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 16:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012411688 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 14:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timest
2025-03-31 11:49:50.842980013  amp:2025-03-31 11:49:45.012412961 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  2:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012414203 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 12:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012415496 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 10:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012416668 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 11:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.0124179 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 20:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:45.012427608 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 20:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012428871 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 18:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012430133 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 14:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012431465 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 19:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012574213 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  9:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012594321 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  0:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012602216 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  4:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.01260977 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  2:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012611313 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  8:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012612836 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  4:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012614218 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  1:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012615611 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  5:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012623846 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 20:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012625109 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 10:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012626331 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  6:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012627764 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 16:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012629146 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 13:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012630419 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 19:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012631811 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 17:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012633184 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 12:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012634516 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 11:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.01264699 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 18:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012648523 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 15:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012649915 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  0:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012651308 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  9:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.01265265 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 16:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012653943 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 14:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012655295 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 23:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012656748 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  1:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012658181 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 13:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012659593 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  9:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012660816 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  6:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012672528 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 21:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.01267396 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 19:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012675133 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 12:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012676385 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  2:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012677798 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 23:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.01267917 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 10:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012680603 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 20:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012681985 +0000 UTC Content:INFO  2025-03-31 
2025-03-31 11:49:50.843037822  11:49:45,012 [shard 14:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012683478 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 21:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012684871 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 15:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012699058 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 18:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.01270056 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 11:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012701853 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  8:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012703195 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  5:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012704578 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 17:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012788325 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  3:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012790148 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard  3:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.012822579 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 22:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.012887531 +0000 UTC Content:INFO  2025-03-31 11:49:45,012 [shard 22:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.013796356 +0000 UTC Content:INFO  2025-03-31 11:49:45,013 [shard  7:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:45.013804742 +0000 UTC Content:INFO  2025-03-31 11:49:45,013 [shard  7:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:45.014974356 +0000 UTC Content:INFO  2025-03-31 11:49:45,014 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:277 - Stopping} {Timestamp:2025-03-31 11:49:45.052113526 +0000 UTC Content:INFO  2025-03-31 11:49:45,052 [shard  0:main] resource_mgmt - storage.cc:88 - Stopping disk space manager service} {Timestamp:2025-03-31 11:49:45.052477949 +0000 UTC Content:INFO  2025-03-31 11:49:45,052 [shard  0:main] auditing - audit_log_manager.cc:843 - Shutting down audit log manager} {Timestamp:2025-03-31 11:49:45.052481406 +0000 UTC Content:INFO  2025-03-31 11:49:45,052 [shard  0:main] auditing - audit_log_manager.cc:607 - stop() invoked on audit_sink} {Timestamp:2025-03-31 11:49:45.05248347 +0000 UTC Content:INFO  2025-03-31 11:49:45,052 [shard  0:main] auditing - audit_log_manager.cc:647 - Setting auditing enabled state to: false} {Timestamp:2025-03-31 11:49:45.052485423 +0000 UTC Content:INFO  2025-03-31 11:49:45,052 [shard  0:main] auditing - audit_log_manager.cc:701 - Ignored update to audit_enabled(), auditing is already disabled} {Timestamp:2025-03-31 11:49:45.05486056 +0000 UTC Content:INFO  2025-03-31 11:49:45,054 [shard  0:main] cluster - leader_balancer.cc:308 - Stopping Leader Balancer...} {Timestamp:2025-03-31 11:49:45.055637688 +0000 UTC Content:INFO  2025-03-31 11:49:45,055 [shard  0:main] data-migrate - data_migration_backend.cc:165 - backend stopping} {Timestamp:2025-03-31 11:49:45.056021187 +0000 UTC Content:INFO  2025-03-31 11:49:45,056 [shard  0:main] data-migrate - data_migration_backend.cc:180 - backend stopped} {Timestamp:2025-03-31 11:49:45.057053965 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard  0:main] cluster - partition_balancer_backend.cc:296 - stopping...} {Timestamp:2025-03-31 11:49:45.057055277 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard  0:main] cluster - metrics_reporter.cc:188 - Stopping Metrics Reporter...} {Timestamp:2025-03-31 11:49:45.057063974 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard  0:main] cluster - feature_manager.cc:197 - Stopping Feature Manager...} {Timestamp:2025-03-31 11:49:45.057078501 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard  0:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057119057 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard  2:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.05712079 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard  4:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057122263 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard  1:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057141759 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard  3:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057143142 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard  6:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057144475 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard  9:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057145727 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard  7:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057147069 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard 13:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057148422 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard 16:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057149704 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard 11:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057150997 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard 21:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057152249 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard 17:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057153522 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard  8:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057154854 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard 14:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057156106 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard 19:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057157379 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard 12:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057176384 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard 18:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057177717 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard 20:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-
2025-03-31 11:49:50.843080031  03-31 11:49:45.057179019 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard 23:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057180272 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard 15:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057441812 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard 10:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057443566 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard 22:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057518426 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard  5:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:45.057862031 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard  0:main] cluster - health_monitor_backend.cc:107 - Stopping Health Monitor Backend...} {Timestamp:2025-03-31 11:49:45.057863944 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard  0:main] cluster - health_manager.cc:61 - Stopping Health Manager...} {Timestamp:2025-03-31 11:49:45.057876248 +0000 UTC Content:INFO  2025-03-31 11:49:45,057 [shard  0:main] cluster - members_backend.cc:62 - Stopping Members Backend...} {Timestamp:2025-03-31 11:49:45.059601404 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard  0:main] cluster - config_manager.cc:249 - Stopping Config Manager...} {Timestamp:2025-03-31 11:49:45.060009991 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard  0:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060011734 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard  1:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060012696 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard  2:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060013517 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard  4:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.06001495 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard  9:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060016343 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard 16:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060017765 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard 23:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060019068 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard 18:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.06002023 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard 13:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060021482 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard 10:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060022825 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard 17:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060024067 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard  6:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.06002533 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard 14:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060026572 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard 20:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060027814 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard  5:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060029097 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard 11:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060055216 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard 12:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060056718 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard 22:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060057971 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard 21:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060059213 +0000 UTC Content:INFO  2025-03-31 11:49:45,060 [shard  3:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060060516 +0000 UTC Content:INFO  2025-03-31 11:49:45,059 [shard  7:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060061808 +0000 UTC Content:INFO  2025-03-31 11:49:45,060 [shard  8:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060079151 +0000 UTC Content:INFO  2025-03-31 11:49:45,060 [shard 15:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.060186382 +0000 UTC Content:INFO  2025-03-31 11:49:45,060 [shard 19:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:45.064612816 +0000 UTC Content:INFO  2025-03-31 11:49:45,064 [shard  0:main] cluster - members_manager.cc:944 - stopping cluster::members_manager...} {Timestamp:2025-03-31 11:49:45.066973645 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard  0:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.066975389 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard  0:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.06697633 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard  1:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.066977072 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard  5:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.066977753 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard 11:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.066978404 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard 18:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.066979036 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard 19:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.066979667 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard 20:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.066980298 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard 21:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.066980919 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard  2:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.06698156 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard 10:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.066982191 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard 13:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.066982813 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard 15:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.066983434 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard  9:main] cluster - drain_manager.cc:31 - Dr
2025-03-31 11:49:50.843121589  ain manager stopping} {Timestamp:2025-03-31 11:49:45.066984055 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard 22:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.066984666 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard 11:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067004694 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard 23:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.067006106 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard 23:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067007309 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard  6:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.067008511 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard  7:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.06701337 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard  6:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067014051 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard  7:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067014732 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard 18:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067047985 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard 12:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.067049367 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard  3:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.067050289 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard 16:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.067051011 +0000 UTC Content:INFO  2025-03-31 11:49:45,067 [shard 10:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067051672 +0000 UTC Content:INFO  2025-03-31 11:49:45,067 [shard 16:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067052313 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard 22:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067053064 +0000 UTC Content:INFO  2025-03-31 11:49:45,067 [shard  9:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067053726 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard 17:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.067062001 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard  8:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.067067712 +0000 UTC Content:INFO  2025-03-31 11:49:45,067 [shard  8:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067082259 +0000 UTC Content:INFO  2025-03-31 11:49:45,067 [shard  5:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067090164 +0000 UTC Content:INFO  2025-03-31 11:49:45,067 [shard 21:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067108989 +0000 UTC Content:INFO  2025-03-31 11:49:45,067 [shard 20:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067110282 +0000 UTC Content:INFO  2025-03-31 11:49:45,067 [shard  2:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067123597 +0000 UTC Content:INFO  2025-03-31 11:49:45,067 [shard 15:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067124809 +0000 UTC Content:INFO  2025-03-31 11:49:45,067 [shard 17:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067142502 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard  4:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.067150447 +0000 UTC Content:INFO  2025-03-31 11:49:45,067 [shard  4:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067151679 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard  1:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067159674 +0000 UTC Content:INFO  2025-03-31 11:49:45,067 [shard 12:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.06716794 +0000 UTC Content:INFO  2025-03-31 11:49:45,066 [shard 14:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:45.067169212 +0000 UTC Content:INFO  2025-03-31 11:49:45,067 [shard 14:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067201082 +0000 UTC Content:INFO  2025-03-31 11:49:45,067 [shard 13:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067203166 +0000 UTC Content:INFO  2025-03-31 11:49:45,067 [shard  3:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.067388584 +0000 UTC Content:INFO  2025-03-31 11:49:45,067 [shard 19:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:45.070586153 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard  0:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.070599408 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard  1:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.070600911 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard  2:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.070609587 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard  4:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.07061097 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard  5:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.070612272 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard  7:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.070613575 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard  9:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.07062203 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard 11:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.070623413 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard 13:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.070624675 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard 14:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.070625898 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard  6:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.07062725 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard  8:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.070635526 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard 12:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.070636888 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard 18:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.070638151 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard 10:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.07063943
2025-03-31 11:49:50.843179397  3 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard 21:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.070640725 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard 23:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.070641968 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard 19:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.070657257 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard 15:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.070658579 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard 20:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.070659861 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard 17:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.070661124 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard 22:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.071214612 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard  3:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.071216045 +0000 UTC Content:INFO  2025-03-31 11:49:45,070 [shard 16:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:45.073753706 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard  0:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/0_0} {Timestamp:2025-03-31 11:49:45.073844056 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard  1:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/1_0} {Timestamp:2025-03-31 11:49:45.07384619 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard  5:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/5_0} {Timestamp:2025-03-31 11:49:45.073847853 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard 11:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/11_0} {Timestamp:2025-03-31 11:49:45.073860887 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard  6:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/6_0} {Timestamp:2025-03-31 11:49:45.07386268 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard  4:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/4_0} {Timestamp:2025-03-31 11:49:45.073864314 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard  9:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/9_0} {Timestamp:2025-03-31 11:49:45.073865796 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard 14:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/14_0} {Timestamp:2025-03-31 11:49:45.073867349 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard 10:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/10_0} {Timestamp:2025-03-31 11:49:45.073868732 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard 17:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/17_0} {Timestamp:2025-03-31 11:49:45.073870114 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard 21:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/21_0} {Timestamp:2025-03-31 11:49:45.073879823 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard 15:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/15_0} {Timestamp:2025-03-31 11:49:45.073881366 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard  3:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/3_0} {Timestamp:2025-03-31 11:49:45.073882798 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard  7:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/7_0} {Timestamp:2025-03-31 11:49:45.073884231 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard 19:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/19_0} {Timestamp:2025-03-31 11:49:45.073885623 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard  2:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/2_0} {Timestamp:2025-03-31 11:49:45.073893188 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard 20:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/20_0} {Timestamp:2025-03-31 11:49:45.073893989 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard 13:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/13_0} {Timestamp:2025-03-31 11:49:45.073894741 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard 18:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/18_0} {Timestamp:2025-03-31 11:49:45.073909308 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard 16:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/16_0} {Timestamp:2025-03-31 11:49:45.073910831 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard 22:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/22_0} {Timestamp:2025-03-31 11:49:45.074207507 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard  8:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/8_0} {Timestamp:2025-03-31 11:49:45.074209171 +0000 UTC Content:INFO  2025-03-31 11:49:45,073 [shard 23:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/23_0} {Timestamp:2025-03-31 11:49:45.074210673 +0000 UTC Content:INFO  2025-03-31 11:49:45,074 [shard 12:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/12_0} {Timestamp:2025-03-31 11:49:45.095831959 +0000 UTC Content:INFO  2025-03-31 11:49:45,095 [shard  0:main] main - application.cc:501 - Shutdown complete.} {Timestamp:2025-03-31 11:49:45.095898213 +0000 UTC Content:ERROR 2025-03-31 11:49:45,095 [shard  0:main] main - application.cc:527 - Failure during startup: std::__1::system_error (error system:98, posix_listen failed for address 0.0.0.0:8082: Address already in use)} {Timestamp:2025-03-31 11:49:46.315077577 +0000 UTC Content:} {Timestamp:2025-03-31 11:49:46.315081484 +0000 UTC Content:} {Timestamp:2025-03-31 11:49:46.315082476 +0000 UTC Content:Welcome to the Redpanda community!} {Timestamp:2025-03-31 11:49:46.315083087 +0000 UTC Content:} {Timestamp:2025-03-31 11:49:46.315084079 +0000 UTC Content:Documentation: https://docs.redpanda.com - Product documentation site} {Timestamp:2025-03-31 11:49:46.315085362 +0000 UTC Content:GitHub Discussion: https://github.com/redpanda-data/redpanda/discussions - Longer, more involved discussions} {Timestamp:2025-03-31 11:49:46.315086494 +0000 UTC Content:GitHub Issues: https://github.com/redpanda-data/redpanda/issues - Report and track issues with the codebase} {Timestamp:2025-03-31 11:49:46.315087606 +0000 UTC Content:Support: https://support.redpanda.com - Contact the support team privately} {Timestamp:2025-03-31 11:49:46.315088798 +0000 UTC Content:Product Feedback: https://redpanda.com/feedback - Let us know how we can improve your experience} {Timestamp:2025-03-31 11:49:46.31508991 +0000 UTC Content:Slack: https://redpanda.com/slack - Chat about all things Redpanda. Join the conversation!} {Timestamp:2025-03-31 11:49:46.315090892 +0000 UTC Content:Twitter: https://twitter.com/redpandadata - All the latest Redpanda news!} {Timestamp:2025-03-31 11:49:46.315091413 +0000 UTC Content:} {Timestamp:2025-03-31 11:49:46.315091864 +0000 UTC Content:} {Timestamp:2025-03-31 11:49:46.326644929 +0000 UTC Content:WARN  2025-03-31 11:49:46,326 seastar - Requested AIO slots too large, please increase request capac
2025-03-31 11:49:50.843221095  ity in /proc/sys/fs/aio-max-nr. configured:65536 available:65536 requested:264624} {Timestamp:2025-03-31 11:49:46.326660168 +0000 UTC Content:WARN  2025-03-31 11:49:46,326 seastar - max-networking-io-control-blocks adjusted from 10000 to 1704, since AIO slots are unavailable} {Timestamp:2025-03-31 11:49:46.326667081 +0000 UTC Content:INFO  2025-03-31 11:49:46,326 seastar - Reactor backend: epoll} {Timestamp:2025-03-31 11:49:46.381884097 +0000 UTC Content:WARN  2025-03-31 11:49:46,379 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.381898534 +0000 UTC Content:WARN  2025-03-31 11:49:46,381 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.401797909 +0000 UTC Content:WARN  2025-03-31 11:49:46,401 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.402436537 +0000 UTC Content:WARN  2025-03-31 11:49:46,402 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.413414313 +0000 UTC Content:WARN  2025-03-31 11:49:46,412 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.41444149 +0000 UTC Content:WARN  2025-03-31 11:49:46,414 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.419823057 +0000 UTC Content:WARN  2025-03-31 11:49:46,419 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.419828978 +0000 UTC Content:WARN  2025-03-31 11:49:46,419 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.420452428 +0000 UTC Content:WARN  2025-03-31 11:49:46,420 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.42045877 +0000 UTC Content:WARN  2025-03-31 11:49:46,420 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.427513836 +0000 UTC Content:WARN  2025-03-31 11:49:46,427 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.428003134 +0000 UTC Content:WARN  2025-03-31 11:49:46,427 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.431196606 +0000 UTC Content:WARN  2025-03-31 11:49:46,431 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.43125223 +0000 UTC Content:WARN  2025-03-31 11:49:46,431 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.432712039 +0000 UTC Content:WARN  2025-03-31 11:49:46,432 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.432832575 +0000 UTC Content:WARN  2025-03-31 11:49:46,432 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.437175794 +0000 UTC Content:WARN  2025-03-31 11:49:46,436 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.438098605 +0000 UTC Content:WARN  2025-03-31 11:49:46,437 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.439744323 +0000 UTC Content:WARN  2025-03-31 11:49:46,439 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.439850242 +0000 UTC Content:WARN  2025-03-31 11:49:46,439 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.443341391 +0000 UTC Content:WARN  2025-03-31 11:49:46,443 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.443439365 +0000 UTC Content:WARN  2025-03-31 11:49:46,443 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.449870321 +0000 UTC Content:WARN  2025-03-31 11:49:46,448 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.450106524 +0000 UTC Content:WARN  2025-03-31 11:49:46,450 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.451174999 +0000 UTC Content:WARN  2025-03-31 11:49:46,450 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.451827633 +0000 UTC Content:WARN  2025-03-31 11:49:46,451 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.451830899 +0000 UTC Content:WARN  2025-03-31 11:49:46,451 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.452459589 +0000 UTC Content:WARN  2025-03-31 11:49:46,452 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.454432841 +0000 UTC Content:WARN  2025-03-31 11:49:46,454 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.454449953 +0000 UTC Content:WARN  2025-03-31 11:49:46,454 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.461375346 +0000 UTC Content:WARN  2025-03-31 11:49:46,461 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (e
2025-03-31 11:49:50.843264547  rror system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.461432283 +0000 UTC Content:WARN  2025-03-31 11:49:46,461 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.468845291 +0000 UTC Content:WARN  2025-03-31 11:49:46,468 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.468888713 +0000 UTC Content:WARN  2025-03-31 11:49:46,468 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.470661118 +0000 UTC Content:WARN  2025-03-31 11:49:46,470 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.470863077 +0000 UTC Content:WARN  2025-03-31 11:49:46,470 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.472537799 +0000 UTC Content:WARN  2025-03-31 11:49:46,472 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.472541155 +0000 UTC Content:WARN  2025-03-31 11:49:46,472 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.474071647 +0000 UTC Content:WARN  2025-03-31 11:49:46,473 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.474122402 +0000 UTC Content:WARN  2025-03-31 11:49:46,474 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.478285232 +0000 UTC Content:WARN  2025-03-31 11:49:46,478 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.478334875 +0000 UTC Content:WARN  2025-03-31 11:49:46,478 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.478937436 +0000 UTC Content:WARN  2025-03-31 11:49:46,478 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.478970608 +0000 UTC Content:WARN  2025-03-31 11:49:46,478 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.479880515 +0000 UTC Content:WARN  2025-03-31 11:49:46,479 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.479882399 +0000 UTC Content:WARN  2025-03-31 11:49:46,479 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.481798333 +0000 UTC Content:WARN  2025-03-31 11:49:46,481 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:46.481801499 +0000 UTC Content:WARN  2025-03-31 11:49:46,481 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:46.48660952 +0000 UTC Content:INFO  2025-03-31 11:49:46,486 [shard  0:main] main - application.cc:480 - Redpanda v24.3.8 - b1dd9f54ab1fcd31110608ff214d0937bf30fdb1} {Timestamp:2025-03-31 11:49:46.486612445 +0000 UTC Content:INFO  2025-03-31 11:49:46,486 [shard  0:main] main - application.cc:481 - Command line: /opt/redpanda/bin/redpanda --redpanda-cfg /run/service/redpanda/config/redpanda.yaml} {Timestamp:2025-03-31 11:49:46.486614008 +0000 UTC Content:INFO  2025-03-31 11:49:46,486 [shard  0:main] main - application.cc:489 - kernel=6.14.0-2-cachyos-bore, nodename=a13f5c90d4a1, machine=x86_64} {Timestamp:2025-03-31 11:49:46.486615491 +0000 UTC Content:INFO  2025-03-31 11:49:46,486 [shard  0:main] main - application.cc:400 - System resources: { cpus: 24, available memory: 116.812GiB, reserved memory: 8.177GiB}} {Timestamp:2025-03-31 11:49:46.486616633 +0000 UTC Content:INFO  2025-03-31 11:49:46,486 [shard  0:main] main - application.cc:408 - File handle limit: 524288/524288} {Timestamp:2025-03-31 11:49:46.491250507 +0000 UTC Content:INFO  2025-03-31 11:49:46,491 [shard  0:main] cluster - config_manager.cc:331 - Ignoring invalid property: log_retention_ms=18446744073709551615} {Timestamp:2025-03-31 11:49:46.491930273 +0000 UTC Content:INFO  2025-03-31 11:49:46,491 [shard  0:main] cluster - config_manager.cc:426 - Ignoring value for 'log_retention_ms' in redpanda.yaml: use `rpk cluster config edit` to edit cluster configuration properties.} {Timestamp:2025-03-31 11:49:46.491932868 +0000 UTC Content:INFO  2025-03-31 11:49:46,491 [shard  0:main] cluster - config_manager.cc:426 - Ignoring value for 'retention_bytes' in redpanda.yaml: use `rpk cluster config edit` to edit cluster configuration properties.} {Timestamp:2025-03-31 11:49:46.491934881 +0000 UTC Content:INFO  2025-03-31 11:49:46,491 [shard  0:main] cluster - config_manager.cc:426 - Ignoring value for 'auto_create_topics_enabled' in redpanda.yaml: use `rpk cluster config edit` to edit cluster configuration properties.} {Timestamp:2025-03-31 11:49:46.491947856 +0000 UTC Content:INFO  2025-03-31 11:49:46,491 [shard  0:main] main - application.cc:896 - Cluster configuration properties:} {Timestamp:2025-03-31 11:49:46.491949268 +0000 UTC Content:INFO  2025-03-31 11:49:46,491 [shard  0:main] main - application.cc:897 - (use `rpk cluster config edit` to change)} {Timestamp:2025-03-31 11:49:46.492366421 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.abort_index_segment_size:50000	- Capacity (in number of txns) of an abort index segment. Each partition tracks the aborted transaction offset ranges to help service client requests.If the number transactions increase beyond this threshold, they are flushed to disk to easy memory pressure.Then they're loaded on demand. This configuration controls the maximum number of aborted transactions  before they are flushed to disk.} {Timestamp:2025-03-31 11:49:46.49244584 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.abort_timed_out_transactions_interval_ms:10000	- Interval, in milliseconds, at which Redpanda looks for inactive transactions and aborts them.} {Timestamp:2025-03-31 11:49:46.492447914 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.admin_api_require_auth:0	- Whether Admin API clients must provide HTTP basic authentication headers.} {Timestamp:2025-03-31 11:49:46.49245126 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.aggregate_metrics:0	- Enable aggregation of metrics returned by the `/metrics` endpoint. Aggregation can simplify monitoring by providing summarized data instead of raw, per-instance metrics. Metric aggregation is performed by 
2025-03-31 11:49:50.843322736  summing the values of samples by labels and is done when it makes sense by the shard and/or partition labels.} {Timestamp:2025-03-31 11:49:46.492453164 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.alive_timeout_ms:5000	- The amount of time since the last broker status heartbeat. After this time, a broker is considered offline and not alive.} {Timestamp:2025-03-31 11:49:46.49245639 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.alter_topic_cfg_timeout_ms:5000	- The duration, in milliseconds, that Redpanda waits for the replication of entries in the controller log when executing a request to alter topic configurations. This timeout ensures that configuration changes are replicated across the cluster before the alteration request is considered complete.} {Timestamp:2025-03-31 11:49:46.492459796 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.append_chunk_size:16384	- Size of direct write operations to disk in bytes. A larger chunk size can improve performance for write-heavy workloads, but increase latency for these writes as more data is collected before each write operation. A smaller chunk size can decrease write latency, but potentially increase the number of disk I/O operations.} {Timestamp:2025-03-31 11:49:46.492463353 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.audit_client_max_buffer_size:16777216	- Defines the number of bytes allocated by the internal audit client for audit messages. When changing this, you must disable audit logging and then re-enable it for the change to take effect. Consider increasing this if your system generates a very large number of audit records in a short amount of time.} {Timestamp:2025-03-31 11:49:46.492465827 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.audit_enabled:0	- Enables or disables audit logging. When you set this to true, Redpanda checks for an existing topic named `_redpanda.audit_log`. If none is found, Redpanda automatically creates one for you.} {Timestamp:2025-03-31 11:49:46.492468973 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.audit_enabled_event_types:{management, authenticate, admin}	- List of strings in JSON style identifying the event types to include in the audit log. This may include any of the following: `management, produce, consume, describe, heartbeat, authenticate, schema_registry, admin`.} {Timestamp:2025-03-31 11:49:46.492470727 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.audit_excluded_principals:{}	- List of user principals to exclude from auditing.} {Timestamp:2025-03-31 11:49:46.4924725 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.audit_excluded_topics:{}	- List of topics to exclude from auditing.} {Timestamp:2025-03-31 11:49:46.492475656 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.audit_log_num_partitions:12	- Defines the number of partitions used by a newly-created audit topic. This configuration applies only to the audit log topic and may be different from the cluster or other topic configurations. This cannot be altered for existing audit log topics.} {Timestamp:2025-03-31 11:49:46.492479964 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.audit_log_replication_factor:{nullopt}	- Defines the replication factor for a newly-created audit log topic. This configuration applies only to the audit log topic and may be different from the cluster or other topic configurations. This cannot be altered for existing audit log topics. Setting this value is optional. If a value is not provided, Redpanda will use the value specified for `internal_topic_replication_factor`.} {Timestamp:2025-03-31 11:49:46.492483751 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.audit_queue_drain_interval_ms:500	- Interval, in milliseconds, at which Redpanda flushes the queued audit log messages to the audit log topic. Longer intervals may help prevent duplicate messages, especially in high throughput scenarios, but they also increase the risk of data loss during shutdowns where the queue is lost.} {Timestamp:2025-03-31 11:49:46.492487759 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.audit_queue_max_buffer_size_per_shard:1048576	- Defines the maximum amount of memory in bytes used by the audit buffer in each shard. Once this size is reached, requests to log additional audit messages will return a non-retryable error. Limiting the buffer size per shard helps prevent any single shard from consuming excessive memory due to audit log messages.} {Timestamp:2025-03-31 11:49:46.492489822 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.auto_create_topics_enabled:1	- Allow automatic topic creation. To prevent excess topics, this property is not supported on Redpanda Cloud BYOC and Dedicated clusters. You should explicitly manage topic creation for these Redpanda Cloud clusters. If you produce to a topic that doesn't exist, the topic will be created with defaults if this property is enabled.} {Timestamp:2025-03-31 11:49:46.492491756 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_access_key:{nullopt}	- AWS or GCP access key. This access key is part of the credentials that Redpanda requires to authenticate with object storage services for Tiered Storage. This access key is used with the <<cloud_storage_secret_key>> to form the complete credentials required for authentication. To authenticate using IAM roles, see cloud_storage_credentials_source.} {Timestamp:2025-03-31 11:49:46.492493289 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_api_endpoint:{nullopt}	- Optional API endpoint. - AWS: When blank, this is automatically generated using <<cloud_storage_region,region>> and <<cloud_storage_bucket,bucket>>. Otherwise, this uses the value assigned. - GCP: Uses `storage.googleapis.com`.} {Timestamp:2025-03-31 11:49:46.49249409 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_api_endpoint_port:443	- TLS port override.} {Timestamp:2025-03-31 11:49:46.492496064 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_attempt_cluster_restore_on_bootstrap:0	- When set to `true`, Redpanda automatically retrieves cluster metadata from a specified object storage bucket at the cluster's first startup. This option is ideal for orchestrated deployments, such as Kubernetes. Ensure any previous cluster linked to the bucket is fully decommissioned to prevent conflicts between Tiered Storage subsystems.} {Timestamp:2025-03-31 11:49:46.492497347 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_azure_adls_endpoint:{nullopt}	- Azure Data Lake Storage v2 endpoint override. Use when hierarchical namespaces are enabled on your storage account and you have set up a custom endpoint.} {Timestamp:2025-03-31 11:49:46.492498759 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_azure_adls_port:{nullopt}	- Azure Data Lake Storage v2 port override. See also `cloud_storage_azure_adls_endpoint`. Use when Hierarchical Namespaces are enabled on your storage account and you have set up a custom endpoint.} {Timestamp:2025-03-31 11:49:46.49258444 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_azure_container:{nullopt}	- The name of the Azure container to use with Tiered Storage. If `null`, the proper
2025-03-31 11:49:50.843364685  ty is disabled. The container must belong to cloud_storage_azure_storage_account.} {Timestamp:2025-03-31 11:49:46.492590732 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_azure_hierarchical_namespace_enabled:{nullopt}	- Whether or not an Azure hierarchical namespace is enabled on the `cloud_storage_azure_storage_account`. If this property is not set, Â´cloud_storage_azure_shared_key` must be set, and each node tries to determine at startup if a hierarchical namespace is enabled. Setting this property to `true` disables the check and treats a hierarchical namespace as active. Setting to `false` disables the check and treats a hierarchical namespace as not active.} {Timestamp:2025-03-31 11:49:46.492593838 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_azure_managed_identity_id:{nullopt}	- The managed identity ID to use for access to the Azure storage account. To use Azure managed identities, you must set `cloud_storage_credentials_source` to `azure_vm_instance_metadata`.} {Timestamp:2025-03-31 11:49:46.492597184 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_azure_shared_key:{nullopt}	- The shared key to be used for Azure Shared Key authentication with the Azure storage account configured by `cloud_storage_azure_storage_account`.  If `null`, the property is disabled. Redpanda expects this key string to be Base64 encoded.} {Timestamp:2025-03-31 11:49:46.492599468 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_azure_storage_account:{nullopt}	- The name of the Azure storage account to use with Tiered Storage. If `null`, the property is disabled.} {Timestamp:2025-03-31 11:49:46.492602554 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_backend:unknown	- Optional object storage backend variant used to select API capabilities. If not supplied, this will be inferred from other configuration properties. Accepted values: [`unknown`, `aws`, `google_s3_compat`, `azure`, `minio`]} {Timestamp:2025-03-31 11:49:46.49260578 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_background_jobs_quota:5000	- The total number of requests the object storage background jobs can make during one background housekeeping run. This is a per-shard limit. Adjusting this limit can optimize object storage traffic and impact shard performance.} {Timestamp:2025-03-31 11:49:46.492607724 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_bucket:{nullopt}	- AWS or GCP bucket or container that should be used to store data.} {Timestamp:2025-03-31 11:49:46.492612443 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_check_interval:5000	- Minimum interval between Tiered Storage cache trims, measured in milliseconds. This setting dictates the cooldown period after a cache trim operation before another trim can occur. If a cache fetch operation requests a trim but the interval since the last trim has not yet passed, the trim will be postponed until this cooldown expires. Adjusting this interval helps manage the balance between cache size and retrieval performance.} {Timestamp:2025-03-31 11:49:46.492614987 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_chunk_size:16777216	- Size of chunks of segments downloaded into object storage cache. Reduces space usage by only downloading the necessary chunk from a segment.} {Timestamp:2025-03-31 11:49:46.492618033 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_max_objects:100000	- Maximum number of objects that may be held in the Tiered Storage cache.  This applies simultaneously with `cloud_storage_cache_size`, and whichever limit is hit first will trigger trimming of the cache.} {Timestamp:2025-03-31 11:49:46.492620858 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_num_buckets:0	- Divide the object storage cache across the specified number of buckets. This only works for objects with randomized prefixes. The names are not changed when the value is set to zero.} {Timestamp:2025-03-31 11:49:46.492623323 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_size:0	- Maximum size of object storage cache. If both this property and cloud_storage_cache_size_percent are set, Redpanda uses the minimum of the two.} {Timestamp:2025-03-31 11:49:46.492628583 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_size_percent:{20}	- Maximum size of the cloud cache as a percentage of unreserved disk space disk_reservation_percent. The default value for this option is tuned for a shared disk configuration. Consider increasing the value if using a dedicated cache disk. The property <<cloud_storage_cache_size,`cloud_storage_cache_size`>> controls the same limit expressed as a fixed number of bytes. If both `cloud_storage_cache_size` and `cloud_storage_cache_size_percent` are set, Redpanda uses the minimum of the two.} {Timestamp:2025-03-31 11:49:46.492632961 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_trim_carryover_bytes:0	- The cache performs a recursive directory inspection during the cache trim. The information obtained during the inspection can be carried over to the next trim operation. This parameter sets a limit on the memory occupied by objects that can be carried over from one trim to next, and allows cache to quickly unblock readers before starting the directory inspection (deprecated)} {Timestamp:2025-03-31 11:49:46.492635866 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_trim_threshold_percent_objects:{nullopt}	- Trim is triggered when the cache reaches this percent of the maximum object count. If this is unset, the default behavioris to start trim when the cache is about 100% full.} {Timestamp:2025-03-31 11:49:46.492638802 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_trim_threshold_percent_size:{nullopt}	- Trim is triggered when the cache reaches this percent of the maximum cache size. If this is unset, the default behavioris to start trim when the cache is about 100% full.} {Timestamp:2025-03-31 11:49:46.492641938 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_trim_walk_concurrency:1	- The maximum number of concurrent tasks launched for directory walk during cache trimming. A higher number allows cache trimming to run faster but can cause latency spikes due to increased pressure on I/O subsystem and syscall threads.} {Timestamp:2025-03-31 11:49:46.492643831 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_chunk_eviction_strategy:eager	- Selects a strategy for evicting unused cache chunks.} {Timestamp:2025-03-31 11:49:46.492645715 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_chunk_prefetch:0	- Number of chunks to prefetch ahead of every downloaded chunk} {Timestamp:2025-03-31 11:49:46.49264866 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cluster_metadata_num_consumer_groups_per_upload:1000	- Number of groups to upload in a single snapshot object during consumer offsets upload. Setting a lower value will mean a larger number of smaller snapshots are uploaded.} {Timestamp:2025-
2025-03-31 11:49:50.843406263  03-31 11:49:46.492650634 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cluster_metadata_retries:5	- Number of attempts metadata operations may be retried.} {Timestamp:2025-03-31 11:49:46.492652588 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cluster_metadata_upload_interval_ms:3600000	- Time interval to wait between cluster metadata uploads.} {Timestamp:2025-03-31 11:49:46.492654161 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cluster_metadata_upload_timeout_ms:60000	- Timeout for cluster metadata uploads.} {Timestamp:2025-03-31 11:49:46.492683907 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_credentials_host:{nullopt}	- The hostname to connect to for retrieving role based credentials. Derived from cloud_storage_credentials_source if not set. Only required when using IAM role based access. To authenticate using access keys, see `cloud_storage_access_key`.} {Timestamp:2025-03-31 11:49:46.492688275 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_credentials_source:config_file	- The source of credentials used to authenticate to object storage services. Required for cluster provider authentication with IAM roles. To authenticate using access keys, see cloud_storage_access_key`. Accepted values: `config_file`, `aws_instance_metadata`, `sts, gcp_instance_metadata`, `azure_vm_instance_metadata`, `azure_aks_oidc_federation` } {Timestamp:2025-03-31 11:49:46.492690329 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_crl_file:{nullopt}	- Path to certificate revocation list for `cloud_storage_trust_file`.} {Timestamp:2025-03-31 11:49:46.492692232 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_disable_archiver_manager:1	- Use legacy upload mode and do not start archiver_manager.} {Timestamp:2025-03-31 11:49:46.492694336 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_disable_chunk_reads:0	- Disable chunk reads and switch back to legacy mode where full segments are downloaded.} {Timestamp:2025-03-31 11:49:46.492697051 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_disable_metadata_consistency_checks:1	- Disable all metadata consistency checks. This will allow redpanda to replay logs with inconsistent tiered-storage metadata. Normally, this option should be disabled.} {Timestamp:2025-03-31 11:49:46.492699666 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_disable_read_replica_loop_for_tests:0	- Begins the read replica sync loop in tiered-storage-enabled topic partitions. The property exists to simplify testing and shouldn't be set in production.} {Timestamp:2025-03-31 11:49:46.492702662 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_disable_remote_labels_for_tests:0	- If 'true', Redpanda disables remote labels and falls back on the hash-based object naming scheme for new topics. This property exists to simplify testing and shouldn't be set in production.} {Timestamp:2025-03-31 11:49:46.492704405 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_disable_tls:0	- Disable TLS for all object storage connections.} {Timestamp:2025-03-31 11:49:46.492707331 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_disable_upload_consistency_checks:0	- Disable all upload consistency checks. This will allow redpanda to upload logs with gaps and replicate metadata with consistency violations. Normally, this options should be disabled.} {Timestamp:2025-03-31 11:49:46.492709895 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_disable_upload_loop_for_tests:0	- Begins the upload loop in tiered-storage-enabled topic partitions. The property exists to simplify testing and shouldn't be set in production.} {Timestamp:2025-03-31 11:49:46.492711599 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_enable_compacted_topic_reupload:1	- Enable re-uploading data for compacted topics} {Timestamp:2025-03-31 11:49:46.492713312 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_enable_remote_read:0	- Default remote read config value for new topics} {Timestamp:2025-03-31 11:49:46.492715045 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_enable_remote_write:0	- Default remote write value for new topics} {Timestamp:2025-03-31 11:49:46.49271747 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_enable_scrubbing:0	- Enable scrubbing of cloud storage partitions. The scrubber validates the integrity of data and metadata uploaded to cloud storage.} {Timestamp:2025-03-31 11:49:46.492720054 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_enable_segment_merging:1	- Enables adjacent segment merging. The segments are reuploaded if there is an opportunity for that and if it will improve the tiered-storage performance} {Timestamp:2025-03-31 11:49:46.492722148 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_enabled:0	- Enable object storage. Must be set to `true` to use Tiered Storage or Remote Read Replicas.} {Timestamp:2025-03-31 11:49:46.492724092 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_full_scrub_interval_ms:43200000	- Time interval between a final scrub and the next.} {Timestamp:2025-03-31 11:49:46.492726086 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_garbage_collect_timeout_ms:30000	- Timeout for running the cloud storage garbage collection (ms).} {Timestamp:2025-03-31 11:49:46.492728781 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_graceful_transfer_timeout_ms:{5000}	- Time limit on waiting for uploads to complete before a leadership transfer.  If this is null, leadership transfers will proceed without waiting.} {Timestamp:2025-03-31 11:49:46.492730704 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_housekeeping_interval_ms:300000	- Interval for cloud storage housekeeping tasks.} {Timestamp:2025-03-31 11:49:46.492733239 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_hydrated_chunks_per_segment_ratio:0.7	- The maximum number of chunks per segment that can be hydrated at a time. Above this number, unused chunks will be trimmed.} {Timestamp:2025-03-31 11:49:46.492735984 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_hydration_timeout_ms:600000	- Duration to wait for a hydration request to be fulfilled, if hydration is not completed within this time, the consumer will be notified with a timeout error.} {Timestamp:2025-03-31 11:49:46.49273897 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_idle_threshold_rps:10	- The cloud storage request rate threshold for idle state detection. If the average request rate for the configured period is lower than this threshold the cloud storage is considered being idle.} {Timestamp
2025-03-31 11:49:50.843464191  :2025-03-31 11:49:46.492740733 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_idle_timeout_ms:10000	- Timeout used to detect idle state of the cloud storage API. If the average cloud storage request rate is below this threshold for a configured amount of time the cloud storage is considered idle and the housekeeping jobs are started.} {Timestamp:2025-03-31 11:49:46.492741695 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_initial_backoff_ms:100	- Initial backoff time for exponential backoff algorithm (ms)} {Timestamp:2025-03-31 11:49:46.492742927 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_based_scrub_enabled:0	- Scrubber uses the latest cloud storage inventory report, if available, to check if the required objects exist in the bucket or container.} {Timestamp:2025-03-31 11:49:46.49274411 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_id:redpanda_scrubber_inventory	- The name of the scheduled inventory job created by Redpanda to generate bucket or container inventory reports.} {Timestamp:2025-03-31 11:49:46.492745813 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_max_hash_size_during_parse:67108864	- Maximum bytes of hashes which will be held in memory before writing data to disk during inventory report parsing. Affects the number of files written by inventory service to disk during report parsing, as when this limit is reached new files are written to disk.} {Timestamp:2025-03-31 11:49:46.492746945 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_report_check_interval_ms:21600000	- Time interval between checks for a new inventory report in the cloud storage bucket or container.} {Timestamp:2025-03-31 11:49:46.492773495 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_reports_prefix:redpanda_scrubber_inventory	- The prefix to the path in the cloud storage bucket or container where inventory reports will be placed.} {Timestamp:2025-03-31 11:49:46.492777703 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_self_managed_report_config:0	- If enabled, Redpanda will not attempt to create the scheduled report configuration using cloud storage APIs. The scrubbing process will look for reports in the expected paths in the bucket or container, and use the latest report found. Primarily intended for use in testing and on backends where scheduled inventory reports are not supported.} {Timestamp:2025-03-31 11:49:46.492779736 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_manifest_cache_size:1048576	- Amount of memory that can be used to handle tiered-storage metadata} {Timestamp:2025-03-31 11:49:46.492783053 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_manifest_max_upload_interval_sec:{60000}	- Wait at least this long between partition manifest uploads. Actual time between uploads may be greater than this interval. If this property is not set, or null, metadata will be updated after each segment upload.} {Timestamp:2025-03-31 11:49:46.492784916 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_manifest_upload_timeout_ms:10000	- Manifest upload timeout (ms).} {Timestamp:2025-03-31 11:49:46.492790507 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_materialized_manifest_ttl_ms:10000	- The time interval that determines how long the materialized manifest can stay in cache under contention. This parameter is used for performance tuning. When the spillover manifest is materialized and stored in cache and the cache needs to evict it it will use 'cloud_storage_materialized_manifest_ttl_ms' value as a timeout. The cursor that uses the spillover manifest uses this value as a TTL interval after which it stops referencing the manifest making it available for eviction. This only affects spillover manifests under contention.} {Timestamp:2025-03-31 11:49:46.492797941 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_max_concurrent_hydrations_per_shard:{nullopt}	- Maximum concurrent segment hydrations of remote data per CPU core.  If unset, value of `cloud_storage_max_connections / 2` is used, which means that half of available S3 bandwidth could be used to download data from S3. If the cloud storage cache is empty every new segment reader will require a download. This will lead to 1:1 mapping between number of partitions scanned by the fetch request and number of parallel downloads. If this value is too large the downloads can affect other workloads. In case of any problem caused by the tiered-storage reads this value can be lowered. This will only affect segment hydrations (downloads) but won't affect cached segments. If fetch request is reading from the tiered-storage cache its concurrency will only be limited by available memory.} {Timestamp:2025-03-31 11:49:46.492799734 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_max_connection_idle_time_ms:5000	- Max https connection idle time (ms)} {Timestamp:2025-03-31 11:49:46.492801978 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_max_connections:20	- Maximum simultaneous object storage connections per shard, applicable to upload and download activities.} {Timestamp:2025-03-31 11:49:46.492804563 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_max_materialized_segments_per_shard:{nullopt}	- Maximum concurrent readers of remote data per CPU core.  If unset, value of `topic_partitions_per_shard` multiplied by 2 is used.} {Timestamp:2025-03-31 11:49:46.492806567 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_max_partition_readers_per_shard:{nullopt}	- Maximum partition readers per shard (deprecated)} {Timestamp:2025-03-31 11:49:46.492810444 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_max_segment_readers_per_shard:{nullopt}	- Maximum concurrent I/O cursors of materialized remote segments per CPU core.  If unset, value of `topic_partitions_per_shard` is used, i.e. one segment reader per partition if the shard is at its maximum partition capacity.  These readers are cachedacross Kafka consume requests and store a readahead buffer.} {Timestamp:2025-03-31 11:49:46.492813991 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_max_segments_pending_deletion_per_partition:5000	- The per-partition limit for the number of segments pending deletion from the cloud. Segments can be deleted due to retention or compaction. If this limit is breached and deletion fails, then segments will be orphaned in the cloud and will have to be removed manually} {Timestamp:2025-03-31 11:49:46.492818599 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_max_throughput_per_shard:{1073741824}	- Max throughput used by tiered-storage per shard in bytes per second. This value is an upper bound of the throughput available to the tiered-storage subsystem. This parameter is intended to be used as a safeguard and in tests when we need to set precise throughput value independent of actual storage media. Please use 'cloud_storage_throughput_limit_percent' instead of this parameter in the 
2025-03-31 11:49:50.843505800  production environment.} {Timestamp:2025-03-31 11:49:46.492820503 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_metadata_sync_timeout_ms:10000	- Timeout for SI metadata synchronization.} {Timestamp:2025-03-31 11:49:46.492823709 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_min_chunks_per_segment_threshold:5	- The minimum number of chunks per segment for trimming to be enabled. If the number of chunks in a segment is below this threshold, the segment is small enough that all chunks in it can be hydrated at any given time} {Timestamp:2025-03-31 11:49:46.492825713 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_partial_scrub_interval_ms:3600000	- Time interval between two partial scrubs of the same partition.} {Timestamp:2025-03-31 11:49:46.492827957 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_readreplica_manifest_sync_timeout_ms:30000	- Timeout to check if new data is available for partition in S3 for read replica.} {Timestamp:2025-03-31 11:49:46.49282947 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_reconciliation_interval_ms:	- } {Timestamp:2025-03-31 11:49:46.492831664 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_recovery_temporary_retention_bytes_default:1073741824	- Retention in bytes for topics created during automated recovery} {Timestamp:2025-03-31 11:49:46.492834469 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_recovery_topic_validation_depth:10	- Number of metadata segments to validate, from newest to oldest, when `cloud_storage_recovery_topic_validation_mode` is set to `check_manifest_and_segment_metadata`.} {Timestamp:2025-03-31 11:49:46.492841793 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_recovery_topic_validation_mode:check_manifest_existence	- Validation performed before recovering a topic from object storage. In case of failure, the reason for the failure appears as `ERROR` lines in the Redpanda application log. For each topic, this reports errors for all partitions, but for each partition, only the first error is reported. This property accepts the following parameters: `no_check`: Skips the checks for topic recovery. `check_manifest_existence`:  Runs an existence check on each `partition_manifest`. Fails if there are connection issues to the object storage. `check_manifest_and_segment_metadata`: Downloads the manifest and runs a consistency check, comparing the metadata with the cloud storage objects. The process fails if metadata references any missing cloud storage objects.} {Timestamp:2025-03-31 11:49:46.492843797 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_region:{nullopt}	- Cloud provider region that houses the bucket or container used for storage.} {Timestamp:2025-03-31 11:49:46.492862632 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_roles_operation_timeout_ms:30000	- Timeout for IAM role related operations (ms)} {Timestamp:2025-03-31 11:49:46.492864766 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_scrubbing_interval_jitter_ms:600000	- Jitter applied to the cloud storage scrubbing interval.} {Timestamp:2025-03-31 11:49:46.492866429 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_secret_key:{nullopt}	- Cloud provider secret key.} {Timestamp:2025-03-31 11:49:46.492868723 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_segment_max_upload_interval_sec:{3600000}	- Time that segment can be kept locally without uploading it to the remote storage (sec).} {Timestamp:2025-03-31 11:49:46.492870968 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_segment_size_min:{nullopt}	- Smallest acceptable segment size in the cloud storage. Default: cloud_storage_segment_size_target/2} {Timestamp:2025-03-31 11:49:46.492873001 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_segment_size_target:{nullopt}	- Desired segment size in the cloud storage. Default: segment.bytes} {Timestamp:2025-03-31 11:49:46.492874795 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_segment_upload_timeout_ms:30000	- Log segment upload timeout (ms)} {Timestamp:2025-03-31 11:49:46.492879043 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_spillover_manifest_max_segments:{nullopt}	- Maximum number of elements in the spillover manifest that can be offloaded to the cloud storage. This property is similar to 'cloud_storage_spillover_manifest_size' but it triggers spillover based on number of segments instead of the size of the manifest in bytes. The property exists to simplify testing and shouldn't be set in the production environment} {Timestamp:2025-03-31 11:49:46.492882549 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_spillover_manifest_size:{65536}	- The size of the manifest which can be offloaded to the cloud. If the size of the local manifest stored in redpanda exceeds cloud_storage_spillover_manifest_size x2 the spillover mechanism will split the manifest into two parts and one of them will be uploaded to S3.} {Timestamp:2025-03-31 11:49:46.49288823 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_throughput_limit_percent:{50}	- Max throughput used by tiered-storage per node expressed as a percentage of the disk bandwidth. If the server has several disks Redpanda will take into account only the one which is used to store tiered-storage cache. Note that even if the tiered-storage is allowed to use full bandwidth of the disk (100%) it won't necessary use it in full. The actual usage depend on your workload and the state of the tiered-storage cache. This parameter is a safeguard that prevents tiered-storage from using too many system resources and not a performance tuning knob.} {Timestamp:2025-03-31 11:49:46.492890274 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_topic_purge_grace_period_ms:30000	- Grace period during which the purger will refuse to purge the topic.} {Timestamp:2025-03-31 11:49:46.492892428 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_trust_file:{nullopt}	- Path to certificate that should be used to validate server certificate during TLS handshake.} {Timestamp:2025-03-31 11:49:46.492894201 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_upload_ctrl_d_coeff:0	- derivative coefficient for upload PID controller.} {Timestamp:2025-03-31 11:49:46.492896145 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_upload_ctrl_max_shares:1000	- maximum number of IO and CPU shares that archival upload can use} {Timestamp:2025-03-31 11:49:46.492898088 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_upload_ctrl_min_shares:100	- minimum number of IO and CPU shares that archival upload can use} {Timestamp:2025-03-31 11:49:46.492899892 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_upload_ctrl_p_c
2025-03-31 11:49:50.843555202  oeff:-2	- proportional coefficient for upload PID controller} {Timestamp:2025-03-31 11:49:46.492901465 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_upload_ctrl_update_interval_ms:60000	- } {Timestamp:2025-03-31 11:49:46.492903479 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_upload_loop_initial_backoff_ms:100	- Initial backoff interval when there is nothing to upload for a partition (ms).} {Timestamp:2025-03-31 11:49:46.492905512 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_upload_loop_max_backoff_ms:10000	- Max backoff interval when there is nothing to upload for a partition (ms).} {Timestamp:2025-03-31 11:49:46.492911664 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_url_style:{nullopt}	- Specifies the addressing style to use for Amazon S3 requests. This configuration determines how S3 bucket URLs are formatted. You can choose between: `virtual_host`, (for example, `<bucket-name>.s3.amazonaws.com`), `path`, (for example, `s3.amazonaws.com/<bucket-name>`), and `null`. Path style is supported for backward compatibility with legacy systems. When this property is not set or is `null`, the client tries to use `virtual_host` addressing. If the initial request fails, the client automatically tries the `path` style. If neither addressing style works, Redpanda terminates the startup, requiring manual configuration to proceed.} {Timestamp:2025-03-31 11:49:46.492913387 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cluster_id:{3953b6f3-7800-4aef-aad3-9890c911f179}	- Cluster identifier.} {Timestamp:2025-03-31 11:49:46.492915271 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.compacted_log_segment_size:268435456	- Size (in bytes) for each compacted log segment.} {Timestamp:2025-03-31 11:49:46.492917685 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.compaction_ctrl_backlog_size:{nullopt}	- Target backlog size for compaction controller. If not set the max backlog size is configured to 80% of total disk space available.} {Timestamp:2025-03-31 11:49:46.492919549 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.compaction_ctrl_d_coeff:0.2	- Derivative coefficient for compaction PID controller.} {Timestamp:2025-03-31 11:49:46.492921312 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.compaction_ctrl_i_coeff:0	- Integral coefficient for compaction PID controller.} {Timestamp:2025-03-31 11:49:46.492923236 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.compaction_ctrl_max_shares:1000	- Maximum number of I/O and CPU shares that compaction process can use.} {Timestamp:2025-03-31 11:49:46.492925189 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.compaction_ctrl_min_shares:10	- Minimum number of I/O and CPU shares that compaction process can use.} {Timestamp:2025-03-31 11:49:46.492927944 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.compaction_ctrl_p_coeff:-12.5	- Proportional coefficient for compaction PID controller. This must be negative, because the compaction backlog should decrease when the number of compaction shares increases.} {Timestamp:2025-03-31 11:49:46.492929447 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.compaction_ctrl_update_interval_ms:30000	- } {Timestamp:2025-03-31 11:49:46.492931511 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.controller_backend_housekeeping_interval_ms:1000	- Interval between iterations of controller backend housekeeping loop.} {Timestamp:2025-03-31 11:49:46.492933976 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.controller_log_accummulation_rps_capacity_acls_and_users_operations:{nullopt}	- Maximum capacity of rate limit accumulation in controller ACLs and users operations limit.} {Timestamp:2025-03-31 11:49:46.49293645 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.controller_log_accummulation_rps_capacity_configuration_operations:{nullopt}	- Maximum capacity of rate limit accumulation in controller configuration operations limit.} {Timestamp:2025-03-31 11:49:46.492938785 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.controller_log_accummulation_rps_capacity_move_operations:{nullopt}	- Maximum capacity of rate limit accumulation in controller move operations limit.} {Timestamp:2025-03-31 11:49:46.492955596 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.controller_log_accummulation_rps_capacity_node_management_operations:{nullopt}	- Maximum capacity of rate limit accumulation in controller node management operations limit.} {Timestamp:2025-03-31 11:49:46.492958021 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.controller_log_accummulation_rps_capacity_topic_operations:{nullopt}	- Maximum capacity of rate limit accumulationin controller topic operations limit} {Timestamp:2025-03-31 11:49:46.492960425 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.controller_snapshot_max_age_sec:60000	- Maximum amount of time before Redpanda attempts to create a controller snapshot after a new controller command appears.} {Timestamp:2025-03-31 11:49:46.492961848 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.coproc_max_batch_size:	- } {Timestamp:2025-03-31 11:49:46.492963231 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.coproc_max_inflight_bytes:	- } {Timestamp:2025-03-31 11:49:46.492964613 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.coproc_max_ingest_bytes:	- } {Timestamp:2025-03-31 11:49:46.492966046 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.coproc_offset_flush_interval_ms:	- } {Timestamp:2025-03-31 11:49:46.49296824 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.core_balancing_continuous:0	- If set to `true`, move partitions between cores in runtime to maintain balanced partition distribution.} {Timestamp:2025-03-31 11:49:46.492970274 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.core_balancing_debounce_timeout:10000	- Interval, in milliseconds, between trigger and invocation of core balancing.} {Timestamp:2025-03-31 11:49:46.492972939 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.core_balancing_on_core_count_change:1	- If set to `true`, and if after a restart the number of cores changes, Redpanda will move partitions between cores to maintain balanced partition distribution.} {Timestamp:2025-03-31 11:49:46.492974552 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cpu_profiler_enabled:0	- Enables CPU profiling for Redpanda.} {Timestamp:2025-03-31 11:49:46.492976275 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.cpu_profiler_sample_period_ms:100	- The sample period for the CPU profiler.} {Timestamp:2025-03-31 11:49:46.492978058 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.create_topic_timeout_ms:2000	- Timeout, in milliseconds, to wait for new topic creation.} {Timestamp:2025
2025-03-31 11:49:50.843613091  -03-31 11:49:46.492980162 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_binary_max_size:10485760	- The maximum size for a deployable WebAssembly binary that the broker can store.} {Timestamp:2025-03-31 11:49:46.492982046 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_commit_interval_ms:3000	- The commit interval at which data transforms progress.} {Timestamp:2025-03-31 11:49:46.492986434 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_enabled:0	- Enables WebAssembly-powered data transforms directly in the broker. When `data_transforms_enabled` is set to `true`, Redpanda reserves memory for data transforms, even if no transform functions are currently deployed. This memory reservation ensures that adequate resources are available for transform functions when they are needed, but it also means that some memory is allocated regardless of usage.} {Timestamp:2025-03-31 11:49:46.49298928 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_logging_buffer_capacity_bytes:512000	- Buffer capacity for transform logs, per shard. Buffer occupancy is calculated as the total size of buffered log messages; that is, logs emitted but not yet produced.} {Timestamp:2025-03-31 11:49:46.492991774 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_logging_flush_interval_ms:500	- Flush interval for transform logs. When a timer expires, pending logs are collected and published to the `transform_logs` topic.} {Timestamp:2025-03-31 11:49:46.492993788 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_logging_line_max_bytes:1024	- Transform log lines truncate to this length. Truncation occurs after any character escaping.} {Timestamp:2025-03-31 11:49:46.492997485 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_per_core_memory_reservation:20971520	- The amount of memory to reserve per core for data transform (Wasm) virtual machines. Memory is reserved on boot. The maximum number of functions that can be deployed to a cluster is equal to `data_transforms_per_core_memory_reservation` / `data_transforms_per_function_memory_limit`.} {Timestamp:2025-03-31 11:49:46.493000982 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_per_function_memory_limit:2097152	- The amount of memory to give an instance of a data transform (Wasm) virtual machine. The maximum number of functions that can be deployed to a cluster is equal to `data_transforms_per_core_memory_reservation` / `data_transforms_per_function_memory_limit`.} {Timestamp:2025-03-31 11:49:46.493003176 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_read_buffer_memory_percentage:45	- The percentage of available memory in the transform subsystem to use for read buffers.} {Timestamp:2025-03-31 11:49:46.4930056 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_runtime_limit_ms:3000	- The maximum amount of runtime to start up a data transform, and the time it takes for a single record to be transformed.} {Timestamp:2025-03-31 11:49:46.493007814 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_write_buffer_memory_percentage:45	- The percentage of available memory in the transform subsystem to use for write buffers.} {Timestamp:2025-03-31 11:49:46.493010129 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.datalake_coordinator_snapshot_max_delay_secs:900000	- Maximum amount of time the coordinator waits to snapshot after a command appears in the log.} {Timestamp:2025-03-31 11:49:46.493012814 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.debug_bundle_auto_removal_seconds:{nullopt}	- If set, how long debug bundles are kept in the debug bundle storage directory after they are created. If not set, debug bundles are kept indefinitely.} {Timestamp:2025-03-31 11:49:46.49301609 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.debug_bundle_storage_dir:{nullopt}	- Path to the debug bundle storage directory. Note: Changing this path does not clean up existing debug bundles. If not set, the debug bundle is stored in the Redpanda data directory specified in the redpanda.yaml broker configuration file.} {Timestamp:2025-03-31 11:49:46.493018264 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.debug_load_slice_warning_depth:{nullopt}	- The recursion depth after which debug logging is enabled automatically for the log reader.} {Timestamp:2025-03-31 11:49:46.493021159 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.default_leaders_preference:none	- Default settings for preferred location of topic partition leaders. It can be either "none" (no preference), or "racks:<rack1>,<rack2>,..." (prefer brokers with rack id from the list).} {Timestamp:2025-03-31 11:49:46.493022853 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.default_num_windows:10	- Default number of quota tracking windows.} {Timestamp:2025-03-31 11:49:46.493024526 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.default_topic_partitions:1	- Default number of partitions per topic.} {Timestamp:2025-03-31 11:49:46.493026159 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.default_topic_replications:1	- Default replication factor for new topics.} {Timestamp:2025-03-31 11:49:46.493027291 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.default_window_sec:1000	- Default quota tracking window size in milliseconds.} {Timestamp:2025-03-31 11:49:46.493028092 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.development_enable_cloud_topics:0	- Enable cloud topics.} {Timestamp:2025-03-31 11:49:46.493029014 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.development_feature_property_testing_only:0	- Development feature property for testing only.} {Timestamp:2025-03-31 11:49:46.493029796 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.disable_batch_cache:0	- Disable batch cache in log manager.} {Timestamp:2025-03-31 11:49:46.493046016 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.disable_cluster_recovery_loop_for_tests:0	- Disables the cluster recovery loop. This property is used to simplify testing and should not be set in production.} {Timestamp:2025-03-31 11:49:46.49304795 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.disable_metrics:0	- Disable registering the metrics exposed on the internal `/metrics` endpoint.} {Timestamp:2025-03-31 11:49:46.493049863 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.disable_public_metrics:0	- Disable registering the metrics exposed on the `/public_metrics` endpoint.} {Timestamp:2025-03-31 11:49:46.493053961 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.disk_reservation_percent:25	- The percentage of total disk capacity that Redpanda will avoid using. This applies both when cloud cache and log data share a disk, as well as when cloud cache uses a dedicated disk. It is recommended to not run disks n
2025-03-31 11:49:50.843654819  ear capacity to avoid blocking I/O due to low disk space, as well as avoiding performance issues associated with SSD garbage collection.} {Timestamp:2025-03-31 11:49:46.493055674 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.election_timeout_ms:1500	- Election timeout expressed in milliseconds.} {Timestamp:2025-03-31 11:49:46.493057007 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.enable_admin_api:	- } {Timestamp:2025-03-31 11:49:46.49305891 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.enable_auto_rebalance_on_node_add:0	- Enable automatic partition rebalancing when new nodes are added} {Timestamp:2025-03-31 11:49:46.493061024 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.enable_cluster_metadata_upload_loop:1	- Enables cluster metadata uploads. Required for whole cluster restore.} {Timestamp:2025-03-31 11:49:46.493062818 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.enable_controller_log_rate_limiting:0	- Limits the write rate for the controller log.} {Timestamp:2025-03-31 11:49:46.49306408 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.enable_coproc:	- } {Timestamp:2025-03-31 11:49:46.493068759 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.enable_developmental_unrecoverable_data_corrupting_features:	- Development features should never be enabled in a production cluster, or any cluster where stability, data loss, or the ability to upgrade are a concern. To enable experimental features, set the value of this configuration option to the current unix epoch expressed in seconds. The value must be within one hour of the current time on the broker.Once experimental features are enabled they cannot be disabled} {Timestamp:2025-03-31 11:49:46.493070372 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.enable_idempotence:1	- Enable idempotent producers.} {Timestamp:2025-03-31 11:49:46.493072035 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.enable_leader_balancer:1	- Enable automatic leadership rebalancing.} {Timestamp:2025-03-31 11:49:46.493076663 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.enable_metrics_reporter:1	- Enable the cluster metrics reporter. If `true`, the metrics reporter collects and exports to Redpanda Data a set of customer usage metrics at the interval set by `metrics_reporter_report_interval`. The cluster metrics of the metrics reporter are different from the monitoring metrics. * The metrics reporter exports customer usage metrics for consumption by Redpanda Data.* Monitoring metrics are exported for consumption by Redpanda users.} {Timestamp:2025-03-31 11:49:46.493078357 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.enable_mpx_extensions:0	- Enable Redpanda extensions for MPX.} {Timestamp:2025-03-31 11:49:46.493079859 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.enable_pid_file:1	- Enable PID file. You should not need to change.} {Timestamp:2025-03-31 11:49:46.493081442 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.enable_rack_awareness:0	- Enable rack-aware replica assignment.} {Timestamp:2025-03-31 11:49:46.493083777 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.enable_sasl:0	- Enable SASL authentication for Kafka connections. Authorization is required to modify this property. See also `kafka_enable_authorization`.} {Timestamp:2025-03-31 11:49:46.493088025 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.enable_schema_id_validation:none	- Mode to enable server-side schema ID validation. Accepted Values: * `none`: Schema validation is disabled (no schema ID checks are done). Associated topic properties cannot be modified. * `redpanda`: Schema validation is enabled. Only Redpanda topic properties are accepted. * `compat`: Schema validation is enabled. Both Redpanda and compatible topic properties are accepted.} {Timestamp:2025-03-31 11:49:46.493089688 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.enable_transactions:1	- Enable transactions (atomic writes).} {Timestamp:2025-03-31 11:49:46.493091802 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.enable_usage:0	- Enables the usage tracking mechanism, storing windowed history of kafka/cloud_storage metrics over time.} {Timestamp:2025-03-31 11:49:46.493094086 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.features_auto_enable:1	- Whether new feature flags auto-activate after upgrades (true) or must wait for manual activation via the Admin API (false).} {Timestamp:2025-03-31 11:49:46.4930959 +0000 UTC Content:INFO  2025-03-31 11:49:46,492 [shard  0:main] main - application.cc:849 - redpanda.fetch_max_bytes:57671680	- Maximum number of bytes returned in a fetch request.} {Timestamp:2025-03-31 11:49:46.493097593 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.fetch_pid_d_coeff:0	- Derivative coefficient for fetch PID controller.} {Timestamp:2025-03-31 11:49:46.493099256 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.fetch_pid_i_coeff:0.01	- Integral coefficient for fetch PID controller.} {Timestamp:2025-03-31 11:49:46.49310119 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.fetch_pid_max_debounce_ms:100	- The maximum debounce time the fetch PID controller will apply, in milliseconds.} {Timestamp:2025-03-31 11:49:46.493102943 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.fetch_pid_p_coeff:100	- Proportional coefficient for fetch PID controller.} {Timestamp:2025-03-31 11:49:46.493105207 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.fetch_pid_target_utilization_fraction:0.2	- A fraction, between 0 and 1, for the target reactor utilization of the fetch scheduling group.} {Timestamp:2025-03-31 11:49:46.493113473 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.fetch_read_strategy:non_polling	- The strategy used to fulfill fetch requests. * `polling`: Repeatedly polls every partition in the request for new data. The polling interval is set by `fetch_reads_debounce_timeout` (deprecated). * `non_polling`: The backend is signaled when a partition has new data, so Redpanda does not need to repeatedly read from every partition in the fetch. Redpanda Data recommends using this value for most workloads, because it can improve fetch latency and CPU utilization. * `non_polling_with_debounce`: This option behaves like `non_polling`, but it includes a debounce mechanism with a fixed delay specified by `fetch_reads_debounce_timeout` at the start of each fetch. By introducing this delay, Redpanda can accumulate more data before processing, leading to fewer fetch operations and returning larger amounts of data. Enabling this option reduces reactor utilization, but it may also increase end-to-end latency.} {Timestamp:2025-03-31 11:49:46.493115637 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.fetch_reads_debounce_timeout:1	- Time to wait for the next read in fetch requests when the requested minimum bytes was not reached.} {Timestamp:2025-03-31 11:49:46.493117741 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.fetch_s
2025-03-31 11:49:50.843697890  ession_eviction_timeout_ms:60000	- Time duration after which the inactive fetch session is removed from the fetch session cache. Fetch sessions are used to implement the incremental fetch requests where a consumer does not send all requested partitions to the server but the server tracks them for the consumer.} {Timestamp:2025-03-31 11:49:46.493118462 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.find_coordinator_timeout_ms:	- } {Timestamp:2025-03-31 11:49:46.493119203 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.full_raft_configuration_recovery_pattern:	- } {Timestamp:2025-03-31 11:49:46.493138059 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.group_initial_rebalance_delay:3000	- Delay added to the rebalance phase to wait for new members.} {Timestamp:2025-03-31 11:49:46.493141365 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.group_max_session_timeout_ms:300000	- The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.} {Timestamp:2025-03-31 11:49:46.49314403 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.group_min_session_timeout_ms:6000	- The minimum allowed session timeout for registered consumers. Shorter timeouts result in quicker failure detection at the cost of more frequent consumer heartbeating, which can overwhelm broker resources.} {Timestamp:2025-03-31 11:49:46.493145723 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.group_new_member_join_timeout:30000	- Timeout for new member joins.} {Timestamp:2025-03-31 11:49:46.493147737 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.group_offset_retention_check_ms:600000	- Frequency rate at which the system should check for expired group offsets.} {Timestamp:2025-03-31 11:49:46.49314962 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.group_offset_retention_sec:{604800000}	- Consumer group offset retention seconds. To disable offset retention, set this to null.} {Timestamp:2025-03-31 11:49:46.493151243 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.group_topic_partitions:16	- Number of partitions in the internal group membership topic.} {Timestamp:2025-03-31 11:49:46.493152696 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.health_manager_tick_interval:180000	- How often the health manager runs.} {Timestamp:2025-03-31 11:49:46.49315457 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.health_monitor_max_metadata_age:10000	- Maximum age of the metadata cached in the health monitor of a non-controller broker.} {Timestamp:2025-03-31 11:49:46.493156443 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.health_monitor_tick_interval:10000	- How often health monitor refresh cluster state} {Timestamp:2025-03-31 11:49:46.493158507 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.http_authentication:{BASIC}	- A list of supported HTTP authentication mechanisms. Accepted Values: `BASIC`, `OIDC`} {Timestamp:2025-03-31 11:49:46.493161362 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.iceberg_catalog_base_location:redpanda-iceberg-catalog	- Base path for the cloud object storage-backed Iceberg catalog. After Iceberg is enabled, do not change this value.} {Timestamp:2025-03-31 11:49:46.49316543 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.iceberg_catalog_commit_interval_ms:60000	- The frequency at which the Iceberg coordinator commits topic files to the catalog. This is the interval between commit transactions across all topics monitored by the coordinator, not the interval between individual commits.} {Timestamp:2025-03-31 11:49:46.493167865 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.iceberg_catalog_type:object_storage	- Iceberg catalog type that Redpanda will use to commit table metadata updates. Supported types: 'rest', 'object_storage'} {Timestamp:2025-03-31 11:49:46.493170439 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.iceberg_delete:1	- Default value for the redpanda.iceberg.delete topic property that determines if the corresponding Iceberg table is deleted upon deleting the topic.} {Timestamp:2025-03-31 11:49:46.493174737 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.iceberg_enabled:0	- Enables the translation of topic data into Iceberg tables. Setting iceberg_enabled to true activates the feature at the cluster level, but each topic must also set the redpanda.iceberg.enabled topic-level property to true to use it. If iceberg_enabled is set to false, the feature is disabled for all topics in the cluster, overriding any topic-level settings.} {Timestamp:2025-03-31 11:49:46.493177252 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_client_id:{nullopt}	- Iceberg REST catalog user ID. This ID is used to query the catalog API for the OAuth token. Required if catalog type is set to `rest`} {Timestamp:2025-03-31 11:49:46.493179577 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_client_secret:{nullopt}	- Secret to authenticate against Iceberg REST catalog. Required if catalog type is set to `rest`} {Timestamp:2025-03-31 11:49:46.49318161 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_crl_file:{nullopt}	- Path to certificate revocation list for `iceberg_rest_catalog_trust_file`.} {Timestamp:2025-03-31 11:49:46.493183364 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_endpoint:{nullopt}	- URL of Iceberg REST catalog endpoint} {Timestamp:2025-03-31 11:49:46.493185668 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_prefix:{nullopt}	- Prefix part of the Iceberg REST catalog URL. Prefix is appended to the catalog path f.e. '/v1/{prefix}/namespaces'} {Timestamp:2025-03-31 11:49:46.493188032 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_request_timeout_ms:10000	- Maximum length of time that Redpanda waits for a response from the REST catalog before aborting the request} {Timestamp:2025-03-31 11:49:46.493190938 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_token:{nullopt}	- Token used to access the REST Iceberg catalog. If the token is present, Redpanda ignores credentials stored in the properties iceberg_rest_catalog_client_id and iceberg_rest_catalog_client_secret} {Timestamp:2025-03-31 11:49:46.493193042 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_trust_file:{nullopt}	- Path to a file containing a certificate chain to trust for the REST Iceberg catalog} {Timestamp:2025-03-31 11:49:46.493195727 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.id_allocator_batch_size:1000	- The ID allocator allocates messages in batches (each batch is a one log record) and then serves requests from memory without touching the log until the batch is exhausted.} {Timestamp:2025-03-31 11:49:46.49319810
2025-03-31 11:49:50.843754817  1 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.id_allocator_log_capacity:100	- Capacity of the `id_allocator` log in number of batches. After it reaches `id_allocator_stm`, it truncates the log's prefix.} {Timestamp:2025-03-31 11:49:46.493199534 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.id_allocator_replication:	- } {Timestamp:2025-03-31 11:49:46.49320279 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.initial_retention_local_target_bytes_default:{nullopt}	- Initial local retention size target for partitions of topics with Tiered Storage enabled. If no initial local target retention is configured all locally retained data will be delivered to learner when joining partition replica set.} {Timestamp:2025-03-31 11:49:46.493206116 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.initial_retention_local_target_ms_default:{nullopt}	- Initial local retention time target for partitions of topics with Tiered Storage enabled. If no initial local target retention is configured all locally retained data will be delivered to learner when joining partition replica set.} {Timestamp:2025-03-31 11:49:46.49320792 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.internal_topic_replication_factor:3	- Target replication factor for internal topics.} {Timestamp:2025-03-31 11:49:46.493209723 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.join_retry_timeout_ms:5000	- Time between cluster join retries in milliseconds.} {Timestamp:2025-03-31 11:49:46.493211637 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_admin_topic_api_rate:{nullopt}	- Target quota rate (partition mutations per default_window_sec)} {Timestamp:2025-03-31 11:49:46.493213901 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_batch_max_bytes:1048576	- Maximum size of a batch processed by the server. If the batch is compressed, the limit applies to the compressed batch size.} {Timestamp:2025-03-31 11:49:46.493246502 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_client_group_byte_rate_quota:{}	- Per-group target produce quota byte rate (bytes per second). Client is considered part of the group if client_id contains clients_prefix.} {Timestamp:2025-03-31 11:49:46.493249167 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_client_group_fetch_byte_rate_quota:{}	- Per-group target fetch quota byte rate (bytes per second). Client is considered part of the group if client_id contains clients_prefix} {Timestamp:2025-03-31 11:49:46.493251081 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_connection_rate_limit:{nullopt}	- Maximum connections per second for one core. If `null` (the default), then the number of connections per second is unlimited.} {Timestamp:2025-03-31 11:49:46.493253125 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_connection_rate_limit_overrides:{}	- Overrides the maximum connections per second for one core for the specified IP addresses (for example, `['127.0.0.1:90', '50.20.1.1:40']`)} {Timestamp:2025-03-31 11:49:46.493254798 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_connections_max:{nullopt}	- Maximum number of Kafka client connections per broker. If `null`, the property is disabled.} {Timestamp:2025-03-31 11:49:46.493256791 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_connections_max_overrides:{}	- A list of IP addresses for which Kafka client connection limits are overridden and don't apply. For example, `(['127.0.0.1:90', '50.20.1.1:40']).`} {Timestamp:2025-03-31 11:49:46.493258665 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_connections_max_per_ip:{nullopt}	- Maximum number of Kafka client connections per IP address, per broker. If `null`, the property is disabled.} {Timestamp:2025-03-31 11:49:46.49326161 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_enable_authorization:{nullopt}	- Flag to require authorization for Kafka connections. If `null`, the property is disabled, and authorization is instead enabled by enable_sasl. * `null`: Ignored. Authorization is enabled with `enable_sasl`: `true` * `true`: authorization is required. * `false`: authorization is disabled.} {Timestamp:2025-03-31 11:49:46.493263584 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_enable_describe_log_dirs_remote_storage:1	- Whether to include Tiered Storage as a special remote:// directory in `DescribeLogDirs Kafka` API requests.} {Timestamp:2025-03-31 11:49:46.493264977 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_enable_partition_reassignment:1	- Enable the Kafka partition reassignment API.} {Timestamp:2025-03-31 11:49:46.493266249 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_group_recovery_timeout_ms:30000	- Kafka group recovery timeout.} {Timestamp:2025-03-31 11:49:46.493268173 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_max_bytes_per_fetch:67108864	- Limit fetch responses to this many bytes, even if the total of partition bytes limits is higher.} {Timestamp:2025-03-31 11:49:46.493270918 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_memory_batch_size_estimate_for_fetch:1048576	- The size of the batch used to estimate memory consumption for fetch requests, in bytes. Smaller sizes allow more concurrent fetch requests per shard. Larger sizes prevent running out of memory because of too many concurrent fetch requests.} {Timestamp:2025-03-31 11:49:46.493272761 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_memory_share_for_fetch:0.5	- The share of Kafka subsystem memory that can be used for fetch read buffers, as a fraction of the Kafka subsystem memory amount.} {Timestamp:2025-03-31 11:49:46.493274595 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_mtls_principal_mapping_rules:{nullopt}	- Principal mapping rules for mTLS authentication on the Kafka API. If `null`, the property is disabled.} {Timestamp:2025-03-31 11:49:46.493276899 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_nodelete_topics:{_redpanda.audit_log, __consumer_offsets, _schemas}	- A list of topics that are protected from deletion and configuration changes by Kafka clients. Set by default to a list of Redpanda internal topics.} {Timestamp:2025-03-31 11:49:46.493278883 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_noproduce_topics:{}	- A list of topics that are protected from being produced to by Kafka clients. Set by default to a list of Redpanda internal topics.} {Timestamp:2025-03-31 11:49:46.493280386 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_depth_alpha:0.8	- Smoothing factor for Kafka queue depth control depth tracking.} {Timestamp:2025-03-31 11:49:46.493281808 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_depth_update_ms:7000	- Update frequency for Kafka queue depth control.} {Timestamp:2025-03-31 11:49:46.493283051 +0000 UTC Content:INFO  2025-03-31 11:49:46,
2025-03-31 11:49:50.843802737  493 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_enable:0	- Enable kafka queue depth control.} {Timestamp:2025-03-31 11:49:46.493284513 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_idle_depth:10	- Queue depth when idleness is detected in Kafka queue depth control.} {Timestamp:2025-03-31 11:49:46.493286046 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_latency_alpha:0.002	- Smoothing parameter for Kafka queue depth control latency tracking.} {Timestamp:2025-03-31 11:49:46.493287449 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_max_depth:100	- Maximum queue depth used in kafka queue depth control.} {Timestamp:2025-03-31 11:49:46.493288932 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_max_latency_ms:80	- Maximum latency threshold for Kafka queue depth control depth tracking.} {Timestamp:2025-03-31 11:49:46.493290314 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_min_depth:1	- Minimum queue depth used in Kafka queue depth control.} {Timestamp:2025-03-31 11:49:46.493291777 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_window_count:12	- Number of windows used in kafka queue depth control latency tracking.} {Timestamp:2025-03-31 11:49:46.49329321 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_window_size_ms:1500	- Window size for Kafka queue depth control latency tracking.} {Timestamp:2025-03-31 11:49:46.493300193 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_quota_balancer_min_shard_throughput_bps:256	- The minimum value of the throughput quota a shard can get in the process of quota balancing, expressed in bytes per second. The value applies equally to ingress and egress traffic. `kafka_quota_balancer_min_shard_throughput_bps` doesn't override the limit settings, `kafka_throughput_limit_node_in_bps` and `kafka_throughput_limit_node_out_bps`. Consequently, the value of `kafka_throughput_limit_node_in_bps` or `kafka_throughput_limit_node_out_bps` can result in lesser throughput than kafka_quota_balancer_min_shard_throughput_bps. Both `kafka_quota_balancer_min_shard_throughput_ratio` and `kafka_quota_balancer_min_shard_throughput_bps` can be specified at the same time. In this case, the balancer will not decrease the effective shard quota below the largest bytes-per-second (bps) value of each of these two properties. If set to `0`, no minimum is enforced.} {Timestamp:2025-03-31 11:49:46.493331852 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_quota_balancer_min_shard_throughput_ratio:0.01	- The minimum value of the throughput quota a shard can get in the process of quota balancing, expressed as a ratio of default shard quota. While the value applies equally to ingress and egress traffic, the default shard quota can be different for ingress and egress and therefore result in different minimum throughput bytes-per-second (bps) values. Both `kafka_quota_balancer_min_shard_throughput_ratio` and `kafka_quota_balancer_min_shard_throughput_bps` can be specified at the same time. In this case, the balancer will not decrease the effective shard quota below the largest bps value of each of these two properties. If set to `0.0`, the minimum is disabled. If set to `1.0`, the balancer won't be able to rebalance quota without violating this ratio, preventing the balancer from adjusting shards' quotas.} {Timestamp:2025-03-31 11:49:46.493347041 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_quota_balancer_node_period_ms:0	- Intra-node throughput quota balancer invocation period, in milliseconds. When set to 0, the balancer is disabled and makes all the throughput quotas immutable.} {Timestamp:2025-03-31 11:49:46.493349205 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_quota_balancer_window_ms:5000	- Time window used to average current throughput measurement for quota balancer, in milliseconds.} {Timestamp:2025-03-31 11:49:46.493351189 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_request_max_bytes:104857600	- Maximum size of a single request processed using the Kafka API.} {Timestamp:2025-03-31 11:49:46.493353363 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_rpc_server_stream_recv_buf:{nullopt}	- Maximum size of the user-space receive buffer. If `null`, this limit is not applied.} {Timestamp:2025-03-31 11:49:46.493355427 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_rpc_server_tcp_recv_buf:{nullopt}	- Size of the Kafka server TCP receive buffer. If `null`, the property is disabled.} {Timestamp:2025-03-31 11:49:46.49335754 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_rpc_server_tcp_send_buf:{nullopt}	- Size of the Kafka server TCP transmit buffer. If `null`, the property is disabled.} {Timestamp:2025-03-31 11:49:46.493361308 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_sasl_max_reauth_ms:{nullopt}	- The maximum time between Kafka client reauthentications. If a client has not reauthenticated a connection within this time frame, that connection is torn down. If this property is not set (or set to `null`), session expiry is disabled, and a connection could live long after the client's credentials are expired or revoked.} {Timestamp:2025-03-31 11:49:46.493363231 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_schema_id_validation_cache_capacity:128	- Per-shard capacity of the cache for validating schema IDs.} {Timestamp:2025-03-31 11:49:46.493375364 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_tcp_keepalive_probe_interval_seconds:60000	- TCP keepalive probe interval in seconds for Kafka connections. This describes the timeout between unacknowledged TCP keepalives. Refers to the TCP_KEEPINTVL socket option. When changed, applies to new connections only.} {Timestamp:2025-03-31 11:49:46.493378199 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_tcp_keepalive_probes:3	- TCP keepalive unacknowledged probes until the connection is considered dead for Kafka connections. Refers to the TCP_KEEPCNT socket option. When changed, applies to new connections only.} {Timestamp:2025-03-31 11:49:46.493381576 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_tcp_keepalive_timeout:120000	- TCP keepalive idle timeout in seconds for Kafka connections. This describes the timeout between TCP keepalive probes that the remote site successfully acknowledged. Refers to the TCP_KEEPIDLE socket option. When changed, applies to new connections only.} {Timestamp:2025-03-31 11:49:46.493385553 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_throughput_control:{}	- List of throughput control groups that define exclusions from node-wide throughput limits. Clients excluded from node-wide throughput limits are still potentially subject to client-specific throughput limits. For more information see https://docs.redpanda.com/current/reference/properties/cluster-properties/#kafka_throughput_control.} {Timestamp:2025-03-31 11:49:46.493387877 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_throughput_controlled_api_keys:{produce, fetch}	- List of Kafka API keys
2025-03-31 11:49:50.843844214   that are subject to cluster-wide and node-wide throughput limit control.} {Timestamp:2025-03-31 11:49:46.493391073 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_throughput_limit_node_in_bps:{nullopt}	- The maximum rate of all ingress Kafka API traffic for a node. Includes all Kafka API traffic (requests, responses, headers, fetched data, produced data, etc.). If `null`, the property is disabled, and traffic is not limited.} {Timestamp:2025-03-31 11:49:46.493394229 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_throughput_limit_node_out_bps:{nullopt}	- The maximum rate of all egress Kafka traffic for a node. Includes all Kafka API traffic (requests, responses, headers, fetched data, produced data, etc.). If `null`, the property is disabled, and traffic is not limited.} {Timestamp:2025-03-31 11:49:46.493399529 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_throughput_replenish_threshold:{nullopt}	- Threshold for refilling the token bucket as part of enforcing throughput limits. This only applies when kafka_throughput_throttling_v2 is `true`. This threshold is evaluated with each request for data. When the number of tokens to replenish exceeds this threshold, then tokens are added to the token bucket. This ensures that the atomic is not being updated for the token count with each request. The range for this threshold is automatically clamped to the corresponding throughput limit for ingress and egress.} {Timestamp:2025-03-31 11:49:46.493404639 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_throughput_throttling_v2:1	- Enables an updated algorithm for enforcing node throughput limits based on a shared token bucket, introduced with Redpanda v23.3.8. Set this property to `false` if you need to use the quota balancing algorithm from Redpanda v23.3.7 and older.  This property defaults to `true` for all new or upgraded Redpanda clusters. Disabling this property is not recommended. It causes your Redpanda cluster to use an outdated throughput throttling mechanism. Only set this to `false` when advised to do so by Redpanda support.} {Timestamp:2025-03-31 11:49:46.493406352 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kvstore_flush_interval:10	- Key-value store flush interval (in milliseconds).} {Timestamp:2025-03-31 11:49:46.493408045 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kvstore_max_segment_size:16777216	- Key-value maximum segment size (in bytes).} {Timestamp:2025-03-31 11:49:46.493409758 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.leader_balancer_idle_timeout:120000	- Leadership rebalancing idle timeout.} {Timestamp:2025-03-31 11:49:46.493425518 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.leader_balancer_mode:	- } {Timestamp:2025-03-31 11:49:46.493427281 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.leader_balancer_mute_timeout:300000	- Leadership rebalancing mute timeout.} {Timestamp:2025-03-31 11:49:46.493429135 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.leader_balancer_transfer_limit_per_shard:512	- Per shard limit for in-progress leadership transfers.} {Timestamp:2025-03-31 11:49:46.49343201 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.legacy_group_offset_retention_enabled:0	- Group offset retention is enabled by default starting in Redpanda version 23.1. To enable offset retention after upgrading from an older version, set this option to true.} {Timestamp:2025-03-31 11:49:46.493436228 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.legacy_permit_unsafe_log_operation:1	- Flag to enable a Redpanda cluster operator to use unsafe control characters within strings, such as consumer group names or user names. This flag applies only for Redpanda clusters that were originally on version 23.1 or earlier and have been upgraded to version 23.2 or later. Starting in version 23.2, newly-created Redpanda clusters ignore this property.} {Timestamp:2025-03-31 11:49:46.493439444 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.legacy_unsafe_log_warning_interval_sec:300000	- Period at which to log a warning about using unsafe strings containing control characters. If unsafe strings are permitted by `legacy_permit_unsafe_log_operation`, a warning will be logged at an interval specified by this property.} {Timestamp:2025-03-31 11:49:46.493441839 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.log_cleanup_policy:delete	- Default cleanup policy for topic logs. The topic property `cleanup.policy` overrides the value of `log_cleanup_policy` at the topic level.} {Timestamp:2025-03-31 11:49:46.493443532 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.log_compaction_interval_ms:10000	- How often to trigger background compaction.} {Timestamp:2025-03-31 11:49:46.493445085 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.log_compaction_use_sliding_window:1	- Use sliding window compaction.} {Timestamp:2025-03-31 11:49:46.493447439 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.log_compression_type:producer	- Default topic compression type. The topic property `compression.type` overrides the value of `log_compression_type` at the topic level.} {Timestamp:2025-03-31 11:49:46.493449804 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.log_disable_housekeeping_for_tests:0	- Disables the housekeeping loop for local storage. This property is used to simplify testing, and should not be set in production.} {Timestamp:2025-03-31 11:49:46.493452469 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.log_message_timestamp_alert_after_ms:7200000	- Threshold in milliseconds for alerting on messages with a timestamp after the broker's time, meaning the messages are in the future relative to the broker's clock.} {Timestamp:2025-03-31 11:49:46.493455324 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.log_message_timestamp_alert_before_ms:{nullopt}	- Threshold in milliseconds for alerting on messages with a timestamp before the broker's time, meaning the messages are in the past relative to the broker's clock. To disable this check, set to `null`.} {Timestamp:2025-03-31 11:49:46.493458159 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.log_message_timestamp_type:CreateTime	- Default timestamp type for topic messages (CreateTime or LogAppendTime). The topic property `message.timestamp.type` overrides the value of `log_message_timestamp_type` at the topic level.} {Timestamp:2025-03-31 11:49:46.493460985 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.log_retention_ms:604800000	- The amount of time to keep a log file before deleting it (in milliseconds). If set to `-1`, no time limit is applied. This is a cluster-wide default when a topic does not set or disable `retention.ms`.} {Timestamp:2025-03-31 11:49:46.493464842 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.log_segment_ms:{1209600000}	- Default lifetime of log segments. If `null`, the property is disabled, and no default lifetime is set. Any value under 60 seconds (60000 ms) is rejected. This property can also be set in the Kafka API using the Kafka-compatible alias, `log.roll.ms`. 
2025-03-31 11:49:50.843901221  The topic property `segment.ms` overrides the value of `log_segment_ms` at the topic level.} {Timestamp:2025-03-31 11:49:46.493466826 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.log_segment_ms_max:31536000000	- Upper bound on topic `segment.ms`: higher values will be clamped to this value.} {Timestamp:2025-03-31 11:49:46.493468709 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.log_segment_ms_min:600000	- Lower bound on topic `segment.ms`: lower values will be clamped to this value.} {Timestamp:2025-03-31 11:49:46.493470593 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.log_segment_size:134217728	- Default log segment size in bytes for topics which do not set segment.bytes} {Timestamp:2025-03-31 11:49:46.493472506 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.log_segment_size_jitter_percent:5	- Random variation to the segment size limit used for each partition.} {Timestamp:2025-03-31 11:49:46.49347453 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.log_segment_size_max:{nullopt}	- Upper bound on topic `segment.bytes`: higher values will be clamped to this limit.} {Timestamp:2025-03-31 11:49:46.493476514 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.log_segment_size_min:{1048576}	- Lower bound on topic `segment.bytes`: lower values will be clamped to this limit.} {Timestamp:2025-03-31 11:49:46.493478397 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.lz4_decompress_reusable_buffers_disabled:0	- Disable reusable preallocated buffers for LZ4 decompression.} {Timestamp:2025-03-31 11:49:46.493480211 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.max_compacted_log_segment_size:5368709120	- Maximum compacted segment size after consolidation.} {Timestamp:2025-03-31 11:49:46.493501921 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.max_concurrent_producer_ids:18446744073709551615	- Maximum number of the active producers sessions. When the threshold is passed, Redpanda terminates old sessions. When an idle producer corresponding to the terminated session wakes up and produces, its message batches are rejected, and an out of order sequence error is emitted. Consumers don't affect this setting.} {Timestamp:2025-03-31 11:49:46.493504717 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.max_in_flight_pandaproxy_requests_per_shard:500	- Maximum number of in-flight HTTP requests to HTTP Proxy permitted per shard.  Any additional requests above this limit will be rejected with a 429 error.} {Timestamp:2025-03-31 11:49:46.493507432 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.max_in_flight_schema_registry_requests_per_shard:500	- Maximum number of in-flight HTTP requests to Schema Registry permitted per shard.  Any additional requests above this limit will be rejected with a 429 error.} {Timestamp:2025-03-31 11:49:46.493509225 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.max_kafka_throttle_delay_ms:30000	- Fail-safe maximum throttle delay on Kafka requests.} {Timestamp:2025-03-31 11:49:46.493513493 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.max_transactions_per_coordinator:18446744073709551615	- Specifies the maximum number of active transaction sessions per coordinator. When the threshold is passed Redpanda terminates old sessions. When an idle producer corresponding to the terminated session wakes up and produces, it leads to its batches being rejected with invalid producer epoch or invalid_producer_id_mapping error (depends on the transaction execution phase).} {Timestamp:2025-03-31 11:49:46.493514665 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.max_version:	- } {Timestamp:2025-03-31 11:49:46.493516409 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.members_backend_retry_ms:5000	- Time between members backend reconciliation loop retries.} {Timestamp:2025-03-31 11:49:46.493518833 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.memory_abort_on_alloc_failure:1	- If `true`, the Redpanda process will terminate immediately when an allocation cannot be satisfied due to memory exhaustion. If false, an exception is thrown.} {Timestamp:2025-03-31 11:49:46.493521779 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.memory_enable_memory_sampling:1	- When `true`, memory allocations are sampled and tracked. A sampled live set of allocations can then be retrieved from the Admin API. Additionally, Redpanda will periodically log the top-n allocation sites.} {Timestamp:2025-03-31 11:49:46.493523452 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.metadata_dissemination_interval_ms:3000	- Interval for metadata dissemination batching.} {Timestamp:2025-03-31 11:49:46.493529002 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.metadata_dissemination_retries:30	- Number of attempts to look up a topic's metadata-like shard before a request fails. This configuration controls the number of retries that request handlers perform when internal topic metadata (for topics like tx, consumer offsets, etc) is missing. These topics are usually created on demand when users try to use the cluster for the first time and it may take some time for the creation to happen and the metadata to propagate to all the brokers (particularly the broker handling the request). In the mean time Redpanda waits and retry. This configuration controls the number retries.} {Timestamp:2025-03-31 11:49:46.493530765 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.metadata_dissemination_retry_delay_ms:320	- Delay before retrying a topic lookup in a shard or other meta tables.} {Timestamp:2025-03-31 11:49:46.493532328 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.metadata_status_wait_timeout_ms:2000	- Maximum time to wait in metadata request for cluster health to be refreshed.} {Timestamp:2025-03-31 11:49:46.493533891 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.metrics_reporter_report_interval:86400000	- Cluster metrics reporter report interval.} {Timestamp:2025-03-31 11:49:46.493535645 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.metrics_reporter_tick_interval:60000	- Cluster metrics reporter tick interval.} {Timestamp:2025-03-31 11:49:46.493537388 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.metrics_reporter_url:https://m.rp.vectorized.io/v2	- URL of the cluster metrics reporter.} {Timestamp:2025-03-31 11:49:46.49353861 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.min_version:	- } {Timestamp:2025-03-31 11:49:46.493545062 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.minimum_topic_replications:1	- Minimum allowable replication factor for topics in this cluster. The set value must be positive, odd, and equal to or less than the number of available brokers. Changing this parameter only restricts newly-created topics. Redpanda returns an `INVALID_REPLICATION_FACTOR` error on any attempt to create a topic with a replication factor less than this property. If you change the `minimum_topic_replications` setting, the replication facto
2025-03-31 11:49:50.843942669  r of existing topics remains unchanged. However, Redpanda will log a warning on start-up with a list of any topics that have fewer replicas than this minimum. For example, you might see a message such as `Topic X has a replication factor less than specified minimum: 1 < 3`.} {Timestamp:2025-03-31 11:49:46.493547186 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.node_isolation_heartbeat_timeout:3000	- How long after the last heartbeat request a node will wait before considering itself to be isolated.} {Timestamp:2025-03-31 11:49:46.493549 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.node_management_operation_timeout_ms:5000	- Timeout for executing node management operations.} {Timestamp:2025-03-31 11:49:46.493551234 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.node_status_interval:100	- Time interval between two node status messages. Node status messages establish liveness status outside of the Raft protocol.} {Timestamp:2025-03-31 11:49:46.493553438 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.node_status_reconnect_max_backoff_ms:15000	- Maximum backoff (in milliseconds) to reconnect to an unresponsive peer during node status liveness checks.} {Timestamp:2025-03-31 11:49:46.493555452 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.oidc_clock_skew_tolerance:0	- The amount of time (in seconds) to allow for when validating the expiry claim in the token.} {Timestamp:2025-03-31 11:49:46.493557756 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.oidc_discovery_url:https://auth.prd.cloud.redpanda.com/.well-known/openid-configuration	- The URL pointing to the well-known discovery endpoint for the OIDC provider.} {Timestamp:2025-03-31 11:49:46.49355967 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.oidc_keys_refresh_interval:3600000	- The frequency of refreshing the JSON Web Keys (JWKS) used to validate access tokens.} {Timestamp:2025-03-31 11:49:46.493561503 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.oidc_principal_mapping:$.sub	- Rule for mapping JWT payload claim to a Redpanda user principal.} {Timestamp:2025-03-31 11:49:46.493563317 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.oidc_token_audience:redpanda	- A string representing the intended recipient of the token.} {Timestamp:2025-03-31 11:49:46.49356515 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.partition_autobalancing_concurrent_moves:50	- Number of partitions that can be reassigned at once.} {Timestamp:2025-03-31 11:49:46.493566783 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.partition_autobalancing_max_disk_usage_percent:80	- When the disk usage of a node exceeds this threshold, it triggers Redpanda to move partitions off of the node. This property applies only when partition_autobalancing_mode is set to `continuous`.} {Timestamp:2025-03-31 11:49:46.493568176 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.partition_autobalancing_min_size_threshold:{nullopt}	- Minimum size of partition that is going to be prioritized when rebalancing a cluster due to the disk size threshold being breached. This value is calculated automatically by default.} {Timestamp:2025-03-31 11:49:46.493588013 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.partition_autobalancing_mode:node_add	- Mode of partition balancing for a cluster. * `node_add`: partition balancing happens when a node is added. * `continuous`: partition balancing happens automatically to maintain optimal performance and availability, based on continuous monitoring for node changes (same as `node_add`) and also high disk usage. This option requires an Enterprise license, and it is customized by `partition_autobalancing_node_availability_timeout_sec` and `partition_autobalancing_max_disk_usage_percent` properties. * `off`: partition balancing is disabled. This option is not recommended for production clusters.} {Timestamp:2025-03-31 11:49:46.493590988 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.partition_autobalancing_movement_batch_size_bytes:5368709120	- Total size of partitions that autobalancer is going to move in one batch (deprecated, use partition_autobalancing_concurrent_moves to limit the autobalancer concurrency)} {Timestamp:2025-03-31 11:49:46.493594174 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.partition_autobalancing_node_availability_timeout_sec:900000	- When a node is unavailable for at least this timeout duration, it triggers Redpanda to move partitions off of the node. This property applies only when `partition_autobalancing_mode` is set to `continuous`.      } {Timestamp:2025-03-31 11:49:46.493595958 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.partition_autobalancing_tick_interval_ms:30000	- Partition autobalancer tick interval.} {Timestamp:2025-03-31 11:49:46.493599474 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.partition_autobalancing_tick_moves_drop_threshold:0.2	- If the number of scheduled tick moves drops by this ratio, a new tick is scheduled immediately. Valid values are (0, 1]. For example, with a value of 0.2 and 100 scheduled moves in a tick, a new tick is scheduled when the in-progress moves are fewer than 80.} {Timestamp:2025-03-31 11:49:46.493603131 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.partition_autobalancing_topic_aware:1	- If `true`, Redpanda prioritizes balancing a topicâ€™s partition replica count evenly across all brokers while itâ€™s balancing the clusterâ€™s overall partition count. Because different topics in a cluster can have vastly different load profiles, this better distributes the workload of the most heavily-used topics evenly across brokers.} {Timestamp:2025-03-31 11:49:46.493606227 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.partition_manager_shutdown_watchdog_timeout:30000	- A threshold value to detect partitions which might have been stuck while shutting down. After this threshold, a watchdog in partition manager will log information about partition shutdown not making progress.} {Timestamp:2025-03-31 11:49:46.493609213 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.pp_sr_smp_max_non_local_requests:{nullopt}	- Maximum number of Cross-core(Inter-shard communication) requests pending in HTTP Proxy and Schema Registry seastar::smp group. (For more details, see the `seastar::smp_service_group` documentation).} {Timestamp:2025-03-31 11:49:46.493610866 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.quota_manager_gc_sec:30000	- Quota manager GC frequency in milliseconds.} {Timestamp:2025-03-31 11:49:46.493614292 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_enable_longest_log_detection:1	- Enables an additional step in leader election where a candidate is allowed to wait for all the replies from the broker it requested votes from. This may introduce a small delay when recovering from failure, but it prevents truncation if any of the replicas have more data than the majority.} {Timestamp:2025-03-31 11:49:46.493615935 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_enable_lw_heartbeat:1	- Enables Raft optimization of heartbeats.} {Timest
2025-03-31 11:49:50.843997222  amp:2025-03-31 11:49:46.4936182 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_flush_timer_interval_ms:100	- Interval of checking partition against the `raft_replica_max_pending_flush_bytes`, deprecated started 24.1, use raft_replica_max_flush_delay_ms instead } {Timestamp:2025-03-31 11:49:46.493620574 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_heartbeat_disconnect_failures:3	- The number of failed heartbeats after which an unresponsive TCP connection is forcibly closed. To disable forced disconnection, set to 0.} {Timestamp:2025-03-31 11:49:46.493622297 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_heartbeat_interval_ms:150	- Number of milliseconds for Raft leader heartbeats.} {Timestamp:2025-03-31 11:49:46.493626645 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_heartbeat_timeout_ms:3000	- Raft heartbeat RPC (remote procedure call) timeout. Raft uses a heartbeat mechanism to maintain leadership authority and to trigger leader elections. The `raft_heartbeat_interval_ms` is a periodic heartbeat sent by the partition leader to all followers to declare its leadership. If a follower does not receive a heartbeat within the `raft_heartbeat_timeout_ms`, then it triggers an election to choose a new partition leader.} {Timestamp:2025-03-31 11:49:46.493628128 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_io_timeout_ms:10000	- Raft I/O timeout.} {Timestamp:2025-03-31 11:49:46.493633408 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_learner_recovery_rate:104857600	- Raft learner recovery rate limit. Throttles the rate of data communicated to nodes (learners) that need to catch up to leaders. This rate limit is placed on a node sending data to a recovering node. Each sending node is limited to this rate. The recovering node accepts data as fast as possible according to the combined limits of all healthy nodes in the cluster. For example, if two nodes are sending data to the recovering node, and `raft_learner_recovery_rate` is 100 MB/sec, then the recovering node will recover at a rate of 200 MB/sec.} {Timestamp:2025-03-31 11:49:46.493635622 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_max_concurrent_append_requests_per_follower:16	- Maximum number of concurrent append entry requests sent by the leader to one follower.} {Timestamp:2025-03-31 11:49:46.493637786 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_max_recovery_memory:{nullopt}	- Maximum memory that can be used for reads in Raft recovery process by default 15% of total memory.} {Timestamp:2025-03-31 11:49:46.493640451 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_recovery_concurrency_per_shard:64	- Number of partitions that may simultaneously recover data to a particular shard. This number is limited to avoid overwhelming nodes when they come back online after an outage.} {Timestamp:2025-03-31 11:49:46.493642455 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_recovery_default_read_size:524288	- Specifies the default size of a read issued during Raft follower recovery.} {Timestamp:2025-03-31 11:49:46.493644779 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_recovery_throttle_disable_dynamic_mode:0	- Disables cross shard sharing used to throttle recovery traffic. Should only be used to debug unexpected problems.} {Timestamp:2025-03-31 11:49:46.493646653 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_replica_max_flush_delay_ms:100	- Maximum delay between two subsequent flushes. After this delay, the log is automatically force flushed.} {Timestamp:2025-03-31 11:49:46.493648025 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_replica_max_pending_flush_bytes:{262144}	- Maximum number of bytes that are not flushed per partition. If the configured threshold is reached, the log is automatically flushed even if it has not been explicitly requested.} {Timestamp:2025-03-31 11:49:46.493648927 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_replicate_batch_window_size:1048576	- Maximum size of requests cached for replication.} {Timestamp:2025-03-31 11:49:46.49365022 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_smp_max_non_local_requests:{nullopt}	- Maximum number of Cross-core(Inter-shard communication) requests pending in Raft seastar::smp group. For details, refer to the `seastar::smp_service_group` documentation).} {Timestamp:2025-03-31 11:49:46.493669065 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_timeout_now_timeout_ms:1000	- Timeout for Raft's timeout_now RPC. This RPC is used to force a follower to dispatch a round of votes immediately.} {Timestamp:2025-03-31 11:49:46.493671169 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.raft_transfer_leader_recovery_timeout_ms:10000	- Follower recovery timeout waiting period when transferring leadership.} {Timestamp:2025-03-31 11:49:46.493673113 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.readers_cache_eviction_timeout_ms:30000	- Duration after which inactive readers are evicted from cache.} {Timestamp:2025-03-31 11:49:46.493675908 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.readers_cache_target_max_size:200	- Maximum desired number of readers cached per NTP. This a soft limit, meaning that a number of readers in cache may temporarily increase as cleanup is performed in the background.} {Timestamp:2025-03-31 11:49:46.493677952 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.reclaim_batch_cache_min_free:67108864	- Minimum amount of free memory maintained by the batch cache background reclaimer.} {Timestamp:2025-03-31 11:49:46.493680917 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.reclaim_growth_window:3000	- Starting from the last point in time when memory was reclaimed from the batch cache, this is the duration during which the amount of memory to reclaim grows at a significant rate, based on heuristics about the amount of available memory.} {Timestamp:2025-03-31 11:49:46.49368253 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.reclaim_max_size:4194304	- Maximum batch cache reclaim size.} {Timestamp:2025-03-31 11:49:46.493684083 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.reclaim_min_size:131072	- Minimum batch cache reclaim size.} {Timestamp:2025-03-31 11:49:46.493687269 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.reclaim_stable_window:10000	- If the duration since the last time memory was reclaimed is longer than the amount of time specified in this property, the memory usage of the batch cache is considered stable, so only the minimum size (`reclaim_min_size`) is set to be reclaimed.} {Timestamp:2025-03-31 11:49:46.493689253 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.recovery_append_timeout_ms:5000	- Timeout for append entry requests issued while updating a stale follower.} {Timestamp:2025-03-31 11:49:46.493691237 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.release_cache_on_s
2025-03-31 11:49:50.844056272  egment_roll:0	- Flag for specifying whether or not to release cache when a full segment is rolled.} {Timestamp:2025-03-31 11:49:46.49369315 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.replicate_append_timeout_ms:3000	- Timeout for append entry requests issued while replicating entries.} {Timestamp:2025-03-31 11:49:46.493696376 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.retention_bytes:{nullopt}	- Default maximum number of bytes per partition on disk before triggering deletion of the oldest messages. If `null` (the default value), no limit is applied. The topic property `retention.bytes` overrides the value of `retention_bytes` at the topic level.} {Timestamp:2025-03-31 11:49:46.493699652 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.retention_local_strict:0	- Flag to allow Tiered Storage topics to expand to consumable retention policy limits. When this flag is enabled, non-local retention settings are used, and local retention settings are used to inform data removal policies in low-disk space scenarios.} {Timestamp:2025-03-31 11:49:46.493702918 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.retention_local_strict_override:1	- Trim log data when a cloud topic reaches its local retention limit. When this option is disabled Redpanda will allow partitions to grow past the local retention limit, and will be trimmed automatically as storage reaches the configured target size.} {Timestamp:2025-03-31 11:49:46.493707377 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.retention_local_target_bytes_default:{nullopt}	- Local retention size target for partitions of topics with object storage write enabled. If `null`, the property is disabled. This property can be overridden on a per-topic basis by setting `retention.local.target.bytes` in each topic enabled for Tiered Storage. Both `retention_local_target_bytes_default` and `retention_local_target_ms_default` can be set. The limit that is reached earlier is applied.} {Timestamp:2025-03-31 11:49:46.493711895 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.retention_local_target_capacity_bytes:{nullopt}	- The target capacity (in bytes) that log storage will try to use before additional retention rules take over to trim data to meet the target. When no target is specified, storage usage is unbounded. Redpanda Data recommends setting only one of `retention_local_target_capacity_bytes` or `retention_local_target_capacity_percent`. If both are set, the minimum of the two is used as the effective target capacity.} {Timestamp:2025-03-31 11:49:46.493716424 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.retention_local_target_capacity_percent:{80}	- The target capacity in percent of unreserved space (`disk_reservation_percent`) that log storage will try to use before additional retention rules will take over to trim data in order to meet the target. When no target is specified storage usage is unbounded. Redpanda Data recommends setting only one of `retention_local_target_capacity_bytes` or `retention_local_target_capacity_percent`. If both are set, the minimum of the two is used as the effective target capacity.} {Timestamp:2025-03-31 11:49:46.493720532 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.retention_local_target_ms_default:86400000	- Local retention time target for partitions of topics with object storage write enabled. This property can be overridden on a per-topic basis by setting `retention.local.target.ms` in each topic enabled for Tiered Storage. Both `retention_local_target_bytes_default` and `retention_local_target_ms_default` can be set. The limit that is reached first is applied.} {Timestamp:2025-03-31 11:49:46.493722625 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.retention_local_trim_interval:30000	- The period during which disk usage is checked for disk pressure, and data is optionally trimmed to meet the target.} {Timestamp:2025-03-31 11:49:46.49372516 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.retention_local_trim_overage_coeff:2	- The space management control loop reclaims the overage multiplied by this this coefficient to compensate for data that is written during the idle period between control loop invocations.} {Timestamp:2025-03-31 11:49:46.493728006 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.rm_sync_timeout_ms:10000	- Resource manager's synchronization timeout. Specifies the maximum time for this node to wait for the internal state machine to catch up with all events written by previous leaders before rejecting a request.} {Timestamp:2025-03-31 11:49:46.493729418 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.rm_violation_recovery_policy:	- } {Timestamp:2025-03-31 11:49:46.493731312 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.rpc_client_connections_per_peer:128	- The maximum number of connections a broker will open to each of its peers.} {Timestamp:2025-03-31 11:49:46.493733255 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.rpc_server_compress_replies:0	- Enable compression for internal RPC (remote procedure call) server replies.} {Timestamp:2025-03-31 11:49:46.49373566 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.rpc_server_listen_backlog:{nullopt}	- Maximum TCP connection queue length for Kafka server and internal RPC server. If `null` (the default value), no queue length is set.} {Timestamp:2025-03-31 11:49:46.493737804 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.rpc_server_tcp_recv_buf:{nullopt}	- Internal RPC TCP receive buffer size. If `null` (the default value), no buffer size is set by Redpanda.} {Timestamp:2025-03-31 11:49:46.49375708 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.rpc_server_tcp_send_buf:{nullopt}	- Internal RPC TCP send buffer size. If `null` (the default value), then no buffer size is set by Redpanda.} {Timestamp:2025-03-31 11:49:46.493758583 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.rpk_path:"/usr/bin/rpk"	- Path to RPK binary} {Timestamp:2025-03-31 11:49:46.493760466 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.rps_limit_acls_and_users_operations:1000	- Rate limit for controller ACLs and user's operations.} {Timestamp:2025-03-31 11:49:46.49376229 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.rps_limit_configuration_operations:1000	- Rate limit for controller configuration operations.} {Timestamp:2025-03-31 11:49:46.493773561 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.rps_limit_move_operations:1000	- Rate limit for controller move operations.} {Timestamp:2025-03-31 11:49:46.493775224 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.rps_limit_node_management_operations:1000	- Rate limit for controller node management operations.} {Timestamp:2025-03-31 11:49:46.493776627 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.rps_limit_topic_operations:1000	- Rate limit for controller topic operations.} {Timestamp:2025-03-31 11:49:46.4937784 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.sasl_kerberos_config:/etc/krb5.conf	- The location of the Kerberos `k
2025-03-31 11:49:50.844099433  rb5.conf` file for Redpanda.} {Timestamp:2025-03-31 11:49:46.493780314 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.sasl_kerberos_keytab:/var/lib/redpanda/redpanda.keytab	- The location of the Kerberos keytab file for Redpanda.} {Timestamp:2025-03-31 11:49:46.493782267 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.sasl_kerberos_principal:redpanda	- The primary of the Kerberos Service Principal Name (SPN) for Redpanda.} {Timestamp:2025-03-31 11:49:46.493784251 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.sasl_kerberos_principal_mapping:{DEFAULT}	- Rules for mapping Kerberos principal names to Redpanda user principals.} {Timestamp:2025-03-31 11:49:46.493786245 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.sasl_mechanisms:{SCRAM}	- A list of supported SASL mechanisms. Accepted values: `SCRAM`, `GSSAPI`, `OAUTHBEARER`.} {Timestamp:2025-03-31 11:49:46.493788138 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.schema_registry_normalize_on_startup:0	- Normalize schemas as they are read from the topic on startup.} {Timestamp:2025-03-31 11:49:46.493789992 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.schema_registry_protobuf_renderer_v2:0	- Enables experimental protobuf renderer to support normalize=true.} {Timestamp:2025-03-31 11:49:46.493791374 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.seed_server_meta_topic_partitions:	- } {Timestamp:2025-03-31 11:49:46.493793138 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.segment_appender_flush_timeout_ms:1000	- Maximum delay until buffered data is written.} {Timestamp:2025-03-31 11:49:46.493794761 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.segment_fallocation_step:33554432	- Size for segments fallocation.} {Timestamp:2025-03-31 11:49:46.493796043 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.seq_table_min_size:	- } {Timestamp:2025-03-31 11:49:46.493798558 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.space_management_enable:1	- Option to explicitly disable automatic disk space management. If this property was explicitly disabled while using v23.2, it will remain disabled following an upgrade.} {Timestamp:2025-03-31 11:49:46.493800732 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.space_management_enable_override:0	- Enable automatic space management. This option is ignored and deprecated in versions >= v23.3.} {Timestamp:2025-03-31 11:49:46.493802686 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.space_management_max_log_concurrency:20	- Maximum parallel logs inspected during space management process.} {Timestamp:2025-03-31 11:49:46.493804729 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.space_management_max_segment_concurrency:10	- Maximum parallel segments inspected during space management process.} {Timestamp:2025-03-31 11:49:46.493806833 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.storage_compaction_index_memory:134217728	- Maximum number of bytes that may be used on each shard by compaction index writers.} {Timestamp:2025-03-31 11:49:46.493809198 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.storage_compaction_key_map_memory:134217728	- Maximum number of bytes that may be used on each shard by compaction key-offset maps. Only applies when `log_compaction_use_sliding_window` is set to `true`.} {Timestamp:2025-03-31 11:49:46.493812945 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.storage_compaction_key_map_memory_limit_percent:12	- Limit on `storage_compaction_key_map_memory`, expressed as a percentage of memory per shard, that bounds the amount of memory used by compaction key-offset maps. Memory per shard is computed after `data_transforms_per_core_memory_reservation`, and only applies when `log_compaction_use_sliding_window` is set to `true`.} {Timestamp:2025-03-31 11:49:46.493815049 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.storage_ignore_cstore_hints:0	- When set, cstore hints are ignored and not used for data access (but are otherwise generated).} {Timestamp:2025-03-31 11:49:46.493828103 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.storage_ignore_timestamps_in_future_sec:{nullopt}	- The maximum number of seconds that a record's timestamp can be ahead of a Redpanda broker's clock and still be used when deciding whether to clean up the record for data retention. This property makes possible the timely cleanup of records from clients with clocks that are drastically unsynchronized relative to Redpanda. When determining whether to clean up a record with timestamp more than `storage_ignore_timestamps_in_future_sec` seconds ahead of the broker, Redpanda ignores the record's timestamp and instead uses a valid timestamp of another record in the same segment, or (if another record's valid timestamp is unavailable) the timestamp of when the segment file was last modified (mtime). By default, `storage_ignore_timestamps_in_future_sec` is disabled (null). To figure out whether to set `storage_ignore_timestamps_in_future_sec` for your system: . Look for logs with segments that are unexpectedly large and not being cleaned up. . In the logs, search for records with unsynchronized timestamps that are further into the future than tolerable by your data retention and storage settings. For example, timestamps 60 seconds or more into the future can be considered to be too unsynchronized. . If you find unsynchronized timestamps throughout your logs, determine the number of seconds that the timestamps are ahead of their actual time, and set `storage_ignore_timestamps_in_future_sec` to that value so data retention can proceed. . If you only find unsynchronized timestamps that are the result of transient behavior, you can disable `storage_ignore_timestamps_in_future_sec`.} {Timestamp:2025-03-31 11:49:46.493830388 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.storage_max_concurrent_replay:1024	- Maximum number of partitions' logs that will be replayed concurrently at startup, or flushed concurrently on shutdown.} {Timestamp:2025-03-31 11:49:46.493832311 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.storage_min_free_bytes:5368709120	- Threshold of minimum bytes free space before rejecting producers.} {Timestamp:2025-03-31 11:49:46.493834195 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.storage_read_buffer_size:131072	- Size of each read buffer (one per in-flight read, per log segment).} {Timestamp:2025-03-31 11:49:46.493836098 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.storage_read_readahead_count:10	- How many additional reads to issue ahead of current read location.} {Timestamp:2025-03-31 11:49:46.493859492 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.storage_reserve_min_segments:2	- The number of segments per partition that the system will attempt to reserve disk capacity for. For example, if the maximum segment size is configured to be 100 MB, and the value of this option is 2, then in a system with 10 partitions Redpanda will attempt to reserve at least 2 GB of disk space.} {Timestamp:2025-03-31 11:49:46.4938
2025-03-31 11:49:50.844140981  61606 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.storage_space_alert_free_threshold_bytes:0	- Threshold of minimum bytes free space before setting storage space alert.} {Timestamp:2025-03-31 11:49:46.49386359 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.storage_space_alert_free_threshold_percent:5	- Threshold of minimum percent free space before setting storage space alert.} {Timestamp:2025-03-31 11:49:46.493865904 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.storage_strict_data_init:0	- Requires that an empty file named `.redpanda_data_dir` be present in the broker configuration `data_directory`. If set to `true`, Redpanda will refuse to start if the file is not found in the data directory.} {Timestamp:2025-03-31 11:49:46.493867718 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.storage_target_replay_bytes:10737418240	- Target bytes to replay from disk on startup after clean shutdown: controls frequency of snapshots and checkpoints.} {Timestamp:2025-03-31 11:49:46.49386887 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.superusers:{}	- List of superuser usernames.} {Timestamp:2025-03-31 11:49:46.493870513 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.target_fetch_quota_byte_rate:{nullopt}	- Target fetch size quota byte rate (bytes per second) - disabled default} {Timestamp:2025-03-31 11:49:46.493871865 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.target_quota_byte_rate:0	- Target request size quota byte rate (bytes per second)} {Timestamp:2025-03-31 11:49:46.493874631 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.tls_enable_renegotiation:0	- TLS client-initiated renegotiation is considered unsafe and is by default disabled.  Only re-enable it if you are experiencing issues with your TLS-enabled client.  This option has no effect on TLSv1.3 connections as client-initiated renegotiation was removed.} {Timestamp:2025-03-31 11:49:46.493877446 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.tls_min_version:v1.2	- The minimum TLS version that Redpanda clusters support. This property prevents client applications from negotiating a downgrade to the TLS version when they make a connection to a Redpanda cluster.} {Timestamp:2025-03-31 11:49:46.49387955 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.tm_sync_timeout_ms:10000	- Transaction manager's synchronization timeout. Maximum time to wait for internal state machine to catch up before rejecting a request.} {Timestamp:2025-03-31 11:49:46.493880772 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.tm_violation_recovery_policy:	- } {Timestamp:2025-03-31 11:49:46.493883507 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.tombstone_retention_ms:{nullopt}	- The retention time for tombstone records in a compacted topic. Cannot be enabled at the same time as any of `cloud_storage_enabled`, `cloud_storage_enable_remote_read`, or `cloud_storage_enable_remote_write`.} {Timestamp:2025-03-31 11:49:46.49388504 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.topic_fds_per_partition:{5}	- Required file handles per partition when creating topics.} {Timestamp:2025-03-31 11:49:46.493886573 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.topic_memory_per_partition:{4194304}	- Required memory per partition when creating topics.} {Timestamp:2025-03-31 11:49:46.493888326 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.topic_partitions_per_shard:1000	- Maximum number of partitions which may be allocated to one shard (CPU core).} {Timestamp:2025-03-31 11:49:46.49389053 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.topic_partitions_reserve_shard0:0	- Reserved partition slots on shard (CPU core) 0 on each node.  If this is >= topic_partitions_per_shard, no data partitions will be scheduled on shard 0} {Timestamp:2025-03-31 11:49:46.493892664 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.transaction_coordinator_cleanup_policy:delete	- Cleanup policy for a transaction coordinator topic. Accepted Values: `compact`, `delete`, `["compact","delete"]`, `none`} {Timestamp:2025-03-31 11:49:46.493895219 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.transaction_coordinator_delete_retention_ms:604800000	- Delete segments older than this age. To ensure transaction state is retained as long as the longest-running transaction, make sure this is no less than `transactional_id_expiration_ms`.} {Timestamp:2025-03-31 11:49:46.493896902 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.transaction_coordinator_log_segment_size:1073741824	- The size (in bytes) each log segment should be.} {Timestamp:2025-03-31 11:49:46.493898525 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.transaction_coordinator_partitions:50	- Number of partitions for transactions coordinator.} {Timestamp:2025-03-31 11:49:46.493899788 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.transaction_coordinator_replication:	- } {Timestamp:2025-03-31 11:49:46.493902723 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.transaction_max_timeout_ms:900000	- The maximum allowed timeout for transactions. If a client-requested transaction timeout exceeds this configuration, the broker returns an error during transactional producer initialization. This guardrail prevents hanging transactions from blocking consumer progress.} {Timestamp:2025-03-31 11:49:46.493904717 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.transactional_id_expiration_ms:604800000	- Expiration time of producer IDs. Measured starting from the time of the last write until now for a given ID.} {Timestamp:2025-03-31 11:49:46.4939065 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.tx_log_stats_interval_s:10000	- How often to log per partition tx stats, works only with debug logging enabled.} {Timestamp:2025-03-31 11:49:46.493907673 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.tx_registry_log_capacity:	- } {Timestamp:2025-03-31 11:49:46.493908835 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.tx_registry_sync_timeout_ms:	- } {Timestamp:2025-03-31 11:49:46.493910388 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.tx_timeout_delay_ms:1000	- Delay before scheduling the next check for timed out transactions.} {Timestamp:2025-03-31 11:49:46.493912722 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.unsafe_enable_consumer_offsets_delete_retention:0	- Enables delete retention of consumer offsets topic. This is an internal-only configuration and should be enabled only after consulting with Redpanda support.} {Timestamp:2025-03-31 11:49:46.493914405 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.usage_disk_persistance_interval_sec:300000	- The interval in which all usage stats are written to disk.} {Timestamp:2025-03-31 11:49:46.493915918 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:mai
2025-03-31 11:49:50.844198169  n] main - application.cc:849 - redpanda.usage_num_windows:24	- The number of windows to persist in memory and disk.} {Timestamp:2025-03-31 11:49:46.493917631 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.usage_window_width_interval_sec:3600000	- The width of a usage window, tracking cloud and kafka ingress/egress traffic each interval.} {Timestamp:2025-03-31 11:49:46.493919204 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.use_fetch_scheduler_group:1	- Use a separate scheduler group for fetch processing.} {Timestamp:2025-03-31 11:49:46.493920336 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.use_scheduling_groups:	- } {Timestamp:2025-03-31 11:49:46.493921969 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.virtual_cluster_min_producer_ids:18446744073709551615	- Minimum number of active producers per virtual cluster.} {Timestamp:2025-03-31 11:49:46.493923522 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.wait_for_leader_timeout_ms:5000	- Timeout to wait for leadership in metadata cache.} {Timestamp:2025-03-31 11:49:46.493950783 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.write_caching_default:false	- The default write caching mode to apply to user topics. Write caching acknowledges a message as soon as it is received and acknowledged on a majority of brokers, without waiting for it to be written to disk. With `acks=all`, this provides lower latency while still ensuring that a majority of brokers acknowledge the write. Fsyncs follow `raft_replica_max_pending_flush_bytes` and `raft_replica_max_flush_delay_ms`, whichever is reached first. The `write_caching_default` cluster property can be overridden with the `write.caching` topic property. Accepted values: * `true` * `false` * `disabled`: This takes precedence over topic overrides and disables write caching for the entire cluster.} {Timestamp:2025-03-31 11:49:46.493952667 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.zstd_decompress_workspace_bytes:8388608	- Size of the zstd decompression workspace.} {Timestamp:2025-03-31 11:49:46.493953929 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:900 - Node configuration properties:} {Timestamp:2025-03-31 11:49:46.493955392 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:901 - (use `rpk redpanda config set <cfg> <value>` to change)} {Timestamp:2025-03-31 11:49:46.493957115 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.admin:{{:{host: 0.0.0.0, port: 9644}}}	- Network address for the Admin API[] server.} {Timestamp:2025-03-31 11:49:46.493967625 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.admin_api_doc_dir:/usr/share/redpanda/admin-api-doc	- Path to the API specifications for the Admin API.} {Timestamp:2025-03-31 11:49:46.493969368 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.admin_api_tls:{}	- Specifies the TLS configuration for the HTTP Admin API.} {Timestamp:2025-03-31 11:49:46.493971352 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.advertised_kafka_api:{{:{host: 127.0.0.1, port: 9092}}}	- Address of Kafka API published to the clients} {Timestamp:2025-03-31 11:49:46.493973376 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.advertised_rpc_api:{{host: 127.0.0.1, port: 33145}}	- Address of RPC endpoint published to other cluster members} {Timestamp:2025-03-31 11:49:46.49397549 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_directory:{nullopt}	- Directory for archival cache. Should be present when `cloud_storage_enabled` is present} {Timestamp:2025-03-31 11:49:46.493977694 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_hash_path_directory:{nullopt}	- Directory to store inventory report hashes for use by cloud storage scrubber} {Timestamp:2025-03-31 11:49:46.493979107 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.coproc_supervisor_server:	- } {Timestamp:2025-03-31 11:49:46.493982733 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.crash_loop_limit:{5}	- A limit on the number of consecutive times a broker can crash within one hour before its crash-tracking logic is reset. This limit prevents a broker from getting stuck in an infinite cycle of crashes. For more information see https://docs.redpanda.com/current/reference/properties/broker-properties/#crash_loop_limit.} {Timestamp:2025-03-31 11:49:46.49398597 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.crash_loop_sleep_sec:{nullopt}	- The amount of time the broker sleeps before terminating the process when it reaches the number of consecutive times a broker can crash. For more information, see https://docs.redpanda.com/current/reference/properties/broker-properties/#crash_loop_limit.} {Timestamp:2025-03-31 11:49:46.493987262 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.dashboard_dir:	- } {Timestamp:2025-03-31 11:49:46.493989236 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.data_directory:{data_directory="/data/redpanda"}	- Path to the directory for storing Redpanda's streaming data files.} {Timestamp:2025-03-31 11:49:46.49400238 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.developer_mode:1	- Skips most of the checks performed at startup, not recomended for production use} {Timestamp:2025-03-31 11:49:46.494004865 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.emergency_disable_data_transforms:0	- Override the cluster property `data_transforms_enabled` and disable Wasm-powered data transforms. This is an emergency shutoff button.} {Timestamp:2025-03-31 11:49:46.494008822 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.empty_seed_starts_cluster:1	- Controls how a new cluster is formed. All brokers in a cluster must have the same value. See how the `empty_seed_starts_cluster` setting works with the `seed_servers` setting to form a cluster. For backward compatibility, `true` is the default. Redpanda recommends using `false` in production environments to prevent accidental cluster formation.} {Timestamp:2025-03-31 11:49:46.494010115 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.enable_central_config:	- } {Timestamp:2025-03-31 11:49:46.494015745 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.fips_mode:disabled	- Controls whether Redpanda starts in FIPS mode.  This property allows for three values: `disabled` - Redpanda does not start in FIPS mode. `permissive` - Redpanda performs the same check as enabled, but a warning is logged, and Redpanda continues to run. Redpanda loads the OpenSSL FIPS provider into the OpenSSL library. After this completes, Redpanda is operating in FIPS mode, which means that the TLS cipher suites available to users are limited to the TLSv1.2 and TLSv1.3 NIST-approved cryptographic methods. `enabled` - Redpanda verifies that the operating system is enabled for FIPS by checking `/proc/sys/crypto/fips_enabled`. If the file does not exist or does not return `1`, Redpanda immediately exits.} {Timestamp:2025-03-31 11:49:46.494017449 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:
2025-03-31 11:49:50.844239105  main] main - application.cc:849 - redpanda.kafka_api:{{:{host: 0.0.0.0, port: 9092}:{nullopt}}}	- IP address and port of the Kafka API endpoint that handles requests.} {Timestamp:2025-03-31 11:49:46.494018981 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.kafka_api_tls:{}	- Transport Layer Security (TLS) configuration for the Kafka API endpoint.} {Timestamp:2025-03-31 11:49:46.494020765 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.memory_allocation_warning_threshold:{131073}	- Threshold for log messages that contain a larger memory allocation than specified.} {Timestamp:2025-03-31 11:49:46.49402331 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.node_id:{nullopt}	- A number that uniquely identifies the broker within the cluster. If `null` (the default value), Redpanda automatically assigns an ID. If set, it must be non-negative value. The `node_id` property must not be changed after a broker joins the cluster.} {Timestamp:2025-03-31 11:49:46.494025734 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.node_id_overrides:{}	- List of node ID and UUID overrides to be applied at broker startup. Each entry includes the current UUID and desired ID and UUID. Each entry applies to a given node if and only if 'current' matches that node's current UUID.} {Timestamp:2025-03-31 11:49:46.494027457 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.openssl_config_file:{nullopt}	- Path to the configuration file used by OpenSSL to properly load the FIPS-compliant module.} {Timestamp:2025-03-31 11:49:46.494029361 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.openssl_module_directory:{nullopt}	- Path to the directory that contains the OpenSSL FIPS-compliant module. The filename that Redpanda looks for is `fips.so`.} {Timestamp:2025-03-31 11:49:46.494032246 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.rack:{nullopt}	- A label that identifies a failure zone. Apply the same label to all brokers in the same failure zone. When `enable_rack_awareness` is set to `true` at the cluster level, the system uses the rack labels to spread partition replicas across different failure zones.} {Timestamp:2025-03-31 11:49:46.4940343 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.recovery_mode_enabled:0	- If `true`, start Redpanda in recovery mode, where user partitions are not loaded and only administrative operations are allowed.} {Timestamp:2025-03-31 11:49:46.494035973 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.rpc_server:{host: 0.0.0.0, port: 33145}	- IP address and port for the Remote Procedure Call (RPC) server.} {Timestamp:2025-03-31 11:49:46.494038117 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.rpc_server_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} crl file: {nullopt} client_auth_required: 0 }	- TLS configuration for the RPC server.} {Timestamp:2025-03-31 11:49:46.494062082 +0000 UTC Content:INFO  2025-03-31 11:49:46,493 [shard  0:main] main - application.cc:849 - redpanda.seed_servers:{}	- List of the seed servers used to join current cluster. If the `seed_servers` list is empty the node will be a cluster root and it will form a new cluster. When `empty_seed_starts_cluster` is `true`, Redpanda enables one broker with an empty `seed_servers` list to initiate a new cluster. The broker with an empty `seed_servers` becomes the cluster root, to which other brokers must connect to join the cluster.  Brokers looking to join the cluster should have their `seed_servers` populated with the cluster root's address, facilitating their connection to the cluster. Only one broker, the designated cluster root, should have an empty `seed_servers` list during the initial cluster bootstrapping. This ensures a single initiation point for cluster formation. When `empty_seed_starts_cluster` is `false`, Redpanda requires all brokers to start with a known set of brokers listed in `seed_servers`. The `seed_servers` list must not be empty and should be identical across these initial seed brokers, containing the addresses of all seed brokers. Brokers not included in the `seed_servers` list use it to discover and join the cluster, allowing for expansion beyond the foundational members. The `seed_servers` list must be consistent across all seed brokers to prevent cluster fragmentation and ensure stable cluster formation.} {Timestamp:2025-03-31 11:49:46.494063986 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - redpanda.storage_failure_injection_config_path:{nullopt}	- Path to the configuration file used for low level storage failure injection.} {Timestamp:2025-03-31 11:49:46.49406604 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - redpanda.storage_failure_injection_enabled:0	- If `true`, inject low level storage failures on the write path. Do _not_ use for production instances.} {Timestamp:2025-03-31 11:49:46.494068013 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - redpanda.upgrade_override_checks:0	- Whether to violate safety checks when starting a Redpanda version newer than the cluster's consensus version.} {Timestamp:2025-03-31 11:49:46.494070929 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - redpanda.verbose_logging_timeout_sec_max:{nullopt}	- Maximum duration in seconds for verbose (`TRACE` or `DEBUG`) logging. Values configured above this will be clamped. If null (the default) there is no limit. Can be overridden in the Admin API on a per-request basis.} {Timestamp:2025-03-31 11:49:46.494072853 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy.advertised_pandaproxy_api:{}	- Network address for the HTTP Proxy API server to publish to clients.} {Timestamp:2025-03-31 11:49:46.494074786 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy.api_doc_dir:/usr/share/redpanda/proxy-api-doc	- Path to the API specifications for the HTTP Proxy API.} {Timestamp:2025-03-31 11:49:46.494078343 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy.client_cache_max_size:10	- The maximum number of Kafka client connections that Redpanda can cache in the LRU (least recently used) cache. The LRU cache helps optimize resource utilization by keeping the most recently used clients in memory, facilitating quicker reconnections for frequent clients while limiting memory usage.} {Timestamp:2025-03-31 11:49:46.494080447 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy.client_keep_alive:300000	- Time, in milliseconds, that an idle client connection may remain open to the HTTP Proxy API.} {Timestamp:2025-03-31 11:49:46.494082901 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy.consumer_instance_timeout_ms:300000	- How long to wait for an idle consumer before removing it. A consumer is considered idle when it's not making requests or heartbeats.} {Timestamp:2025-03-31 11:49:46.494084805 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy.pandaproxy_api:{{:{host: 0.0.0.0, port: 8082}:<nullopt>}}	- Rest API listen address and port} {Timestamp:2025-03-31 11:49:46.494099402 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy.pandaproxy_api_tls:{}	- TLS configuration for Pandaproxy api.} {Timestamp:2025-03-31 11:49:46.494102238 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.broker_tl
2025-03-31 11:49:50.844279932  s:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} crl file: {nullopt} client_auth_required: 0 }	- TLS configuration for the Kafka API servers to which the HTTP Proxy client should connect.} {Timestamp:2025-03-31 11:49:46.494104482 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.brokers:{{host: 0.0.0.0, port: 9092}}	- Network addresses of the Kafka API servers to which the HTTP Proxy client should connect.} {Timestamp:2025-03-31 11:49:46.494107087 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.client_identifier:{pandaproxy_client}	- Custom identifier to include in the Kafka request header for the HTTP Proxy client. This identifier can help debug or monitor client activities.} {Timestamp:2025-03-31 11:49:46.49410872 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.consumer_heartbeat_interval_ms:500	- Interval (in milliseconds) for consumer heartbeats.} {Timestamp:2025-03-31 11:49:46.494110243 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.consumer_rebalance_timeout_ms:2000	- Timeout (in milliseconds) for consumer rebalance.} {Timestamp:2025-03-31 11:49:46.494111735 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.consumer_request_max_bytes:1048576	- Maximum bytes to fetch per request.} {Timestamp:2025-03-31 11:49:46.494113188 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.consumer_request_min_bytes:1	- Minimum bytes to fetch per request.} {Timestamp:2025-03-31 11:49:46.494114861 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.consumer_request_timeout_ms:100	- Interval (in milliseconds) for consumer request timeout.} {Timestamp:2025-03-31 11:49:46.494116394 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.consumer_session_timeout_ms:300000	- Timeout (in milliseconds) for consumer session.} {Timestamp:2025-03-31 11:49:46.494118288 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.produce_ack_level:-1	- Number of acknowledgments the producer requires the leader to have received before considering a request complete.} {Timestamp:2025-03-31 11:49:46.494119891 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.produce_batch_delay_ms:100	- Delay (in milliseconds) to wait before sending batch.} {Timestamp:2025-03-31 11:49:46.494121754 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.produce_batch_record_count:1000	- Number of records to batch before sending to broker.} {Timestamp:2025-03-31 11:49:46.494123528 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.produce_batch_size_bytes:1048576	- Number of bytes to batch before sending to broker.} {Timestamp:2025-03-31 11:49:46.494126092 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.produce_compression_type:none	- Enable or disable compression by the Kafka client. Specify `none` to disable compression or one of the supported types [gzip, snappy, lz4, zstd].} {Timestamp:2025-03-31 11:49:46.49414087 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.produce_shutdown_delay_ms:0	- Delay (in milliseconds) to allow for final flush of buffers before shutting down.} {Timestamp:2025-03-31 11:49:46.494142563 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.retries:5	- Number of times to retry a request to a broker.} {Timestamp:2025-03-31 11:49:46.494144397 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.retry_base_backoff_ms:100	- Delay (in milliseconds) for initial retry backoff.} {Timestamp:2025-03-31 11:49:46.49414604 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.sasl_mechanism:	- The SASL mechanism to use when connecting.} {Timestamp:2025-03-31 11:49:46.494147813 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.scram_password:	- Password to use for SCRAM authentication mechanisms.} {Timestamp:2025-03-31 11:49:46.494149576 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - pandaproxy_client.scram_username:	- Username to use for SCRAM authentication mechanisms.} {Timestamp:2025-03-31 11:49:46.49415126 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry.api_doc_dir:/usr/share/redpanda/proxy-api-doc	- API doc directory} {Timestamp:2025-03-31 11:49:46.494154656 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry.mode_mutability:1	- Enable modifications to the read-only `mode` of the Schema Registry.When set to `true`, the entire Schema Registry or its subjects can be switched to `READONLY` or `READWRITE`. This property is useful for preventing unwanted changes to the entire Schema Registry or specific subjects.} {Timestamp:2025-03-31 11:49:46.49415683 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry.schema_registry_api:{{:{host: 0.0.0.0, port: 8081}:<nullopt>}}	- Schema Registry API listener address and port} {Timestamp:2025-03-31 11:49:46.494158503 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry.schema_registry_api_tls:{}	- TLS configuration for Schema Registry API.} {Timestamp:2025-03-31 11:49:46.494160707 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry.schema_registry_replication_factor:{nullopt}	- Replication factor for internal `_schemas` topic.  If unset, defaults to `default_topic_replication`.} {Timestamp:2025-03-31 11:49:46.494176327 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.broker_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} crl file: {nullopt} client_auth_required: 0 }	- TLS configuration for the Kafka API servers to which the HTTP Proxy client should connect.} {Timestamp:2025-03-31 11:49:46.494178641 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.brokers:{{host: 0.0.0.0, port: 9092}}	- Network addresses of the Kafka API servers to which the HTTP Proxy client should connect.} {Timestamp:2025-03-31 11:49:46.494181346 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.client_identifier:{schema_registry_client}	- Custom identifier to include in the Kafka request header for the HTTP Proxy client. This identifier can help debug or monitor client activities.} {Timestamp:2025-03-31 11:49:46.49418326 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.consumer_heartbeat_interval_ms:500	- Interval (in milliseconds) for consumer heartbeats.} {Timestamp:2025-03-31 11:49:46.494185213 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.consumer_rebalance_timeout_ms:2000	- Timeout (in milliseconds) for consumer rebalance.} {Timestamp:2025-03-31 11:49:46.494186997 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.consumer_request_max_bytes:1048576	- Maximum bytes to fetch per request.} {Timestamp:2025-03-31 11:49:46.49418879 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] m
2025-03-31 11:49:50.844336728  ain - application.cc:849 - schema_registry_client.consumer_request_min_bytes:1	- Minimum bytes to fetch per request.} {Timestamp:2025-03-31 11:49:46.494190704 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.consumer_request_timeout_ms:100	- Interval (in milliseconds) for consumer request timeout.} {Timestamp:2025-03-31 11:49:46.494192627 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.consumer_session_timeout_ms:10000	- Timeout (in milliseconds) for consumer session.} {Timestamp:2025-03-31 11:49:46.494202907 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.produce_ack_level:-1	- Number of acknowledgments the producer requires the leader to have received before considering a request complete.} {Timestamp:2025-03-31 11:49:46.49420479 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.produce_batch_delay_ms:0	- Delay (in milliseconds) to wait before sending batch.} {Timestamp:2025-03-31 11:49:46.494206644 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.produce_batch_record_count:0	- Number of records to batch before sending to broker.} {Timestamp:2025-03-31 11:49:46.494208487 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.produce_batch_size_bytes:0	- Number of bytes to batch before sending to broker.} {Timestamp:2025-03-31 11:49:46.494210982 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.produce_compression_type:none	- Enable or disable compression by the Kafka client. Specify `none` to disable compression or one of the supported types [gzip, snappy, lz4, zstd].} {Timestamp:2025-03-31 11:49:46.494213056 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.produce_shutdown_delay_ms:0	- Delay (in milliseconds) to allow for final flush of buffers before shutting down.} {Timestamp:2025-03-31 11:49:46.494214749 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.retries:5	- Number of times to retry a request to a broker.} {Timestamp:2025-03-31 11:49:46.494216572 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.retry_base_backoff_ms:100	- Delay (in milliseconds) for initial retry backoff.} {Timestamp:2025-03-31 11:49:46.494218335 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.sasl_mechanism:	- The SASL mechanism to use when connecting.} {Timestamp:2025-03-31 11:49:46.494220089 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.scram_password:	- Password to use for SCRAM authentication mechanisms.} {Timestamp:2025-03-31 11:49:46.494221892 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - schema_registry_client.scram_username:	- Username to use for SCRAM authentication mechanisms.} {Timestamp:2025-03-31 11:49:46.494235057 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.broker_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} crl file: {nullopt} client_auth_required: 0 }	- TLS configuration for the Kafka API servers to which the HTTP Proxy client should connect.} {Timestamp:2025-03-31 11:49:46.494237241 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.brokers:{{host: 0.0.0.0, port: 9092}}	- Network addresses of the Kafka API servers to which the HTTP Proxy client should connect.} {Timestamp:2025-03-31 11:49:46.494239786 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.client_identifier:{audit_log_client}	- Custom identifier to include in the Kafka request header for the HTTP Proxy client. This identifier can help debug or monitor client activities.} {Timestamp:2025-03-31 11:49:46.494241649 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.consumer_heartbeat_interval_ms:500	- Interval (in milliseconds) for consumer heartbeats.} {Timestamp:2025-03-31 11:49:46.494243543 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.consumer_rebalance_timeout_ms:2000	- Timeout (in milliseconds) for consumer rebalance.} {Timestamp:2025-03-31 11:49:46.494245306 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.consumer_request_max_bytes:1048576	- Maximum bytes to fetch per request.} {Timestamp:2025-03-31 11:49:46.494246989 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.consumer_request_min_bytes:1	- Minimum bytes to fetch per request.} {Timestamp:2025-03-31 11:49:46.494248883 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.consumer_request_timeout_ms:100	- Interval (in milliseconds) for consumer request timeout.} {Timestamp:2025-03-31 11:49:46.494250446 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.consumer_session_timeout_ms:10000	- Timeout (in milliseconds) for consumer session.} {Timestamp:2025-03-31 11:49:46.494252229 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.produce_ack_level:1	- Number of acknowledgments the producer requires the leader to have received before considering a request complete.} {Timestamp:2025-03-31 11:49:46.494253642 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.produce_batch_delay_ms:0	- Delay (in milliseconds) to wait before sending batch.} {Timestamp:2025-03-31 11:49:46.494262889 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.produce_batch_record_count:0	- Number of records to batch before sending to broker.} {Timestamp:2025-03-31 11:49:46.494264382 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.produce_batch_size_bytes:0	- Number of bytes to batch before sending to broker.} {Timestamp:2025-03-31 11:49:46.494266436 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.produce_compression_type:zstd	- Enable or disable compression by the Kafka client. Specify `none` to disable compression or one of the supported types [gzip, snappy, lz4, zstd].} {Timestamp:2025-03-31 11:49:46.494268239 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.produce_shutdown_delay_ms:3000	- Delay (in milliseconds) to allow for final flush of buffers before shutting down.} {Timestamp:2025-03-31 11:49:46.494269552 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.retries:5	- Number of times to retry a request to a broker.} {Timestamp:2025-03-31 11:49:46.494271155 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.retry_base_backoff_ms:100	- Delay (in milliseconds) for initial retry backoff.} {Timestamp:2025-03-31 11:49:46.494272738 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.sasl_mechanism:	- The SASL mechanism to use when connecting.} {Timestamp:2025-03-31 11:49:46.494274351 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.scram_password:	- Password to use for SCRAM authentication mechanisms.} {Timestamp:2025-
2025-03-31 11:49:50.844377685  03-31 11:49:46.494546832 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] main - application.cc:849 - audit_log_client.scram_username:	- Username to use for SCRAM authentication mechanisms.} {Timestamp:2025-03-31 11:49:46.494733773 +0000 UTC Content:INFO  2025-03-31 11:49:46,494 [shard  0:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.497394885 +0000 UTC Content:INFO  2025-03-31 11:49:46,497 [shard 17:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.497401798 +0000 UTC Content:INFO  2025-03-31 11:49:46,497 [shard 20:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.497403321 +0000 UTC Content:INFO  2025-03-31 11:49:46,497 [shard 21:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.497404663 +0000 UTC Content:INFO  2025-03-31 11:49:46,497 [shard 16:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.497676573 +0000 UTC Content:INFO  2025-03-31 11:49:46,497 [shard  9:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.497677996 +0000 UTC Content:INFO  2025-03-31 11:49:46,497 [shard  5:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.497679308 +0000 UTC Content:INFO  2025-03-31 11:49:46,497 [shard 11:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.497680521 +0000 UTC Content:INFO  2025-03-31 11:49:46,497 [shard 19:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.497681703 +0000 UTC Content:INFO  2025-03-31 11:49:46,497 [shard  1:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.497682925 +0000 UTC Content:INFO  2025-03-31 11:49:46,497 [shard 14:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.497684107 +0000 UTC Content:INFO  2025-03-31 11:49:46,497 [shard 18:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.49768533 +0000 UTC Content:INFO  2025-03-31 11:49:46,497 [shard 23:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.49773329 +0000 UTC Content:INFO  2025-03-31 11:49:46,497 [shard 22:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.49783991 +0000 UTC Content:INFO  2025-03-31 11:49:46,497 [shard 12:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.49820235 +0000 UTC Content:INFO  2025-03-31 11:49:46,497 [shard  3:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.498206638 +0000 UTC Content:INFO  2025-03-31 11:49:46,498 [shard 15:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.498208511 +0000 UTC Content:INFO  2025-03-31 11:49:46,498 [shard 13:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.498210134 +0000 UTC Content:INFO  2025-03-31 11:49:46,498 [shard  7:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.498338044 +0000 UTC Content:INFO  2025-03-31 11:49:46,498 [shard  8:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.498349095 +0000 UTC Content:INFO  2025-03-31 11:49:46,498 [shard  4:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.498392657 +0000 UTC Content:INFO  2025-03-31 11:49:46,498 [shard 10:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.498523041 +0000 UTC Content:INFO  2025-03-31 11:49:46,498 [shard  6:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.498561333 +0000 UTC Content:INFO  2025-03-31 11:49:46,498 [shard  2:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:46.499226561 +0000 UTC Content:INFO  2025-03-31 11:49:46,499 [shard  0:main] main - application.cc:563 - Setting abort_on_allocation_failure (abort on OOM): true} {Timestamp:2025-03-31 11:49:46.50932621 +0000 UTC Content:INFO  2025-03-31 11:49:46,509 [shard  0:main] syschecks - Writing pid file "/data/redpanda/pid.lock"} {Timestamp:2025-03-31 11:49:46.525238775 +0000 UTC Content:INFO  2025-03-31 11:49:46,525 [shard  0:main] storage - api.cc:70 - Checking `/data/redpanda` for supported filesystems} {Timestamp:2025-03-31 11:49:46.525393535 +0000 UTC Content:INFO  2025-03-31 11:49:46,525 [shard  0:main] syschecks - Detected file system type is btrfs} {Timestamp:2025-03-31 11:49:46.525396551 +0000 UTC Content:ERROR 2025-03-31 11:49:46,525 [shard  0:main] syschecks - Path: `/data/redpanda' uses btrfs filesystem which is not XFS or ext4. This is a unsupported configuration. You may experience poor performance or instability.} {Timestamp:2025-03-31 11:49:46.528241038 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard  1:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528244103 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard  2:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528245526 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard  5:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528246899 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard  7:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528248231 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard  8:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528249554 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard 10:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528250846 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard  6:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528252138 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard 12:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528253431 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard 15:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528254723 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard 23:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528256016 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard 13:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528257298 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard 20:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528258581 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard 14:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528259893 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard 21:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:
2025-03-31 11:49:50.844418692  46.528261155 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard 22:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528262418 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard 18:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.52826369 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard 17:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528264943 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard  0:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528296752 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard  9:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.52850905 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard 19:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528511064 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard  4:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528536893 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard 11:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528544988 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard 16:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:46.528809975 +0000 UTC Content:INFO  2025-03-31 11:49:46,528 [shard  3:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready} {Timestamp:2025-03-31 11:49:47.760006802 +0000 UTC Content:INFO  2025-03-31 11:49:47,759 [shard  0:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760043981 +0000 UTC Content:INFO  2025-03-31 11:49:47,759 [shard  1:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760047107 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard  7:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760049842 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard 10:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760052688 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard  6:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760055262 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard  9:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760067135 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard  8:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.76006998 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard 14:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760072775 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard 11:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760102271 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard 19:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760104845 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard  5:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.76010728 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard 15:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760109645 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard 18:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760112009 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard 21:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760133499 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard 17:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760136024 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard 20:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760138509 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard 12:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, li
2025-03-31 11:49:50.844477743  sten_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760141013 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard 22:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760143368 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard 23:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760354504 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard  4:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.76035801 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard 13:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760360505 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard  2:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760380042 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard  3:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.760409237 +0000 UTC Content:INFO  2025-03-31 11:49:47,760 [shard 16:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 1018377010, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.761738911 +0000 UTC Content:ERROR 2025-03-31 11:49:47,761 [shard  0:main] cluster - storage space alert: free space at 3.571% on /data/redpanda: 930.512GiB total, 33.231GiB free, min. free for alert 0.000bytes, min. free for degraded 5.000GiB. Please adjust retention policies as needed to avoid running out of space} {Timestamp:2025-03-31 11:49:47.771873696 +0000 UTC Content:INFO  2025-03-31 11:49:47,771 [shard  0:main] storage - segment_set.cc:347 - Recovered: {offset_tracker:{term:0, base_offset:266, committed_offset:277, dirty_offset:277}, compacted_segment=0, finished_self_compaction=0, finished_windowed_compaction=0, generation=1, reader={/data/redpanda/redpanda/kvstore/0_0/266-0-v1.log, (4457 bytes)}, writer=nullptr, cache=nullptr, compaction_index:nullopt, closed=0, tombstone=0, index={file:/data/redpanda/redpanda/kvstore/0_0/266-0-v1.base_index, offsets:266, index:{header_bitflags:0, base_offset:266, max_offset:277, base_timestamp:{timestamp: 1743421784943}, max_timestamp:{timestamp: 1743421785034}, batch_timestamps_are_monotonic:1, with_offset:false, non_data_timestamps:0, broker_timestamp:{nullopt}, num_compactible_records_appended:{12}, clean_compact_timestamp:{nullopt}, may_have_tombstone_records:1, index(1, 1, 1)}, step:32768, needs_persistence:0}}} {Timestamp:2025-03-31 11:49:47.771875519 +0000 UTC Content:INFO  2025-03-31 11:49:47,771 [shard  0:main] kvstore - kvstore.cc:565 - Replaying segment with base offset 266} {Timestamp:2025-03-31 11:49:47.781179415 +0000 UTC Content:INFO  2025-03-31 11:49:47,781 [shard  0:main] main - application.cc:2648 - Loaded stored node ID for node: 0} {Timestamp:2025-03-31 11:49:47.781287097 +0000 UTC Content:INFO  2025-03-31 11:49:47,781 [shard  0:main] main - application.cc:2675 - Loaded existing UUID for node: 9cf6cb6c-d630-4b06-be29-1b0a8da51615} {Timestamp:2025-03-31 11:49:47.782132293 +0000 UTC Content:INFO  2025-03-31 11:49:47,782 [shard  0:main] main - application.cc:2727 - Started RPC server listening at {host: 0.0.0.0, port: 33145}} {Timestamp:2025-03-31 11:49:47.782134016 +0000 UTC Content:INFO  2025-03-31 11:49:47,782 [shard  0:main] main - application.cc:2758 - Running with already-established node ID {0}} {Timestamp:2025-03-31 11:49:47.782155016 +0000 UTC Content:INFO  2025-03-31 11:49:47,782 [shard  0:main] main - application.cc:2833 - Starting Redpanda with node_id 0, cluster UUID {d68df077-1f89-425b-8663-9b6f92a567cc}} {Timestamp:2025-03-31 11:49:47.785244913 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard  0:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785247738 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard  5:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785249842 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard 10:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785251846 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard 16:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785253659 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard  4:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785255583 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard 19:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785257356 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard 23:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785259059 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard 17:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785260823 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard 20:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785262636 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard 22:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785264469 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard 18:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785266363 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard 15:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785268146 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard  8:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.78527013 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard  9:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785271883 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard 11:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785273767 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard 
2025-03-31 11:49:50.844518750  12:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785298954 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard 21:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785301148 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard 14:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785302992 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard 13:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785304955 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard  7:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785306809 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard  1:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785703403 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard  3:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785706078 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard  2:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.785707911 +0000 UTC Content:INFO  2025-03-31 11:49:47,785 [shard  6:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600} {Timestamp:2025-03-31 11:49:47.786140323 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard  0:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786182983 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard  1:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786223028 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard  5:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.7862244 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard 10:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786475542 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard  2:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786515677 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard  7:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.78651729 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard  8:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786518532 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard  3:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786519775 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard  9:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.78672511 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard  6:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786726723 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard 12:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786728326 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard 13:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786729628 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard 14:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786730841 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard 21:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786732083 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard 23:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786733445 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard 20:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786734758 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard 22:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.78673606 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard 16:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786737463 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard 18:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786738715 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard 19:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786739998 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard 17:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.78674134 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard  4:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786742633 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard 11:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786743885 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard 15:main] cluster - producer_state_manager.cc:45 - Started producer state manager} {Timestamp:2025-03-31 11:49:47.786806462 +0000 UTC Content:INFO  2025-03-31 11:49:47,786 [shard  0:main] main - application.cc:1730 - Partition manager started} {Timestamp:2025-03-31 11:49:47.790814452 +0000 UTC Content:INFO  2025-03-31 11:49:47,790 [shard  0:main] main - application.cc:1818 - Archiver service setup, cloud_storage_enabled: false, legacy_upload_mode_enabled: true} {Timestamp:2025-03-31 11:49:47.792677327 +0000 UTC Content:INFO  2025-03-31 11:49:47,792 [shard  0:main] resource_mgmt - storage.cc:182 - Setting new target log data size 558.307GiB. Disk size 930.512GiB reservation percent 25 target percent {80} bytes {nullopt}} {Timestamp:2025-03-31 11:49:47.798787772 +0000 UTC Content:INFO  2025-03-31 11:49:47,798 [shard  0:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799505668 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard  1:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799541205 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard 22:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799544902 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard  2:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799547908 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard  3:main] kafka - server.cc:43 - Cre
2025-03-31 11:49:50.844559737  ating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799563647 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard  4:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799566312 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard  6:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799568987 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard  7:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799571803 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard  8:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799581601 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard  9:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799584386 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard 11:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799593854 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard 13:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799596369 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard 21:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799605055 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard 14:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.79961864 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard 15:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799621386 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard 17:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799633098 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard 16:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799635642 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard 20:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799638377 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard 19:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799648627 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard 18:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799651382 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard 23:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.799660449 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard 12:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.800004905 +0000 UTC Content:INFO  2025-03-31 11:49:47,799 [shard  5:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.800805217 +0000 UTC Content:INFO  2025-03-31 11:49:47,800 [shard 10:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 1527565515, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}} {Timestamp:2025-03-31 11:49:47.803820674 +0000 UTC Content:INFO  2025-03-31 11:49:47,803 [shard  0:main] cluster - cluster_discovery.cc:100 - Controller directory /data/redpanda/redpanda/controller/0_0 not empty; assuming existing cluster exists} {Timestamp:2025-03-31 11:49:47.817800222 +0000 UTC Content:INFO  2025-03-31 11:49:47,817 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1676 - Recovered persistent state from kvstore: voted for: {id: 0, revision: 0}, term: 32} {Timestamp:2025-03-31 11:49:47.817813988 +0000 UTC Content:INFO  2025-03-31 11:49:47,817 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1407 - Starting with voted_for {id: 0, revision: 0} term 32 initial_state false} {Timestamp:2025-03-31 11:49:47.817926639 +0000 UTC Content:INFO  2025-03-31 11:49:47,817 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1451 - Current l
2025-03-31 11:49:50.844616533  og offsets: {start_offset:0, committed_offset:41, committed_offset_term:32, dirty_offset:41, dirty_offset_term:32}, read bootstrap state: data_seen 0 config_seen 0 eol false commit 0 term 0 prev_idx 0 prev_term 0 config_tracker -9223372036854775808 commit_base_tracker -9223372036854775808 configurations []} {Timestamp:2025-03-31 11:49:47.817928132 +0000 UTC Content:INFO  2025-03-31 11:49:47,817 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1478 - Truncating configurations at 41} {Timestamp:2025-03-31 11:49:47.827966426 +0000 UTC Content:INFO  2025-03-31 11:49:47,827 [shard  0:main] storage - segment.cc:817 - Creating new segment /data/redpanda/redpanda/kvstore/0_0/278-0-v1.log} {Timestamp:2025-03-31 11:49:47.846130414 +0000 UTC Content:INFO  2025-03-31 11:49:47,846 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1589 - started raft, log offsets: {start_offset:0, committed_offset:41, committed_offset_term:32, dirty_offset:41, dirty_offset_term:32}, term: 32, configuration: {current: {voters: {{id: 0, revision: 0}}, learners: {}}, old:{nullopt}, revision: 0, update: {nullopt}, version: 6}}} {Timestamp:2025-03-31 11:49:47.846584897 +0000 UTC Content:INFO  2025-03-31 11:49:47,846 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1008 - current node priority 1 is lower than target 4294967295, however the node is the only voter, continue with dispatching vote} {Timestamp:2025-03-31 11:49:47.846587552 +0000 UTC Content:INFO  2025-03-31 11:49:47,846 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:940 - starting pre-vote leader election, current term: 32, leadership transfer: false} {Timestamp:2025-03-31 11:49:47.854361237 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard  0:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.854418274 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard  1:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.854462908 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard  5:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.854498775 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard 13:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.854521848 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard  2:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.85452301 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard  3:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.854523972 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard  4:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.854524854 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard  7:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.854804659 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard  9:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.854817653 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard 12:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.854818915 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard 16:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.854839674 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard 23:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.854840817 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard 22:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.854858971 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard 19:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.855017248 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard 10:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.855051422 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard 17:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.855064606 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard  8:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.855084163 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard 14:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.855085516 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard 21:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.855101195 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard 20:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.855128987 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard  6:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.855190863 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard 15:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.85520505 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard 18:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.855213856 +0000 UTC Content:INFO  2025-03-31 11:49:47,854 [shard 11:main] cluster - drain_manager.cc:21 - Drain manager starting} {Timestamp:2025-03-31 11:49:47.855235427 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard  0:main] cluster - members_manager.cc:98 - starting  members manager with founding brokers: {}} {Timestamp:2025-03-31 11:49:47.855347307 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard  0:main] cluster - controller.cc:572 - Controller log replay starting (to offset 41)} {Timestamp:2025-03-31 11:49:47.855550558 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard  0:main] cluster - bootstrap_backend.cc:92 - Applying update to bootstrap_manager} {Timestamp:2025-03-31 11:49:47.855603718 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard  0:main] cluster - members_manager.cc:855 - initializing cluster state with initial brokers [{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}], and node UUID map: {{9cf6cb6c-d630-4b06-be29-1b0a8da51615 -> 0}} at offset: 1} {Timestamp:2025-03-31 11:49:47.855611883 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard  0:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855665203 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard  2:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.85566891 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard  1:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855672096 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard  4:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Times
2025-03-31 11:49:50.844657490  tamp:2025-03-31 11:49:47.855674831 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard  6:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855688988 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard 11:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855691753 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard  8:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855694568 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard 13:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855697233 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard 14:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855700029 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard 17:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855702764 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard 10:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855705389 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard 18:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855708044 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard  9:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855710739 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard 20:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855728512 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard 15:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855731357 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard 22:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855734002 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard 16:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855736617 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard 19:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855739222 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard 21:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855741837 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard 12:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855744522 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard 23:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855747157 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard  7:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.85580188 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard  5:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.855815515 +0000 UTC Content:INFO  2025-03-31 11:49:47,855 [shard  3:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 24, mem_available 125426466816, disk_available 930, in_fips_mode disabled}}}} {Timestamp:2025-03-31 11:49:47.859517471 +0000 UTC Content:INFO  2025-03-31 11:49:47,859 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] vote_stm.cc:421 - becoming the leader term:33} {Timestamp:2025-03-31 11:49:47.859569538 +0000 UTC Content:INFO  2025-03-31 11:49:47,859 [shard  0:main] storage - segment.cc:817 - Creating new segment /data/red
2025-03-31 11:49:50.844698397  panda/redpanda/controller/0_0/42-33-v1.log} {Timestamp:2025-03-31 11:49:47.864772139 +0000 UTC Content:INFO  2025-03-31 11:49:47,864 [shard  0:main] cluster - controller.cc:583 - Controller log replay complete.} {Timestamp:2025-03-31 11:49:47.864784833 +0000 UTC Content:INFO  2025-03-31 11:49:47,864 [shard  0:main] cluster - controller.cc:1032 - Member of cluster UUID d68df077-1f89-425b-8663-9b6f92a567cc} {Timestamp:2025-03-31 11:49:47.867862367 +0000 UTC Content:INFO  2025-03-31 11:49:47,867 [shard  0:main] cluster - controller_backend.cc:833 - Cleaning up orphan topic files. bootstrap_revision: -9223372036854775808} {Timestamp:2025-03-31 11:49:47.86824245 +0000 UTC Content:INFO  2025-03-31 11:49:47,868 [shard  0:main] cluster - feature_manager.cc:100 - Starting...} {Timestamp:2025-03-31 11:49:47.868244464 +0000 UTC Content:INFO  2025-03-31 11:49:47,868 [shard  0:main] cluster - partition_balancer_backend.cc:108 - partition balancer started} {Timestamp:2025-03-31 11:49:47.868863976 +0000 UTC Content:INFO  2025-03-31 11:49:47,868 [shard  0:main] cluster - metrics_reporter.cc:390 - Generated cluster metrics ID 3953b6f3-7800-4aef-aad3-9890c911f179} {Timestamp:2025-03-31 11:49:47.868875177 +0000 UTC Content:INFO  2025-03-31 11:49:47,868 [shard  0:main] data-migrate - data_migration_backend.cc:107 - backend starting} {Timestamp:2025-03-31 11:49:47.868876881 +0000 UTC Content:INFO  2025-03-31 11:49:47,868 [shard  0:main] data-migrate - data_migration_backend.cc:160 - backend not started as cloud_storage_api is not available} {Timestamp:2025-03-31 11:49:47.869368363 +0000 UTC Content:INFO  2025-03-31 11:49:47,869 [shard  0:main] cluster - leader_balancer.cc:112 - Leader balancer: controller leadership detected. Starting rebalancer in 30 seconds} {Timestamp:2025-03-31 11:49:47.873386371 +0000 UTC Content:ERROR 2025-03-31 11:49:47,873 [shard  0:main] debug-bundle-service - Current specified RPK location /usr/bin/rpk does not exist!  Debug bundle creation is not available until this is fixed!} {Timestamp:2025-03-31 11:49:47.87448391 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard 14:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874486004 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard 18:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874487147 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard 16:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874488158 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard  8:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.87448905 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard  7:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874489862 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard 16:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874524637 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard  9:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874525669 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard  0:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.87452654 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard 19:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874527422 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard  6:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874528284 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard 11:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874529125 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard  6:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874529997 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard 21:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874530859 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard 14:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.87453169 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard 18:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874532532 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard  0:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874541689 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard 19:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874543462 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard 11:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874545085 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard  7:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874546778 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard 21:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874548401 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard  9:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874563911 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard 10:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874565764 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard 12:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874567487 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard 17:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.87456913 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard 15:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874570603 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard 12:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874572086 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard 17:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874573619 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard  2:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0
2025-03-31 11:49:50.844755224  .0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874588577 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard 20:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.8745904 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard 13:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874591993 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard 22:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874593727 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard  3:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.87459535 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard 15:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874596892 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard  4:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874598365 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard 22:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874600118 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard 20:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874601882 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard  4:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.87461179 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard  8:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874614215 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard  2:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874616559 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard 10:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874618914 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard  1:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874620968 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard  1:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874623192 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard  3:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874926661 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard 13:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874928885 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard  5:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874930308 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard  5:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.874931801 +0000 UTC Content:WARN  2025-03-31 11:49:47,874 [shard 23:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`} {Timestamp:2025-03-31 11:49:47.874933314 +0000 UTC Content:INFO  2025-03-31 11:49:47,874 [shard 23:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}} {Timestamp:2025-03-31 11:49:47.87681822 +0000 UTC Content:INFO  2025-03-31 11:49:47,876 [shard  0:main] resource_mgmt - storage.cc:73 - Starting disk space manager service (enabled)} {Timestamp:2025-03-31 11:49:47.88319804 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  0:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883200084 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  0:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883236241 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  3:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883237764 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  5:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883238967 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  7:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883240339 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  6:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883241591 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  9:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883242924 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 11:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883244226 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  2:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883245429 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 10:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883254736 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 13:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883256179 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 15:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883257521 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 17:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883258844 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 16:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883260146 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 18:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883261439 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 20:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883262721 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 23:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883263933 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 12:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883265106 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 14:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883266338 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 21:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.88326766 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 23:main] kafka - 
2025-03-31 11:49:50.844801190  server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883283991 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  8:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883285333 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  7:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883286526 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  9:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883287728 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 15:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.88328889 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 22:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883290052 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 19:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883291335 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 22:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883292527 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 14:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883293649 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  4:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883308046 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  5:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883309409 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 21:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883310621 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 18:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883311823 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  3:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883313045 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 12:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883314218 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 20:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.88331537 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 11:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883316552 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 17:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883317744 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 16:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883325319 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 13:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883326611 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  8:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883327733 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  4:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883336079 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 19:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883337421 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 10:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883511488 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  6:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883513101 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  1:main] kafka - server.cc:273 - kafka rpc protocol - Stopping 0 listeners} {Timestamp:2025-03-31 11:49:47.883514433 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  1:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883548357 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  2:main] kafka - server.cc:285 - kafka rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883729467 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  0:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883818614 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  2:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883820848 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 10:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883822421 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  4:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883823814 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  3:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883825236 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  5:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883826699 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  2:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883828192 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  7:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883829545 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 16:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883830817 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 18:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883832109 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 11:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883833392 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  8:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883865532 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 21:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883866955 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  0:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883867917 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 12:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883868768 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 15:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.88386954 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  8:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883870311 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 11:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 conn
2025-03-31 11:49:50.844842417  ections} {Timestamp:2025-03-31 11:49:47.883871032 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  6:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883871784 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 19:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883877975 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 20:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883882223 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 19:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883907421 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 14:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883908913 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 21:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883909845 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 13:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883910677 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 22:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883911448 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 13:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.88391221 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 23:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883912971 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  3:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883929021 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 22:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883929863 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 14:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883951353 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 17:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.883957204 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard 17:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883968966 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  6:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.883969758 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  7:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.884126071 +0000 UTC Content:INFO  2025-03-31 11:49:47,884 [shard 16:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.884127524 +0000 UTC Content:INFO  2025-03-31 11:49:47,884 [shard 18:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.884142662 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  9:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.884143554 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  1:main] rpc - server.cc:273 - vectorized internal rpc protocol - Stopping 1 listeners} {Timestamp:2025-03-31 11:49:47.884144345 +0000 UTC Content:INFO  2025-03-31 11:49:47,884 [shard  9:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.884145127 +0000 UTC Content:INFO  2025-03-31 11:49:47,884 [shard  1:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.884325725 +0000 UTC Content:INFO  2025-03-31 11:49:47,884 [shard 20:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.884327519 +0000 UTC Content:INFO  2025-03-31 11:49:47,884 [shard 10:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.884328781 +0000 UTC Content:INFO  2025-03-31 11:49:47,884 [shard 23:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.884330054 +0000 UTC Content:INFO  2025-03-31 11:49:47,883 [shard  5:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.884331356 +0000 UTC Content:INFO  2025-03-31 11:49:47,884 [shard 15:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.884332668 +0000 UTC Content:INFO  2025-03-31 11:49:47,884 [shard 12:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.884333901 +0000 UTC Content:INFO  2025-03-31 11:49:47,884 [shard  4:main] rpc - server.cc:285 - vectorized internal rpc protocol - Shutting down 0 connections} {Timestamp:2025-03-31 11:49:47.884705578 +0000 UTC Content:INFO  2025-03-31 11:49:47,884 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:277 - Stopping} {Timestamp:2025-03-31 11:49:47.888580127 +0000 UTC Content:INFO  2025-03-31 11:49:47,888 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] gate-closed, waiting to finish background requests} {Timestamp:2025-03-31 11:49:47.894287726 +0000 UTC Content:INFO  2025-03-31 11:49:47,894 [shard  0:main] raft - [group_id:0, {redpanda/controller/0}] vote_stm.cc:433 - unable to replicate configuration as a leader - error code: 4 - raft::errc::not_leader } {Timestamp:2025-03-31 11:49:47.941892791 +0000 UTC Content:INFO  2025-03-31 11:49:47,941 [shard  0:main] resource_mgmt - storage.cc:88 - Stopping disk space manager service} {Timestamp:2025-03-31 11:49:47.942702881 +0000 UTC Content:INFO  2025-03-31 11:49:47,942 [shard  0:main] auditing - audit_log_manager.cc:843 - Shutting down audit log manager} {Timestamp:2025-03-31 11:49:47.942704243 +0000 UTC Content:INFO  2025-03-31 11:49:47,942 [shard  0:main] auditing - audit_log_manager.cc:607 - stop() invoked on audit_sink} {Timestamp:2025-03-31 11:49:47.942714032 +0000 UTC Content:INFO  2025-03-31 11:49:47,942 [shard  0:main] auditing - audit_log_manager.cc:647 - Setting auditing enabled state to: false} {Timestamp:2025-03-31 11:49:47.942715024 +0000 UTC Content:INFO  2025-03-31 11:49:47,942 [shard  0:main] auditing - audit_log_manager.cc:701 - Ignored update to audit_enabled(), auditing is already disabled} {Timestamp:2025-03-31 11:49:47.946623767 +0000 UTC Content:INFO  2025-03-31 11:49:47,946 [shard  0:main] cluster - leader_balancer.cc:308 - Stopping Leader Balancer...} {Timestamp:2025-03-31 11:49:47.948833653 +0000 UTC Content:INFO  2025-03-31 11:49:47,948 [shard  0:main] data-migrate - data_migration_backend.cc:165 - backend stopping} {Timestamp:2025-03-31 11:49:47.949806028 +0000 UTC Content:INFO  2025-03-31 11:49:47,949 [shard  0:main] data-migrate - data_migration_backend.cc:180 - backend stopped} {Timestamp:2025-03-31 11:49:47.951595746 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard  0:main] cluster - partition_balancer_backend.cc:296 - stopping...} {Timestamp:2025-03-31 11:49:47.951597799 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard  0:main] cluster - metrics_reporter.cc:188 - Stopping Metrics Reporter...} {Timestamp:2025-03-31 11:49:47.951609431 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard  0:main] cluster - feature_mana
2025-03-31 11:49:50.844899514  ger.cc:197 - Stopping Feature Manager...} {Timestamp:2025-03-31 11:49:47.951637824 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard  0:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.951669604 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard  3:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.951691806 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard 11:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.951693479 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard  2:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.951700332 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard  8:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.951701083 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard 13:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.951979435 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard 19:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.951980988 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard 21:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.951982361 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard 10:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.951983784 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard 14:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.951985146 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard  7:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.951986599 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard  4:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.951987941 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard 17:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.951989394 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard 12:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.951998952 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard 23:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.952000415 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard 18:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.952001687 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard 15:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.952002799 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard 20:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.952003941 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard  5:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.952005043 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard  9:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.952006145 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard  1:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.952066599 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard 16:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.952068563 +0000 UTC Content:INFO  2025-03-31 11:49:47,952 [shard  6:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.952069625 +0000 UTC Content:INFO  2025-03-31 11:49:47,951 [shard 22:main] cluster - health_monitor_frontend.cc:43 - Stopping Health Monitor Frontend...} {Timestamp:2025-03-31 11:49:47.953962817 +0000 UTC Content:INFO  2025-03-31 11:49:47,953 [shard  0:main] cluster - health_monitor_backend.cc:107 - Stopping Health Monitor Backend...} {Timestamp:2025-03-31 11:49:47.953964259 +0000 UTC Content:INFO  2025-03-31 11:49:47,953 [shard  0:main] cluster - health_manager.cc:61 - Stopping Health Manager...} {Timestamp:2025-03-31 11:49:47.95397531 +0000 UTC Content:INFO  2025-03-31 11:49:47,953 [shard  0:main] cluster - members_backend.cc:62 - Stopping Members Backend...} {Timestamp:2025-03-31 11:49:47.955583597 +0000 UTC Content:INFO  2025-03-31 11:49:47,955 [shard  0:main] cluster - config_manager.cc:249 - Stopping Config Manager...} {Timestamp:2025-03-31 11:49:47.956377497 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard  0:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956408585 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard  1:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956410238 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard  2:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956411691 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard  4:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956412923 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard 14:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.95642689 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard 10:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956435977 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard 15:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956437339 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard 12:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956438732 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard 16:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956439994 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard 18:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956441156 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard 11:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956550792 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard  6:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956552365 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard 23:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956553597 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard  9:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.95655479 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard 13:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956556172 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard 17:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956557565 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard  7:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956575298 +0000 UTC Content:INFO  2025-03-31 11
2025-03-31 11:49:50.844940982  :49:47,956 [shard 20:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956576621 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard  3:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956577803 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard 19:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956578935 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard  5:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956580087 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard  8:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.956652062 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard 22:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.95674157 +0000 UTC Content:INFO  2025-03-31 11:49:47,956 [shard 21:main] cluster - controller_backend.cc:372 - Stopping Controller Backend...} {Timestamp:2025-03-31 11:49:47.969681999 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  0:main] cluster - members_manager.cc:944 - stopping cluster::members_manager...} {Timestamp:2025-03-31 11:49:47.969683942 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  0:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969685315 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  0:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.969686587 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  1:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969687779 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  6:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969688942 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 11:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969690124 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 20:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969691296 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 16:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969692458 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 14:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.96969354 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  2:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969694702 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 10:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969695885 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 12:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969697017 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 18:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969698119 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  7:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969779952 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 23:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969781946 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 22:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969797345 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 13:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969798707 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 23:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.96979983 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  6:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.969800952 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  5:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969802074 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  9:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969803196 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  4:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969804258 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 14:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.96980536 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 21:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969806472 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 15:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969807544 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 20:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.969836178 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 21:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.9698375 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 12:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.969838602 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 13:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.969839704 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  8:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969840806 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 22:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.969841909 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 17:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969842991 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 17:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.969844093 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 10:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.969845225 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  3:main] cluster - drain_manager.cc:31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.969846337 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 11:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.969847409 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  1:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.969848511 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 16:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.969849583 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 18:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.970293396 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  5:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.970294728 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  9:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.97029584 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  2:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.970296912 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  4:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.970298044 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 19:main] cluster - drain_manager.cc
2025-03-31 11:49:50.844982039  :31 - Drain manager stopping} {Timestamp:2025-03-31 11:49:47.970317892 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  3:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.970319154 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  7:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.970320306 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard  8:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.970321378 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 15:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.97032252 +0000 UTC Content:INFO  2025-03-31 11:49:47,969 [shard 19:main] cluster - drain_manager.cc:35 - Drain manager stopped} {Timestamp:2025-03-31 11:49:47.976827565 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard  0:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.976839648 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard  5:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.976858874 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard  7:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.976860316 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard  4:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.976861579 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard  8:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.976862801 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard  2:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.976877739 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard  9:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.976879011 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard 11:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.976880164 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard 15:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.976888149 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard 20:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.976889451 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard 17:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.976890643 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard 14:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.976891815 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard 22:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.97689969 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard 21:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.976900973 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard 18:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.976908316 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard 16:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.976909589 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard 10:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.976917333 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard 13:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.976925108 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard  3:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.97692644 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard 19:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.976934255 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard 12:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.97694199 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard 23:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.977127297 +0000 UTC Content:INFO  2025-03-31 11:49:47,976 [shard  6:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.977140001 +0000 UTC Content:INFO  2025-03-31 11:49:47,977 [shard  1:main] raft - coordinated_recovery_throttle.cc:134 - Stopping recovery throttle} {Timestamp:2025-03-31 11:49:47.982095428 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard  0:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/0_0} {Timestamp:2025-03-31 11:49:47.982231413 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard  1:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/1_0} {Timestamp:2025-03-31 11:49:47.982234119 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard  5:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/5_0} {Timestamp:2025-03-31 11:49:47.982235912 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard  6:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/6_0} {Timestamp:2025-03-31 11:49:47.982242033 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard  8:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/8_0} {Timestamp:2025-03-31 11:49:47.982243837 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard 16:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/16_0} {Timestamp:2025-03-31 11:49:47.98224561 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard 15:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/15_0} {Timestamp:2025-03-31 11:49:47.982247554 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard 17:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/17_0} {Timestamp:2025-03-31 11:49:47.982249197 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard  2:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/2_0} {Timestamp:2025-03-31 11:49:47.98225101 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard 11:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/11_0} {Timestamp:2025-03-31 11:49:47.982252784 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard  3:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/3_0} {Timestamp:2025-03-31 11:49:47.982254567 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard 14:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/14_0} {Timestamp:2025-03-31 11:49:47.98225628 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard  9:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/9_0} {Timestamp:2025-03-31 11:49:47.982258023 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard 13:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/13_0} {Timestamp:2025-03-31 11:49:47.982259616 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard 21:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/21_0} {Timestamp:2025-03-31 11:49:47.98226132 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard 20:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/20_0} {Timestamp
2025-03-31 11:49:50.845038926  :2025-03-31 11:49:47.982263013 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard 22:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/22_0} {Timestamp:2025-03-31 11:49:47.982264596 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard 19:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/19_0} {Timestamp:2025-03-31 11:49:47.982288991 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard 10:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/10_0} {Timestamp:2025-03-31 11:49:47.982473969 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard 12:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/12_0} {Timestamp:2025-03-31 11:49:47.982475401 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard  4:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/4_0} {Timestamp:2025-03-31 11:49:47.982484428 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard  7:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/7_0} {Timestamp:2025-03-31 11:49:47.982486081 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard 18:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/18_0} {Timestamp:2025-03-31 11:49:47.982529132 +0000 UTC Content:INFO  2025-03-31 11:49:47,982 [shard 23:main] kvstore - kvstore.cc:127 - Stopping kvstore: dir /data/redpanda/redpanda/kvstore/23_0} {Timestamp:2025-03-31 11:49:48.005323498 +0000 UTC Content:INFO  2025-03-31 11:49:48,005 [shard  0:main] main - application.cc:501 - Shutdown complete.} {Timestamp:2025-03-31 11:49:48.005326514 +0000 UTC Content:ERROR 2025-03-31 11:49:48,005 [shard  0:main] main - application.cc:527 - Failure during startup: std::__1::system_error (error system:98, posix_listen failed for address 0.0.0.0:8082: Address already in use)} {Timestamp:2025-03-31 11:49:49.237081886 +0000 UTC Content:} {Timestamp:2025-03-31 11:49:49.237082928 +0000 UTC Content:} {Timestamp:2025-03-31 11:49:49.237083519 +0000 UTC Content:Welcome to the Redpanda community!} {Timestamp:2025-03-31 11:49:49.23708398 +0000 UTC Content:} {Timestamp:2025-03-31 11:49:49.237084711 +0000 UTC Content:Documentation: https://docs.redpanda.com - Product documentation site} {Timestamp:2025-03-31 11:49:49.237085703 +0000 UTC Content:GitHub Discussion: https://github.com/redpanda-data/redpanda/discussions - Longer, more involved discussions} {Timestamp:2025-03-31 11:49:49.237086715 +0000 UTC Content:GitHub Issues: https://github.com/redpanda-data/redpanda/issues - Report and track issues with the codebase} {Timestamp:2025-03-31 11:49:49.237087526 +0000 UTC Content:Support: https://support.redpanda.com - Contact the support team privately} {Timestamp:2025-03-31 11:49:49.237088458 +0000 UTC Content:Product Feedback: https://redpanda.com/feedback - Let us know how we can improve your experience} {Timestamp:2025-03-31 11:49:49.23708932 +0000 UTC Content:Slack: https://redpanda.com/slack - Chat about all things Redpanda. Join the conversation!} {Timestamp:2025-03-31 11:49:49.237090081 +0000 UTC Content:Twitter: https://twitter.com/redpandadata - All the latest Redpanda news!} {Timestamp:2025-03-31 11:49:49.237090492 +0000 UTC Content:} {Timestamp:2025-03-31 11:49:49.237090893 +0000 UTC Content:} {Timestamp:2025-03-31 11:49:49.249200463 +0000 UTC Content:WARN  2025-03-31 11:49:49,249 seastar - Requested AIO slots too large, please increase request capacity in /proc/sys/fs/aio-max-nr. configured:65536 available:65536 requested:264624} {Timestamp:2025-03-31 11:49:49.249234236 +0000 UTC Content:WARN  2025-03-31 11:49:49,249 seastar - max-networking-io-control-blocks adjusted from 10000 to 1704, since AIO slots are unavailable} {Timestamp:2025-03-31 11:49:49.249235408 +0000 UTC Content:INFO  2025-03-31 11:49:49,249 seastar - Reactor backend: epoll} {Timestamp:2025-03-31 11:49:49.300612701 +0000 UTC Content:WARN  2025-03-31 11:49:49,300 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.301364802 +0000 UTC Content:WARN  2025-03-31 11:49:49,301 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.318433927 +0000 UTC Content:WARN  2025-03-31 11:49:49,317 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.31880358 +0000 UTC Content:WARN  2025-03-31 11:49:49,318 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.319862667 +0000 UTC Content:WARN  2025-03-31 11:49:49,319 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.319939411 +0000 UTC Content:WARN  2025-03-31 11:49:49,319 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.323033276 +0000 UTC Content:WARN  2025-03-31 11:49:49,322 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.323678426 +0000 UTC Content:WARN  2025-03-31 11:49:49,323 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.329973727 +0000 UTC Content:WARN  2025-03-31 11:49:49,329 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.330488072 +0000 UTC Content:WARN  2025-03-31 11:49:49,330 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.334136087 +0000 UTC Content:WARN  2025-03-31 11:49:49,333 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.334568688 +0000 UTC Content:WARN  2025-03-31 11:49:49,334 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.337121358 +0000 UTC Content:WARN  2025-03-31 11:49:49,336 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.337265929 +0000 UTC Content:WARN  2025-03-31 11:49:49,337 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.339394903 +0000 UTC Content:WARN  2025-03-31 11:49:49,339 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.339423156 +0000 UTC Content:WARN  2025-03-31 11:49:49,339 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.343103401 +0000 UTC Content:WARN  2025-03-31 11:49:49,342 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_e
2025-03-31 11:49:50.845099980  rror (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.343107409 +0000 UTC Content:WARN  2025-03-31 11:49:49,342 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.345060824 +0000 UTC Content:WARN  2025-03-31 11:49:49,344 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.345117971 +0000 UTC Content:WARN  2025-03-31 11:49:49,345 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.345182762 +0000 UTC Content:WARN  2025-03-31 11:49:49,345 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.345216466 +0000 UTC Content:WARN  2025-03-31 11:49:49,345 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.351237232 +0000 UTC Content:WARN  2025-03-31 11:49:49,351 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.351298818 +0000 UTC Content:WARN  2025-03-31 11:49:49,351 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.351798134 +0000 UTC Content:WARN  2025-03-31 11:49:49,351 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.352011735 +0000 UTC Content:WARN  2025-03-31 11:49:49,351 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.359816749 +0000 UTC Content:WARN  2025-03-31 11:49:49,359 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.359820866 +0000 UTC Content:WARN  2025-03-31 11:49:49,359 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.359852987 +0000 UTC Content:WARN  2025-03-31 11:49:49,359 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.359854019 +0000 UTC Content:WARN  2025-03-31 11:49:49,359 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.369275425 +0000 UTC Content:WARN  2025-03-31 11:49:49,369 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.369401642 +0000 UTC Content:WARN  2025-03-31 11:49:49,369 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.390706764 +0000 UTC Content:WARN  2025-03-31 11:49:49,390 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.390906939 +0000 UTC Content:WARN  2025-03-31 11:49:49,390 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.391314735 +0000 UTC Content:WARN  2025-03-31 11:49:49,391 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.391506494 +0000 UTC Content:WARN  2025-03-31 11:49:49,391 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.391723341 +0000 UTC Content:WARN  2025-03-31 11:49:49,391 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.392011291 +0000 UTC Content:WARN  2025-03-31 11:49:49,391 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.392361088 +0000 UTC Content:WARN  2025-03-31 11:49:49,392 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.39250629 +0000 UTC Content:WARN  2025-03-31 11:49:49,392 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.393680744 +0000 UTC Content:WARN  2025-03-31 11:49:49,393 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.39372686 +0000 UTC Content:WARN  2025-03-31 11:49:49,393 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.39589096 +0000 UTC Content:WARN  2025-03-31 11:49:49,395 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.395985237 +0000 UTC Content:WARN  2025-03-31 11:49:49,395 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.39625344 +0000 UTC Content:WARN  2025-03-31 11:49:49,396 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.396281974 +0000 UTC Content:WARN  2025-03-31 11:49:49,396 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.39691954 +0000 UTC Content:WARN  2025-03-31 11:49:49,396 seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)} {Timestamp:2025-03-31 11:49:49.396936221 +0000 UTC Content:WARN  2025-03-31 11:49:49,396 cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted} {Timestamp:2025-03-31 11:49:49.401409895 +0000 UTC Content:INFO  2025-03-31 11:49:49,401 [shard  0:main] main - application.cc:480 - Redpanda v24.3.8 - b1dd9f54ab1fcd31110608ff214d0937bf30fdb1} {Timestamp:2025-03-31 11:49:49.40141281 +0000 UTC Content:INFO  2025-03-31 11:49:49,401 [shard  0:main] m
2025-03-31 11:49:50.845142240  ain - application.cc:481 - Command line: /opt/redpanda/bin/redpanda --redpanda-cfg /run/service/redpanda/config/redpanda.yaml} {Timestamp:2025-03-31 11:49:49.401414524 +0000 UTC Content:INFO  2025-03-31 11:49:49,401 [shard  0:main] main - application.cc:489 - kernel=6.14.0-2-cachyos-bore, nodename=a13f5c90d4a1, machine=x86_64} {Timestamp:2025-03-31 11:49:49.401416187 +0000 UTC Content:INFO  2025-03-31 11:49:49,401 [shard  0:main] main - application.cc:400 - System resources: { cpus: 24, available memory: 116.812GiB, reserved memory: 8.177GiB}} {Timestamp:2025-03-31 11:49:49.401417429 +0000 UTC Content:INFO  2025-03-31 11:49:49,401 [shard  0:main] main - application.cc:408 - File handle limit: 524288/524288} {Timestamp:2025-03-31 11:49:49.405020048 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] cluster - config_manager.cc:331 - Ignoring invalid property: log_retention_ms=18446744073709551615} {Timestamp:2025-03-31 11:49:49.405481624 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] cluster - config_manager.cc:426 - Ignoring value for 'log_retention_ms' in redpanda.yaml: use `rpk cluster config edit` to edit cluster configuration properties.} {Timestamp:2025-03-31 11:49:49.405483568 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] cluster - config_manager.cc:426 - Ignoring value for 'retention_bytes' in redpanda.yaml: use `rpk cluster config edit` to edit cluster configuration properties.} {Timestamp:2025-03-31 11:49:49.40548477 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] cluster - config_manager.cc:426 - Ignoring value for 'auto_create_topics_enabled' in redpanda.yaml: use `rpk cluster config edit` to edit cluster configuration properties.} {Timestamp:2025-03-31 11:49:49.405492965 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:896 - Cluster configuration properties:} {Timestamp:2025-03-31 11:49:49.405493697 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:897 - (use `rpk cluster config edit` to change)} {Timestamp:2025-03-31 11:49:49.405787458 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.abort_index_segment_size:50000	- Capacity (in number of txns) of an abort index segment. Each partition tracks the aborted transaction offset ranges to help service client requests.If the number transactions increase beyond this threshold, they are flushed to disk to easy memory pressure.Then they're loaded on demand. This configuration controls the maximum number of aborted transactions  before they are flushed to disk.} {Timestamp:2025-03-31 11:49:49.40578869 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.abort_timed_out_transactions_interval_ms:10000	- Interval, in milliseconds, at which Redpanda looks for inactive transactions and aborts them.} {Timestamp:2025-03-31 11:49:49.405789642 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.admin_api_require_auth:0	- Whether Admin API clients must provide HTTP basic authentication headers.} {Timestamp:2025-03-31 11:49:49.405796375 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.aggregate_metrics:0	- Enable aggregation of metrics returned by the `/metrics` endpoint. Aggregation can simplify monitoring by providing summarized data instead of raw, per-instance metrics. Metric aggregation is performed by summing the values of samples by labels and is done when it makes sense by the shard and/or partition labels.} {Timestamp:2025-03-31 11:49:49.405797497 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.alive_timeout_ms:5000	- The amount of time since the last broker status heartbeat. After this time, a broker is considered offline and not alive.} {Timestamp:2025-03-31 11:49:49.40579924 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.alter_topic_cfg_timeout_ms:5000	- The duration, in milliseconds, that Redpanda waits for the replication of entries in the controller log when executing a request to alter topic configurations. This timeout ensures that configuration changes are replicated across the cluster before the alteration request is considered complete.} {Timestamp:2025-03-31 11:49:49.405801043 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.append_chunk_size:16384	- Size of direct write operations to disk in bytes. A larger chunk size can improve performance for write-heavy workloads, but increase latency for these writes as more data is collected before each write operation. A smaller chunk size can decrease write latency, but potentially increase the number of disk I/O operations.} {Timestamp:2025-03-31 11:49:49.405802817 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.audit_client_max_buffer_size:16777216	- Defines the number of bytes allocated by the internal audit client for audit messages. When changing this, you must disable audit logging and then re-enable it for the change to take effect. Consider increasing this if your system generates a very large number of audit records in a short amount of time.} {Timestamp:2025-03-31 11:49:49.405804159 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.audit_enabled:0	- Enables or disables audit logging. When you set this to true, Redpanda checks for an existing topic named `_redpanda.audit_log`. If none is found, Redpanda automatically creates one for you.} {Timestamp:2025-03-31 11:49:49.405805722 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.audit_enabled_event_types:{management, authenticate, admin}	- List of strings in JSON style identifying the event types to include in the audit log. This may include any of the following: `management, produce, consume, describe, heartbeat, authenticate, schema_registry, admin`.} {Timestamp:2025-03-31 11:49:49.405817284 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.audit_excluded_principals:{}	- List of user principals to exclude from auditing.} {Timestamp:2025-03-31 11:49:49.405818146 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.audit_excluded_topics:{}	- List of topics to exclude from auditing.} {Timestamp:2025-03-31 11:49:49.405819718 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.audit_log_num_partitions:12	- Defines the number of partitions used by a newly-created audit topic. This configuration applies only to the audit log topic and may be different from the cluster or other topic configurations. This cannot be altered for existing audit log topics.} {Timestamp:2025-03-31 11:49:49.405821782 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.audit_log_replication_factor:{nullopt}	- Defines the replication factor for a newly-created audit log topic. This configuration applies only to the audit log topic and may be different from the cluster or other topic configurations. This cannot be altered for existing audit log topics. Setting this value is optional. If a value is not provided, Redpanda will use the value specified for `internal_topic_replication_factor`.} {Timestamp:2025-03-31 11:49:49.405823506 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.audit_queue_drain_interval_ms:500	- Interval, in milliseconds, at which Redpanda flushes the queued audit log messages to the audit log topic. Longer intervals may help prevent duplicate messages, especially in high throughput scenarios, but they also increase the risk of data loss during shutdowns where the queue is lost.} {Timestamp:2025-03-31 11:49:49.405825369 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:84
2025-03-31 11:49:50.845202282  9 - redpanda.audit_queue_max_buffer_size_per_shard:1048576	- Defines the maximum amount of memory in bytes used by the audit buffer in each shard. Once this size is reached, requests to log additional audit messages will return a non-retryable error. Limiting the buffer size per shard helps prevent any single shard from consuming excessive memory due to audit log messages.} {Timestamp:2025-03-31 11:49:49.405827213 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.auto_create_topics_enabled:1	- Allow automatic topic creation. To prevent excess topics, this property is not supported on Redpanda Cloud BYOC and Dedicated clusters. You should explicitly manage topic creation for these Redpanda Cloud clusters. If you produce to a topic that doesn't exist, the topic will be created with defaults if this property is enabled.} {Timestamp:2025-03-31 11:49:49.405829206 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_access_key:{nullopt}	- AWS or GCP access key. This access key is part of the credentials that Redpanda requires to authenticate with object storage services for Tiered Storage. This access key is used with the <<cloud_storage_secret_key>> to form the complete credentials required for authentication. To authenticate using IAM roles, see cloud_storage_credentials_source.} {Timestamp:2025-03-31 11:49:49.405830749 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_api_endpoint:{nullopt}	- Optional API endpoint. - AWS: When blank, this is automatically generated using <<cloud_storage_region,region>> and <<cloud_storage_bucket,bucket>>. Otherwise, this uses the value assigned. - GCP: Uses `storage.googleapis.com`.} {Timestamp:2025-03-31 11:49:49.405831521 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_api_endpoint_port:443	- TLS port override.} {Timestamp:2025-03-31 11:49:49.405833494 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_attempt_cluster_restore_on_bootstrap:0	- When set to `true`, Redpanda automatically retrieves cluster metadata from a specified object storage bucket at the cluster's first startup. This option is ideal for orchestrated deployments, such as Kubernetes. Ensure any previous cluster linked to the bucket is fully decommissioned to prevent conflicts between Tiered Storage subsystems.} {Timestamp:2025-03-31 11:49:49.405834767 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_azure_adls_endpoint:{nullopt}	- Azure Data Lake Storage v2 endpoint override. Use when hierarchical namespaces are enabled on your storage account and you have set up a custom endpoint.} {Timestamp:2025-03-31 11:49:49.405836179 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_azure_adls_port:{nullopt}	- Azure Data Lake Storage v2 port override. See also `cloud_storage_azure_adls_endpoint`. Use when Hierarchical Namespaces are enabled on your storage account and you have set up a custom endpoint.} {Timestamp:2025-03-31 11:49:49.405848472 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_azure_container:{nullopt}	- The name of the Azure container to use with Tiered Storage. If `null`, the property is disabled. The container must belong to cloud_storage_azure_storage_account.} {Timestamp:2025-03-31 11:49:49.405850907 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_azure_hierarchical_namespace_enabled:{nullopt}	- Whether or not an Azure hierarchical namespace is enabled on the `cloud_storage_azure_storage_account`. If this property is not set, Â´cloud_storage_azure_shared_key` must be set, and each node tries to determine at startup if a hierarchical namespace is enabled. Setting this property to `true` disables the check and treats a hierarchical namespace as active. Setting to `false` disables the check and treats a hierarchical namespace as not active.} {Timestamp:2025-03-31 11:49:49.40585234 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_azure_managed_identity_id:{nullopt}	- The managed identity ID to use for access to the Azure storage account. To use Azure managed identities, you must set `cloud_storage_credentials_source` to `azure_vm_instance_metadata`.} {Timestamp:2025-03-31 11:49:49.405853923 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_azure_shared_key:{nullopt}	- The shared key to be used for Azure Shared Key authentication with the Azure storage account configured by `cloud_storage_azure_storage_account`.  If `null`, the property is disabled. Redpanda expects this key string to be Base64 encoded.} {Timestamp:2025-03-31 11:49:49.405855025 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_azure_storage_account:{nullopt}	- The name of the Azure storage account to use with Tiered Storage. If `null`, the property is disabled.} {Timestamp:2025-03-31 11:49:49.405856487 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_backend:unknown	- Optional object storage backend variant used to select API capabilities. If not supplied, this will be inferred from other configuration properties. Accepted values: [`unknown`, `aws`, `google_s3_compat`, `azure`, `minio`]} {Timestamp:2025-03-31 11:49:49.40585799 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_background_jobs_quota:5000	- The total number of requests the object storage background jobs can make during one background housekeeping run. This is a per-shard limit. Adjusting this limit can optimize object storage traffic and impact shard performance.} {Timestamp:2025-03-31 11:49:49.405858912 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_bucket:{nullopt}	- AWS or GCP bucket or container that should be used to store data.} {Timestamp:2025-03-31 11:49:49.405861136 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_check_interval:5000	- Minimum interval between Tiered Storage cache trims, measured in milliseconds. This setting dictates the cooldown period after a cache trim operation before another trim can occur. If a cache fetch operation requests a trim but the interval since the last trim has not yet passed, the trim will be postponed until this cooldown expires. Adjusting this interval helps manage the balance between cache size and retrieval performance.} {Timestamp:2025-03-31 11:49:49.405862338 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_chunk_size:16777216	- Size of chunks of segments downloaded into object storage cache. Reduces space usage by only downloading the necessary chunk from a segment.} {Timestamp:2025-03-31 11:49:49.405863791 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_max_objects:100000	- Maximum number of objects that may be held in the Tiered Storage cache.  This applies simultaneously with `cloud_storage_cache_size`, and whichever limit is hit first will trigger trimming of the cache.} {Timestamp:2025-03-31 11:49:49.405865134 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_num_buckets:0	- Divide the object storage cache across the specified number of buckets. This only works for objects with randomized prefixes. The names are not changed when the value is set to zero.} {Timestamp:2025-03-31 11:49:49.405866296 +0000 UTC Content:IN
2025-03-31 11:49:50.845243349  FO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_size:0	- Maximum size of object storage cache. If both this property and cloud_storage_cache_size_percent are set, Redpanda uses the minimum of the two.} {Timestamp:2025-03-31 11:49:49.4058687 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_size_percent:{20}	- Maximum size of the cloud cache as a percentage of unreserved disk space disk_reservation_percent. The default value for this option is tuned for a shared disk configuration. Consider increasing the value if using a dedicated cache disk. The property <<cloud_storage_cache_size,`cloud_storage_cache_size`>> controls the same limit expressed as a fixed number of bytes. If both `cloud_storage_cache_size` and `cloud_storage_cache_size_percent` are set, Redpanda uses the minimum of the two.} {Timestamp:2025-03-31 11:49:49.405870754 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_trim_carryover_bytes:0	- The cache performs a recursive directory inspection during the cache trim. The information obtained during the inspection can be carried over to the next trim operation. This parameter sets a limit on the memory occupied by objects that can be carried over from one trim to next, and allows cache to quickly unblock readers before starting the directory inspection (deprecated)} {Timestamp:2025-03-31 11:49:49.405872137 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_trim_threshold_percent_objects:{nullopt}	- Trim is triggered when the cache reaches this percent of the maximum object count. If this is unset, the default behavioris to start trim when the cache is about 100% full.} {Timestamp:2025-03-31 11:49:49.405873529 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_trim_threshold_percent_size:{nullopt}	- Trim is triggered when the cache reaches this percent of the maximum cache size. If this is unset, the default behavioris to start trim when the cache is about 100% full.} {Timestamp:2025-03-31 11:49:49.405875112 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_trim_walk_concurrency:1	- The maximum number of concurrent tasks launched for directory walk during cache trimming. A higher number allows cache trimming to run faster but can cause latency spikes due to increased pressure on I/O subsystem and syscall threads.} {Timestamp:2025-03-31 11:49:49.405876054 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_chunk_eviction_strategy:eager	- Selects a strategy for evicting unused cache chunks.} {Timestamp:2025-03-31 11:49:49.405876966 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_chunk_prefetch:0	- Number of chunks to prefetch ahead of every downloaded chunk} {Timestamp:2025-03-31 11:49:49.405878379 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cluster_metadata_num_consumer_groups_per_upload:1000	- Number of groups to upload in a single snapshot object during consumer offsets upload. Setting a lower value will mean a larger number of smaller snapshots are uploaded.} {Timestamp:2025-03-31 11:49:49.40587928 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cluster_metadata_retries:5	- Number of attempts metadata operations may be retried.} {Timestamp:2025-03-31 11:49:49.405892735 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cluster_metadata_upload_interval_ms:3600000	- Time interval to wait between cluster metadata uploads.} {Timestamp:2025-03-31 11:49:49.405893717 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cluster_metadata_upload_timeout_ms:60000	- Timeout for cluster metadata uploads.} {Timestamp:2025-03-31 11:49:49.4058953 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_credentials_host:{nullopt}	- The hostname to connect to for retrieving role based credentials. Derived from cloud_storage_credentials_source if not set. Only required when using IAM role based access. To authenticate using access keys, see `cloud_storage_access_key`.} {Timestamp:2025-03-31 11:49:49.405897244 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_credentials_source:config_file	- The source of credentials used to authenticate to object storage services. Required for cluster provider authentication with IAM roles. To authenticate using access keys, see cloud_storage_access_key`. Accepted values: `config_file`, `aws_instance_metadata`, `sts, gcp_instance_metadata`, `azure_vm_instance_metadata`, `azure_aks_oidc_federation` } {Timestamp:2025-03-31 11:49:49.405898186 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_crl_file:{nullopt}	- Path to certificate revocation list for `cloud_storage_trust_file`.} {Timestamp:2025-03-31 11:49:49.405899097 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_disable_archiver_manager:1	- Use legacy upload mode and do not start archiver_manager.} {Timestamp:2025-03-31 11:49:49.405900109 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_disable_chunk_reads:0	- Disable chunk reads and switch back to legacy mode where full segments are downloaded.} {Timestamp:2025-03-31 11:49:49.405901432 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_disable_metadata_consistency_checks:1	- Disable all metadata consistency checks. This will allow redpanda to replay logs with inconsistent tiered-storage metadata. Normally, this option should be disabled.} {Timestamp:2025-03-31 11:49:49.405902734 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_disable_read_replica_loop_for_tests:0	- Begins the read replica sync loop in tiered-storage-enabled topic partitions. The property exists to simplify testing and shouldn't be set in production.} {Timestamp:2025-03-31 11:49:49.405904147 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_disable_remote_labels_for_tests:0	- If 'true', Redpanda disables remote labels and falls back on the hash-based object naming scheme for new topics. This property exists to simplify testing and shouldn't be set in production.} {Timestamp:2025-03-31 11:49:49.405904998 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_disable_tls:0	- Disable TLS for all object storage connections.} {Timestamp:2025-03-31 11:49:49.405906381 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_disable_upload_consistency_checks:0	- Disable all upload consistency checks. This will allow redpanda to upload logs with gaps and replicate metadata with consistency violations. Normally, this options should be disabled.} {Timestamp:2025-03-31 11:49:49.405907613 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_disable_upload_loop_for_tests:0	- Begins the upload loop in tiered-storage-enabled topic partitions. The property exists to simplify testing and shouldn't be set in production.} {Timestamp:2025-03-31 11:49:49.405908515 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_enable_compacted_to
2025-03-31 11:49:50.845284717  pic_reupload:1	- Enable re-uploading data for compacted topics} {Timestamp:2025-03-31 11:49:49.405909377 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_enable_remote_read:0	- Default remote read config value for new topics} {Timestamp:2025-03-31 11:49:49.405910228 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_enable_remote_write:0	- Default remote write value for new topics} {Timestamp:2025-03-31 11:49:49.40591136 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_enable_scrubbing:0	- Enable scrubbing of cloud storage partitions. The scrubber validates the integrity of data and metadata uploaded to cloud storage.} {Timestamp:2025-03-31 11:49:49.405912603 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_enable_segment_merging:1	- Enables adjacent segment merging. The segments are reuploaded if there is an opportunity for that and if it will improve the tiered-storage performance} {Timestamp:2025-03-31 11:49:49.405913585 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_enabled:0	- Enable object storage. Must be set to `true` to use Tiered Storage or Remote Read Replicas.} {Timestamp:2025-03-31 11:49:49.405914496 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_full_scrub_interval_ms:43200000	- Time interval between a final scrub and the next.} {Timestamp:2025-03-31 11:49:49.405915448 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_garbage_collect_timeout_ms:30000	- Timeout for running the cloud storage garbage collection (ms).} {Timestamp:2025-03-31 11:49:49.4059167 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_graceful_transfer_timeout_ms:{5000}	- Time limit on waiting for uploads to complete before a leadership transfer.  If this is null, leadership transfers will proceed without waiting.} {Timestamp:2025-03-31 11:49:49.405917602 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_housekeeping_interval_ms:300000	- Interval for cloud storage housekeeping tasks.} {Timestamp:2025-03-31 11:49:49.405918794 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_hydrated_chunks_per_segment_ratio:0.7	- The maximum number of chunks per segment that can be hydrated at a time. Above this number, unused chunks will be trimmed.} {Timestamp:2025-03-31 11:49:49.405920087 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_hydration_timeout_ms:600000	- Duration to wait for a hydration request to be fulfilled, if hydration is not completed within this time, the consumer will be notified with a timeout error.} {Timestamp:2025-03-31 11:49:49.405921469 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_idle_threshold_rps:10	- The cloud storage request rate threshold for idle state detection. If the average request rate for the configured period is lower than this threshold the cloud storage is considered being idle.} {Timestamp:2025-03-31 11:49:49.405922992 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_idle_timeout_ms:10000	- Timeout used to detect idle state of the cloud storage API. If the average cloud storage request rate is below this threshold for a configured amount of time the cloud storage is considered idle and the housekeeping jobs are started.} {Timestamp:2025-03-31 11:49:49.405923914 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_initial_backoff_ms:100	- Initial backoff time for exponential backoff algorithm (ms)} {Timestamp:2025-03-31 11:49:49.405925126 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_based_scrub_enabled:0	- Scrubber uses the latest cloud storage inventory report, if available, to check if the required objects exist in the bucket or container.} {Timestamp:2025-03-31 11:49:49.405926338 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_id:redpanda_scrubber_inventory	- The name of the scheduled inventory job created by Redpanda to generate bucket or container inventory reports.} {Timestamp:2025-03-31 11:49:49.405928062 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_max_hash_size_during_parse:67108864	- Maximum bytes of hashes which will be held in memory before writing data to disk during inventory report parsing. Affects the number of files written by inventory service to disk during report parsing, as when this limit is reached new files are written to disk.} {Timestamp:2025-03-31 11:49:49.405940184 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_report_check_interval_ms:21600000	- Time interval between checks for a new inventory report in the cloud storage bucket or container.} {Timestamp:2025-03-31 11:49:49.405941417 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_reports_prefix:redpanda_scrubber_inventory	- The prefix to the path in the cloud storage bucket or container where inventory reports will be placed.} {Timestamp:2025-03-31 11:49:49.40594338 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_self_managed_report_config:0	- If enabled, Redpanda will not attempt to create the scheduled report configuration using cloud storage APIs. The scrubbing process will look for reports in the expected paths in the bucket or container, and use the latest report found. Primarily intended for use in testing and on backends where scheduled inventory reports are not supported.} {Timestamp:2025-03-31 11:49:49.405944342 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_manifest_cache_size:1048576	- Amount of memory that can be used to handle tiered-storage metadata} {Timestamp:2025-03-31 11:49:49.405945845 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_manifest_max_upload_interval_sec:{60000}	- Wait at least this long between partition manifest uploads. Actual time between uploads may be greater than this interval. If this property is not set, or null, metadata will be updated after each segment upload.} {Timestamp:2025-03-31 11:49:49.405946697 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_manifest_upload_timeout_ms:10000	- Manifest upload timeout (ms).} {Timestamp:2025-03-31 11:49:49.405949312 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_materialized_manifest_ttl_ms:10000	- The time interval that determines how long the materialized manifest can stay in cache under contention. This parameter is used for performance tuning. When the spillover manifest is materialized and stored in cache and the cache needs to evict it it will use 'cloud_storage_materialized_manifest_ttl_ms' value as a timeout. The cursor that uses the spillover manifest uses this value as a TTL interval after which it stops referencing the manifest making it available for eviction. This only affects spillover manifests under contention.} {Timestamp:2025-03-31 11:49:49.405952848 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - app
2025-03-31 11:49:50.845342355  lication.cc:849 - redpanda.cloud_storage_max_concurrent_hydrations_per_shard:{nullopt}	- Maximum concurrent segment hydrations of remote data per CPU core.  If unset, value of `cloud_storage_max_connections / 2` is used, which means that half of available S3 bandwidth could be used to download data from S3. If the cloud storage cache is empty every new segment reader will require a download. This will lead to 1:1 mapping between number of partitions scanned by the fetch request and number of parallel downloads. If this value is too large the downloads can affect other workloads. In case of any problem caused by the tiered-storage reads this value can be lowered. This will only affect segment hydrations (downloads) but won't affect cached segments. If fetch request is reading from the tiered-storage cache its concurrency will only be limited by available memory.} {Timestamp:2025-03-31 11:49:49.40595372 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_max_connection_idle_time_ms:5000	- Max https connection idle time (ms)} {Timestamp:2025-03-31 11:49:49.405954772 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_max_connections:20	- Maximum simultaneous object storage connections per shard, applicable to upload and download activities.} {Timestamp:2025-03-31 11:49:49.405956024 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_max_materialized_segments_per_shard:{nullopt}	- Maximum concurrent readers of remote data per CPU core.  If unset, value of `topic_partitions_per_shard` multiplied by 2 is used.} {Timestamp:2025-03-31 11:49:49.405956986 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_max_partition_readers_per_shard:{nullopt}	- Maximum partition readers per shard (deprecated)} {Timestamp:2025-03-31 11:49:49.405958819 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_max_segment_readers_per_shard:{nullopt}	- Maximum concurrent I/O cursors of materialized remote segments per CPU core.  If unset, value of `topic_partitions_per_shard` is used, i.e. one segment reader per partition if the shard is at its maximum partition capacity.  These readers are cachedacross Kafka consume requests and store a readahead buffer.} {Timestamp:2025-03-31 11:49:49.405960563 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_max_segments_pending_deletion_per_partition:5000	- The per-partition limit for the number of segments pending deletion from the cloud. Segments can be deleted due to retention or compaction. If this limit is breached and deletion fails, then segments will be orphaned in the cloud and will have to be removed manually} {Timestamp:2025-03-31 11:49:49.405962777 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_max_throughput_per_shard:{1073741824}	- Max throughput used by tiered-storage per shard in bytes per second. This value is an upper bound of the throughput available to the tiered-storage subsystem. This parameter is intended to be used as a safeguard and in tests when we need to set precise throughput value independent of actual storage media. Please use 'cloud_storage_throughput_limit_percent' instead of this parameter in the production environment.} {Timestamp:2025-03-31 11:49:49.405963648 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_metadata_sync_timeout_ms:10000	- Timeout for SI metadata synchronization.} {Timestamp:2025-03-31 11:49:49.405965151 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_min_chunks_per_segment_threshold:5	- The minimum number of chunks per segment for trimming to be enabled. If the number of chunks in a segment is below this threshold, the segment is small enough that all chunks in it can be hydrated at any given time} {Timestamp:2025-03-31 11:49:49.405966123 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_partial_scrub_interval_ms:3600000	- Time interval between two partial scrubs of the same partition.} {Timestamp:2025-03-31 11:49:49.405967185 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_readreplica_manifest_sync_timeout_ms:30000	- Timeout to check if new data is available for partition in S3 for read replica.} {Timestamp:2025-03-31 11:49:49.405967906 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_reconciliation_interval_ms:	- } {Timestamp:2025-03-31 11:49:49.405968928 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_recovery_temporary_retention_bytes_default:1073741824	- Retention in bytes for topics created during automated recovery} {Timestamp:2025-03-31 11:49:49.405970251 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_recovery_topic_validation_depth:10	- Number of metadata segments to validate, from newest to oldest, when `cloud_storage_recovery_topic_validation_mode` is set to `check_manifest_and_segment_metadata`.} {Timestamp:2025-03-31 11:49:49.405973717 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_recovery_topic_validation_mode:check_manifest_existence	- Validation performed before recovering a topic from object storage. In case of failure, the reason for the failure appears as `ERROR` lines in the Redpanda application log. For each topic, this reports errors for all partitions, but for each partition, only the first error is reported. This property accepts the following parameters: `no_check`: Skips the checks for topic recovery. `check_manifest_existence`:  Runs an existence check on each `partition_manifest`. Fails if there are connection issues to the object storage. `check_manifest_and_segment_metadata`: Downloads the manifest and runs a consistency check, comparing the metadata with the cloud storage objects. The process fails if metadata references any missing cloud storage objects.} {Timestamp:2025-03-31 11:49:49.40598571 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_region:{nullopt}	- Cloud provider region that houses the bucket or container used for storage.} {Timestamp:2025-03-31 11:49:49.405986672 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_roles_operation_timeout_ms:30000	- Timeout for IAM role related operations (ms)} {Timestamp:2025-03-31 11:49:49.405987644 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_scrubbing_interval_jitter_ms:600000	- Jitter applied to the cloud storage scrubbing interval.} {Timestamp:2025-03-31 11:49:49.405988435 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_secret_key:{nullopt}	- Cloud provider secret key.} {Timestamp:2025-03-31 11:49:49.405989537 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_segment_max_upload_interval_sec:{3600000}	- Time that segment can be kept locally without uploading it to the remote storage (sec).} {Timestamp:2025-03-31 11:49:49.405990609 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_segment_size_min:{nullopt}	- Smallest acceptable segment size in the cloud storage. Default: cloud_storage_segment_size_target/2} {Timestamp:2025-03-31 11:49:49.405991591 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redp
2025-03-31 11:49:50.845383563  anda.cloud_storage_segment_size_target:{nullopt}	- Desired segment size in the cloud storage. Default: segment.bytes} {Timestamp:2025-03-31 11:49:49.405992453 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_segment_upload_timeout_ms:30000	- Log segment upload timeout (ms)} {Timestamp:2025-03-31 11:49:49.405994486 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_spillover_manifest_max_segments:{nullopt}	- Maximum number of elements in the spillover manifest that can be offloaded to the cloud storage. This property is similar to 'cloud_storage_spillover_manifest_size' but it triggers spillover based on number of segments instead of the size of the manifest in bytes. The property exists to simplify testing and shouldn't be set in the production environment} {Timestamp:2025-03-31 11:49:49.40599618 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_spillover_manifest_size:{65536}	- The size of the manifest which can be offloaded to the cloud. If the size of the local manifest stored in redpanda exceeds cloud_storage_spillover_manifest_size x2 the spillover mechanism will split the manifest into two parts and one of them will be uploaded to S3.} {Timestamp:2025-03-31 11:49:49.405998865 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_throughput_limit_percent:{50}	- Max throughput used by tiered-storage per node expressed as a percentage of the disk bandwidth. If the server has several disks Redpanda will take into account only the one which is used to store tiered-storage cache. Note that even if the tiered-storage is allowed to use full bandwidth of the disk (100%) it won't necessary use it in full. The actual usage depend on your workload and the state of the tiered-storage cache. This parameter is a safeguard that prevents tiered-storage from using too many system resources and not a performance tuning knob.} {Timestamp:2025-03-31 11:49:49.405999856 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_topic_purge_grace_period_ms:30000	- Grace period during which the purger will refuse to purge the topic.} {Timestamp:2025-03-31 11:49:49.406000878 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_trust_file:{nullopt}	- Path to certificate that should be used to validate server certificate during TLS handshake.} {Timestamp:2025-03-31 11:49:49.40600178 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_upload_ctrl_d_coeff:0	- derivative coefficient for upload PID controller.} {Timestamp:2025-03-31 11:49:49.406002732 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_upload_ctrl_max_shares:1000	- maximum number of IO and CPU shares that archival upload can use} {Timestamp:2025-03-31 11:49:49.406003684 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_upload_ctrl_min_shares:100	- minimum number of IO and CPU shares that archival upload can use} {Timestamp:2025-03-31 11:49:49.406004555 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_upload_ctrl_p_coeff:-2	- proportional coefficient for upload PID controller} {Timestamp:2025-03-31 11:49:49.406005307 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_upload_ctrl_update_interval_ms:60000	- } {Timestamp:2025-03-31 11:49:49.406006329 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_upload_loop_initial_backoff_ms:100	- Initial backoff interval when there is nothing to upload for a partition (ms).} {Timestamp:2025-03-31 11:49:49.40600735 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_upload_loop_max_backoff_ms:10000	- Max backoff interval when there is nothing to upload for a partition (ms).} {Timestamp:2025-03-31 11:49:49.406010276 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_url_style:{nullopt}	- Specifies the addressing style to use for Amazon S3 requests. This configuration determines how S3 bucket URLs are formatted. You can choose between: `virtual_host`, (for example, `<bucket-name>.s3.amazonaws.com`), `path`, (for example, `s3.amazonaws.com/<bucket-name>`), and `null`. Path style is supported for backward compatibility with legacy systems. When this property is not set or is `null`, the client tries to use `virtual_host` addressing. If the initial request fails, the client automatically tries the `path` style. If neither addressing style works, Redpanda terminates the startup, requiring manual configuration to proceed.} {Timestamp:2025-03-31 11:49:49.406011108 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cluster_id:{3953b6f3-7800-4aef-aad3-9890c911f179}	- Cluster identifier.} {Timestamp:2025-03-31 11:49:49.406011969 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.compacted_log_segment_size:268435456	- Size (in bytes) for each compacted log segment.} {Timestamp:2025-03-31 11:49:49.406013141 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.compaction_ctrl_backlog_size:{nullopt}	- Target backlog size for compaction controller. If not set the max backlog size is configured to 80% of total disk space available.} {Timestamp:2025-03-31 11:49:49.406014003 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.compaction_ctrl_d_coeff:0.2	- Derivative coefficient for compaction PID controller.} {Timestamp:2025-03-31 11:49:49.406014855 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.compaction_ctrl_i_coeff:0	- Integral coefficient for compaction PID controller.} {Timestamp:2025-03-31 11:49:49.406015786 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.compaction_ctrl_max_shares:1000	- Maximum number of I/O and CPU shares that compaction process can use.} {Timestamp:2025-03-31 11:49:49.406016708 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.compaction_ctrl_min_shares:10	- Minimum number of I/O and CPU shares that compaction process can use.} {Timestamp:2025-03-31 11:49:49.406018 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.compaction_ctrl_p_coeff:-12.5	- Proportional coefficient for compaction PID controller. This must be negative, because the compaction backlog should decrease when the number of compaction shares increases.} {Timestamp:2025-03-31 11:49:49.406018722 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.compaction_ctrl_update_interval_ms:30000	- } {Timestamp:2025-03-31 11:49:49.406019714 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.controller_backend_housekeeping_interval_ms:1000	- Interval between iterations of controller backend housekeeping loop.} {Timestamp:2025-03-31 11:49:49.406032838 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.controller_log_accummulation_rps_capacity_acls_and_users_operations:{nullopt}	- Maximum capacity of rate limit accumulation in controller ACLs and users operations limit.} {Timestamp:2025-03-31 11:49:49.406034051 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.controller_log_accummulation_rps_capacity_configuration_operations:{nullopt}	- M
2025-03-31 11:49:50.845424720  aximum capacity of rate limit accumulation in controller configuration operations limit.} {Timestamp:2025-03-31 11:49:49.406035153 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.controller_log_accummulation_rps_capacity_move_operations:{nullopt}	- Maximum capacity of rate limit accumulation in controller move operations limit.} {Timestamp:2025-03-31 11:49:49.406036335 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.controller_log_accummulation_rps_capacity_node_management_operations:{nullopt}	- Maximum capacity of rate limit accumulation in controller node management operations limit.} {Timestamp:2025-03-31 11:49:49.406037427 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.controller_log_accummulation_rps_capacity_topic_operations:{nullopt}	- Maximum capacity of rate limit accumulationin controller topic operations limit} {Timestamp:2025-03-31 11:49:49.406038559 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.controller_snapshot_max_age_sec:60000	- Maximum amount of time before Redpanda attempts to create a controller snapshot after a new controller command appears.} {Timestamp:2025-03-31 11:49:49.40603922 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.coproc_max_batch_size:	- } {Timestamp:2025-03-31 11:49:49.406039892 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.coproc_max_inflight_bytes:	- } {Timestamp:2025-03-31 11:49:49.406040563 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.coproc_max_ingest_bytes:	- } {Timestamp:2025-03-31 11:49:49.406041244 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.coproc_offset_flush_interval_ms:	- } {Timestamp:2025-03-31 11:49:49.406042286 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.core_balancing_continuous:0	- If set to `true`, move partitions between cores in runtime to maintain balanced partition distribution.} {Timestamp:2025-03-31 11:49:49.406043278 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.core_balancing_debounce_timeout:10000	- Interval, in milliseconds, between trigger and invocation of core balancing.} {Timestamp:2025-03-31 11:49:49.40604454 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.core_balancing_on_core_count_change:1	- If set to `true`, and if after a restart the number of cores changes, Redpanda will move partitions between cores to maintain balanced partition distribution.} {Timestamp:2025-03-31 11:49:49.406045322 +0000 UTC Content:INFO  2025-03-31 11:49:49,405 [shard  0:main] main - application.cc:849 - redpanda.cpu_profiler_enabled:0	- Enables CPU profiling for Redpanda.} {Timestamp:2025-03-31 11:49:49.406046153 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.cpu_profiler_sample_period_ms:100	- The sample period for the CPU profiler.} {Timestamp:2025-03-31 11:49:49.406047035 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.create_topic_timeout_ms:2000	- Timeout, in milliseconds, to wait for new topic creation.} {Timestamp:2025-03-31 11:49:49.406048037 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_binary_max_size:10485760	- The maximum size for a deployable WebAssembly binary that the broker can store.} {Timestamp:2025-03-31 11:49:49.406048949 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_commit_interval_ms:3000	- The commit interval at which data transforms progress.} {Timestamp:2025-03-31 11:49:49.406051022 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_enabled:0	- Enables WebAssembly-powered data transforms directly in the broker. When `data_transforms_enabled` is set to `true`, Redpanda reserves memory for data transforms, even if no transform functions are currently deployed. This memory reservation ensures that adequate resources are available for transform functions when they are needed, but it also means that some memory is allocated regardless of usage.} {Timestamp:2025-03-31 11:49:49.406052375 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_logging_buffer_capacity_bytes:512000	- Buffer capacity for transform logs, per shard. Buffer occupancy is calculated as the total size of buffered log messages; that is, logs emitted but not yet produced.} {Timestamp:2025-03-31 11:49:49.406053567 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_logging_flush_interval_ms:500	- Flush interval for transform logs. When a timer expires, pending logs are collected and published to the `transform_logs` topic.} {Timestamp:2025-03-31 11:49:49.406054629 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_logging_line_max_bytes:1024	- Transform log lines truncate to this length. Truncation occurs after any character escaping.} {Timestamp:2025-03-31 11:49:49.406056392 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_per_core_memory_reservation:20971520	- The amount of memory to reserve per core for data transform (Wasm) virtual machines. Memory is reserved on boot. The maximum number of functions that can be deployed to a cluster is equal to `data_transforms_per_core_memory_reservation` / `data_transforms_per_function_memory_limit`.} {Timestamp:2025-03-31 11:49:49.406058046 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_per_function_memory_limit:2097152	- The amount of memory to give an instance of a data transform (Wasm) virtual machine. The maximum number of functions that can be deployed to a cluster is equal to `data_transforms_per_core_memory_reservation` / `data_transforms_per_function_memory_limit`.} {Timestamp:2025-03-31 11:49:49.406059108 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_read_buffer_memory_percentage:45	- The percentage of available memory in the transform subsystem to use for read buffers.} {Timestamp:2025-03-31 11:49:49.40606024 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_runtime_limit_ms:3000	- The maximum amount of runtime to start up a data transform, and the time it takes for a single record to be transformed.} {Timestamp:2025-03-31 11:49:49.406061302 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.data_transforms_write_buffer_memory_percentage:45	- The percentage of available memory in the transform subsystem to use for write buffers.} {Timestamp:2025-03-31 11:49:49.406062384 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.datalake_coordinator_snapshot_max_delay_secs:900000	- Maximum amount of time the coordinator waits to snapshot after a command appears in the log.} {Timestamp:2025-03-31 11:49:49.406063646 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.debug_bundle_auto_removal_seconds:{nullopt}	- If set, how long debug bundles are kept in the debug bundle storage directory after they are created. If not set, debug bundles are kept indefinitely.} {Timestamp:2025-03-31 11:49:49.406065189 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.debug_bundle_storage_dir:{nullopt}	- Path to the debu
2025-03-31 11:49:50.845482057  g bundle storage directory. Note: Changing this path does not clean up existing debug bundles. If not set, the debug bundle is stored in the Redpanda data directory specified in the redpanda.yaml broker configuration file.} {Timestamp:2025-03-31 11:49:49.406066231 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.debug_load_slice_warning_depth:{nullopt}	- The recursion depth after which debug logging is enabled automatically for the log reader.} {Timestamp:2025-03-31 11:49:49.406067573 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.default_leaders_preference:none	- Default settings for preferred location of topic partition leaders. It can be either "none" (no preference), or "racks:<rack1>,<rack2>,..." (prefer brokers with rack id from the list).} {Timestamp:2025-03-31 11:49:49.406068375 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.default_num_windows:10	- Default number of quota tracking windows.} {Timestamp:2025-03-31 11:49:49.406069197 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.default_topic_partitions:1	- Default number of partitions per topic.} {Timestamp:2025-03-31 11:49:49.406083443 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.default_topic_replications:1	- Default replication factor for new topics.} {Timestamp:2025-03-31 11:49:49.406084325 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.default_window_sec:1000	- Default quota tracking window size in milliseconds.} {Timestamp:2025-03-31 11:49:49.406085136 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.development_enable_cloud_topics:0	- Enable cloud topics.} {Timestamp:2025-03-31 11:49:49.406086038 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.development_feature_property_testing_only:0	- Development feature property for testing only.} {Timestamp:2025-03-31 11:49:49.40608682 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.disable_batch_cache:0	- Disable batch cache in log manager.} {Timestamp:2025-03-31 11:49:49.406087942 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.disable_cluster_recovery_loop_for_tests:0	- Disables the cluster recovery loop. This property is used to simplify testing and should not be set in production.} {Timestamp:2025-03-31 11:49:49.406088853 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.disable_metrics:0	- Disable registering the metrics exposed on the internal `/metrics` endpoint.} {Timestamp:2025-03-31 11:49:49.406089785 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.disable_public_metrics:0	- Disable registering the metrics exposed on the `/public_metrics` endpoint.} {Timestamp:2025-03-31 11:49:49.406091719 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.disk_reservation_percent:25	- The percentage of total disk capacity that Redpanda will avoid using. This applies both when cloud cache and log data share a disk, as well as when cloud cache uses a dedicated disk. It is recommended to not run disks near capacity to avoid blocking I/O due to low disk space, as well as avoiding performance issues associated with SSD garbage collection.} {Timestamp:2025-03-31 11:49:49.40609254 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.election_timeout_ms:1500	- Election timeout expressed in milliseconds.} {Timestamp:2025-03-31 11:49:49.406093171 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.enable_admin_api:	- } {Timestamp:2025-03-31 11:49:49.406094093 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.enable_auto_rebalance_on_node_add:0	- Enable automatic partition rebalancing when new nodes are added} {Timestamp:2025-03-31 11:49:49.406095055 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.enable_cluster_metadata_upload_loop:1	- Enables cluster metadata uploads. Required for whole cluster restore.} {Timestamp:2025-03-31 11:49:49.406095917 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.enable_controller_log_rate_limiting:0	- Limits the write rate for the controller log.} {Timestamp:2025-03-31 11:49:49.406096548 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.enable_coproc:	- } {Timestamp:2025-03-31 11:49:49.406098762 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.enable_developmental_unrecoverable_data_corrupting_features:	- Development features should never be enabled in a production cluster, or any cluster where stability, data loss, or the ability to upgrade are a concern. To enable experimental features, set the value of this configuration option to the current unix epoch expressed in seconds. The value must be within one hour of the current time on the broker.Once experimental features are enabled they cannot be disabled} {Timestamp:2025-03-31 11:49:49.406099523 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.enable_idempotence:1	- Enable idempotent producers.} {Timestamp:2025-03-31 11:49:49.406100325 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.enable_leader_balancer:1	- Enable automatic leadership rebalancing.} {Timestamp:2025-03-31 11:49:49.406102529 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.enable_metrics_reporter:1	- Enable the cluster metrics reporter. If `true`, the metrics reporter collects and exports to Redpanda Data a set of customer usage metrics at the interval set by `metrics_reporter_report_interval`. The cluster metrics of the metrics reporter are different from the monitoring metrics. * The metrics reporter exports customer usage metrics for consumption by Redpanda Data.* Monitoring metrics are exported for consumption by Redpanda users.} {Timestamp:2025-03-31 11:49:49.406103321 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.enable_mpx_extensions:0	- Enable Redpanda extensions for MPX.} {Timestamp:2025-03-31 11:49:49.406104122 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.enable_pid_file:1	- Enable PID file. You should not need to change.} {Timestamp:2025-03-31 11:49:49.406104914 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.enable_rack_awareness:0	- Enable rack-aware replica assignment.} {Timestamp:2025-03-31 11:49:49.406106016 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.enable_sasl:0	- Enable SASL authentication for Kafka connections. Authorization is required to modify this property. See also `kafka_enable_authorization`.} {Timestamp:2025-03-31 11:49:49.406108019 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.enable_schema_id_validation:none	- Mode to enable server-side schema ID validation. Accepted Values: * `none`: Schema validation is disabled (no schema ID checks are done). Associated topic properties cannot be modified. * `redpanda`: Schema validation is enabled. Only Redpanda topic properties are accepted. * `compat`: Schema validation is enabled. Both Redpanda and compatible topic properties are accepted.} {Timestamp:2025-03-31 11:49:49.406108801 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpand
2025-03-31 11:49:50.845523385  a.enable_transactions:1	- Enable transactions (atomic writes).} {Timestamp:2025-03-31 11:49:49.406109793 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.enable_usage:0	- Enables the usage tracking mechanism, storing windowed history of kafka/cloud_storage metrics over time.} {Timestamp:2025-03-31 11:49:49.406110875 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.features_auto_enable:1	- Whether new feature flags auto-activate after upgrades (true) or must wait for manual activation via the Admin API (false).} {Timestamp:2025-03-31 11:49:49.406111726 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.fetch_max_bytes:57671680	- Maximum number of bytes returned in a fetch request.} {Timestamp:2025-03-31 11:49:49.406112538 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.fetch_pid_d_coeff:0	- Derivative coefficient for fetch PID controller.} {Timestamp:2025-03-31 11:49:49.406113359 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.fetch_pid_i_coeff:0.01	- Integral coefficient for fetch PID controller.} {Timestamp:2025-03-31 11:49:49.406114331 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.fetch_pid_max_debounce_ms:100	- The maximum debounce time the fetch PID controller will apply, in milliseconds.} {Timestamp:2025-03-31 11:49:49.406115163 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.fetch_pid_p_coeff:100	- Proportional coefficient for fetch PID controller.} {Timestamp:2025-03-31 11:49:49.406116225 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.fetch_pid_target_utilization_fraction:0.2	- A fraction, between 0 and 1, for the target reactor utilization of the fetch scheduling group.} {Timestamp:2025-03-31 11:49:49.406128818 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.fetch_read_strategy:non_polling	- The strategy used to fulfill fetch requests. * `polling`: Repeatedly polls every partition in the request for new data. The polling interval is set by `fetch_reads_debounce_timeout` (deprecated). * `non_polling`: The backend is signaled when a partition has new data, so Redpanda does not need to repeatedly read from every partition in the fetch. Redpanda Data recommends using this value for most workloads, because it can improve fetch latency and CPU utilization. * `non_polling_with_debounce`: This option behaves like `non_polling`, but it includes a debounce mechanism with a fixed delay specified by `fetch_reads_debounce_timeout` at the start of each fetch. By introducing this delay, Redpanda can accumulate more data before processing, leading to fewer fetch operations and returning larger amounts of data. Enabling this option reduces reactor utilization, but it may also increase end-to-end latency.} {Timestamp:2025-03-31 11:49:49.40612989 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.fetch_reads_debounce_timeout:1	- Time to wait for the next read in fetch requests when the requested minimum bytes was not reached.} {Timestamp:2025-03-31 11:49:49.406131594 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.fetch_session_eviction_timeout_ms:60000	- Time duration after which the inactive fetch session is removed from the fetch session cache. Fetch sessions are used to implement the incremental fetch requests where a consumer does not send all requested partitions to the server but the server tracks them for the consumer.} {Timestamp:2025-03-31 11:49:49.406132315 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.find_coordinator_timeout_ms:	- } {Timestamp:2025-03-31 11:49:49.406133036 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.full_raft_configuration_recovery_pattern:	- } {Timestamp:2025-03-31 11:49:49.406133938 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.group_initial_rebalance_delay:3000	- Delay added to the rebalance phase to wait for new members.} {Timestamp:2025-03-31 11:49:49.406135321 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.group_max_session_timeout_ms:300000	- The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.} {Timestamp:2025-03-31 11:49:49.406136733 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.group_min_session_timeout_ms:6000	- The minimum allowed session timeout for registered consumers. Shorter timeouts result in quicker failure detection at the cost of more frequent consumer heartbeating, which can overwhelm broker resources.} {Timestamp:2025-03-31 11:49:49.406137535 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.group_new_member_join_timeout:30000	- Timeout for new member joins.} {Timestamp:2025-03-31 11:49:49.406138517 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.group_offset_retention_check_ms:600000	- Frequency rate at which the system should check for expired group offsets.} {Timestamp:2025-03-31 11:49:49.406139528 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.group_offset_retention_sec:{604800000}	- Consumer group offset retention seconds. To disable offset retention, set this to null.} {Timestamp:2025-03-31 11:49:49.40614041 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.group_topic_partitions:16	- Number of partitions in the internal group membership topic.} {Timestamp:2025-03-31 11:49:49.406141232 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.health_manager_tick_interval:180000	- How often the health manager runs.} {Timestamp:2025-03-31 11:49:49.406142234 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.health_monitor_max_metadata_age:10000	- Maximum age of the metadata cached in the health monitor of a non-controller broker.} {Timestamp:2025-03-31 11:49:49.406143095 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.health_monitor_tick_interval:10000	- How often health monitor refresh cluster state} {Timestamp:2025-03-31 11:49:49.406144067 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.http_authentication:{BASIC}	- A list of supported HTTP authentication mechanisms. Accepted Values: `BASIC`, `OIDC`} {Timestamp:2025-03-31 11:49:49.406145239 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.iceberg_catalog_base_location:redpanda-iceberg-catalog	- Base path for the cloud object storage-backed Iceberg catalog. After Iceberg is enabled, do not change this value.} {Timestamp:2025-03-31 11:49:49.406146752 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.iceberg_catalog_commit_interval_ms:60000	- The frequency at which the Iceberg coordinator commits topic files to the catalog. This is the interval between commit transactions across all topics monitored by the coordinator, not the interval between individual commits.} {Timestamp:2025-03-31 11:49:49.406147864 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.iceberg_catalog_type:object_storage	- Iceberg catalog type that Redpanda will use to commit table metadata updates. Supported types: 'rest', 'object_storage'} {Times
2025-03-31 11:49:50.845564302  tamp:2025-03-31 11:49:49.406149026 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.iceberg_delete:1	- Default value for the redpanda.iceberg.delete topic property that determines if the corresponding Iceberg table is deleted upon deleting the topic.} {Timestamp:2025-03-31 11:49:49.40615095 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.iceberg_enabled:0	- Enables the translation of topic data into Iceberg tables. Setting iceberg_enabled to true activates the feature at the cluster level, but each topic must also set the redpanda.iceberg.enabled topic-level property to true to use it. If iceberg_enabled is set to false, the feature is disabled for all topics in the cluster, overriding any topic-level settings.} {Timestamp:2025-03-31 11:49:49.406152152 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_client_id:{nullopt}	- Iceberg REST catalog user ID. This ID is used to query the catalog API for the OAuth token. Required if catalog type is set to `rest`} {Timestamp:2025-03-31 11:49:49.406153214 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_client_secret:{nullopt}	- Secret to authenticate against Iceberg REST catalog. Required if catalog type is set to `rest`} {Timestamp:2025-03-31 11:49:49.406154186 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_crl_file:{nullopt}	- Path to certificate revocation list for `iceberg_rest_catalog_trust_file`.} {Timestamp:2025-03-31 11:49:49.406155028 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_endpoint:{nullopt}	- URL of Iceberg REST catalog endpoint} {Timestamp:2025-03-31 11:49:49.40615614 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_prefix:{nullopt}	- Prefix part of the Iceberg REST catalog URL. Prefix is appended to the catalog path f.e. '/v1/{prefix}/namespaces'} {Timestamp:2025-03-31 11:49:49.406157252 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_request_timeout_ms:10000	- Maximum length of time that Redpanda waits for a response from the REST catalog before aborting the request} {Timestamp:2025-03-31 11:49:49.406158644 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_token:{nullopt}	- Token used to access the REST Iceberg catalog. If the token is present, Redpanda ignores credentials stored in the properties iceberg_rest_catalog_client_id and iceberg_rest_catalog_client_secret} {Timestamp:2025-03-31 11:49:49.406159666 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_trust_file:{nullopt}	- Path to a file containing a certificate chain to trust for the REST Iceberg catalog} {Timestamp:2025-03-31 11:49:49.406160969 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.id_allocator_batch_size:1000	- The ID allocator allocates messages in batches (each batch is a one log record) and then serves requests from memory without touching the log until the batch is exhausted.} {Timestamp:2025-03-31 11:49:49.406174905 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.id_allocator_log_capacity:100	- Capacity of the `id_allocator` log in number of batches. After it reaches `id_allocator_stm`, it truncates the log's prefix.} {Timestamp:2025-03-31 11:49:49.406175616 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.id_allocator_replication:	- } {Timestamp:2025-03-31 11:49:49.406256688 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.initial_retention_local_target_bytes_default:{nullopt}	- Initial local retention size target for partitions of topics with Tiered Storage enabled. If no initial local target retention is configured all locally retained data will be delivered to learner when joining partition replica set.} {Timestamp:2025-03-31 11:49:49.406259103 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.initial_retention_local_target_ms_default:{nullopt}	- Initial local retention time target for partitions of topics with Tiered Storage enabled. If no initial local target retention is configured all locally retained data will be delivered to learner when joining partition replica set.} {Timestamp:2025-03-31 11:49:49.406260195 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.internal_topic_replication_factor:3	- Target replication factor for internal topics.} {Timestamp:2025-03-31 11:49:49.406261137 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.join_retry_timeout_ms:5000	- Time between cluster join retries in milliseconds.} {Timestamp:2025-03-31 11:49:49.406262108 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_admin_topic_api_rate:{nullopt}	- Target quota rate (partition mutations per default_window_sec)} {Timestamp:2025-03-31 11:49:49.406263251 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_batch_max_bytes:1048576	- Maximum size of a batch processed by the server. If the batch is compressed, the limit applies to the compressed batch size.} {Timestamp:2025-03-31 11:49:49.406264513 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_client_group_byte_rate_quota:{}	- Per-group target produce quota byte rate (bytes per second). Client is considered part of the group if client_id contains clients_prefix.} {Timestamp:2025-03-31 11:49:49.406265755 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_client_group_fetch_byte_rate_quota:{}	- Per-group target fetch quota byte rate (bytes per second). Client is considered part of the group if client_id contains clients_prefix} {Timestamp:2025-03-31 11:49:49.406266948 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_connection_rate_limit:{nullopt}	- Maximum connections per second for one core. If `null` (the default), then the number of connections per second is unlimited.} {Timestamp:2025-03-31 11:49:49.40626816 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_connection_rate_limit_overrides:{}	- Overrides the maximum connections per second for one core for the specified IP addresses (for example, `['127.0.0.1:90', '50.20.1.1:40']`)} {Timestamp:2025-03-31 11:49:49.406269172 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_connections_max:{nullopt}	- Maximum number of Kafka client connections per broker. If `null`, the property is disabled.} {Timestamp:2025-03-31 11:49:49.406270394 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_connections_max_overrides:{}	- A list of IP addresses for which Kafka client connection limits are overridden and don't apply. For example, `(['127.0.0.1:90', '50.20.1.1:40']).`} {Timestamp:2025-03-31 11:49:49.406271486 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_connections_max_per_ip:{nullopt}	- Maximum number of Kafka client connections per IP address, per broker. If `null`, the property is disabled.} {Timestamp:2025-03-31 11:49:49.406273229 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_enable_authorization:{nullopt}	- Flag to req
2025-03-31 11:49:50.845621880  uire authorization for Kafka connections. If `null`, the property is disabled, and authorization is instead enabled by enable_sasl. * `null`: Ignored. Authorization is enabled with `enable_sasl`: `true` * `true`: authorization is required. * `false`: authorization is disabled.} {Timestamp:2025-03-31 11:49:49.406274381 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_enable_describe_log_dirs_remote_storage:1	- Whether to include Tiered Storage as a special remote:// directory in `DescribeLogDirs Kafka` API requests.} {Timestamp:2025-03-31 11:49:49.406275253 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_enable_partition_reassignment:1	- Enable the Kafka partition reassignment API.} {Timestamp:2025-03-31 11:49:49.406276085 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_group_recovery_timeout_ms:30000	- Kafka group recovery timeout.} {Timestamp:2025-03-31 11:49:49.406277117 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_max_bytes_per_fetch:67108864	- Limit fetch responses to this many bytes, even if the total of partition bytes limits is higher.} {Timestamp:2025-03-31 11:49:49.4062787 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_memory_batch_size_estimate_for_fetch:1048576	- The size of the batch used to estimate memory consumption for fetch requests, in bytes. Smaller sizes allow more concurrent fetch requests per shard. Larger sizes prevent running out of memory because of too many concurrent fetch requests.} {Timestamp:2025-03-31 11:49:49.406279862 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_memory_share_for_fetch:0.5	- The share of Kafka subsystem memory that can be used for fetch read buffers, as a fraction of the Kafka subsystem memory amount.} {Timestamp:2025-03-31 11:49:49.406280964 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_mtls_principal_mapping_rules:{nullopt}	- Principal mapping rules for mTLS authentication on the Kafka API. If `null`, the property is disabled.} {Timestamp:2025-03-31 11:49:49.406282336 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_nodelete_topics:{_redpanda.audit_log, __consumer_offsets, _schemas}	- A list of topics that are protected from deletion and configuration changes by Kafka clients. Set by default to a list of Redpanda internal topics.} {Timestamp:2025-03-31 11:49:49.406283469 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_noproduce_topics:{}	- A list of topics that are protected from being produced to by Kafka clients. Set by default to a list of Redpanda internal topics.} {Timestamp:2025-03-31 11:49:49.40628438 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_depth_alpha:0.8	- Smoothing factor for Kafka queue depth control depth tracking.} {Timestamp:2025-03-31 11:49:49.406285252 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_depth_update_ms:7000	- Update frequency for Kafka queue depth control.} {Timestamp:2025-03-31 11:49:49.406286003 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_enable:0	- Enable kafka queue depth control.} {Timestamp:2025-03-31 11:49:49.406286905 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_idle_depth:10	- Queue depth when idleness is detected in Kafka queue depth control.} {Timestamp:2025-03-31 11:49:49.406287837 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_latency_alpha:0.002	- Smoothing parameter for Kafka queue depth control latency tracking.} {Timestamp:2025-03-31 11:49:49.406288688 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_max_depth:100	- Maximum queue depth used in kafka queue depth control.} {Timestamp:2025-03-31 11:49:49.40628962 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_max_latency_ms:80	- Maximum latency threshold for Kafka queue depth control depth tracking.} {Timestamp:2025-03-31 11:49:49.406290472 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_min_depth:1	- Minimum queue depth used in Kafka queue depth control.} {Timestamp:2025-03-31 11:49:49.406291383 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_window_count:12	- Number of windows used in kafka queue depth control latency tracking.} {Timestamp:2025-03-31 11:49:49.406292275 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_qdc_window_size_ms:1500	- Window size for Kafka queue depth control latency tracking.} {Timestamp:2025-03-31 11:49:49.406323333 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_quota_balancer_min_shard_throughput_bps:256	- The minimum value of the throughput quota a shard can get in the process of quota balancing, expressed in bytes per second. The value applies equally to ingress and egress traffic. `kafka_quota_balancer_min_shard_throughput_bps` doesn't override the limit settings, `kafka_throughput_limit_node_in_bps` and `kafka_throughput_limit_node_out_bps`. Consequently, the value of `kafka_throughput_limit_node_in_bps` or `kafka_throughput_limit_node_out_bps` can result in lesser throughput than kafka_quota_balancer_min_shard_throughput_bps. Both `kafka_quota_balancer_min_shard_throughput_ratio` and `kafka_quota_balancer_min_shard_throughput_bps` can be specified at the same time. In this case, the balancer will not decrease the effective shard quota below the largest bytes-per-second (bps) value of each of these two properties. If set to `0`, no minimum is enforced.} {Timestamp:2025-03-31 11:49:49.406331238 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_quota_balancer_min_shard_throughput_ratio:0.01	- The minimum value of the throughput quota a shard can get in the process of quota balancing, expressed as a ratio of default shard quota. While the value applies equally to ingress and egress traffic, the default shard quota can be different for ingress and egress and therefore result in different minimum throughput bytes-per-second (bps) values. Both `kafka_quota_balancer_min_shard_throughput_ratio` and `kafka_quota_balancer_min_shard_throughput_bps` can be specified at the same time. In this case, the balancer will not decrease the effective shard quota below the largest bps value of each of these two properties. If set to `0.0`, the minimum is disabled. If set to `1.0`, the balancer won't be able to rebalance quota without violating this ratio, preventing the balancer from adjusting shards' quotas.} {Timestamp:2025-03-31 11:49:49.406333943 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_quota_balancer_node_period_ms:0	- Intra-node throughput quota balancer invocation period, in milliseconds. When set to 0, the balancer is disabled and makes all the throughput quotas immutable.} {Timestamp:2025-03-31 11:49:49.406336207 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_quota_balancer_window_ms:5000	- Time window used to average current throughput measurement for quota balancer, in milliseconds.} {Timestamp:2025-03-31 11:49:49.406338191 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_request_max_bytes:104857600	- Maximum size of a s
2025-03-31 11:49:50.845663257  ingle request processed using the Kafka API.} {Timestamp:2025-03-31 11:49:49.406340405 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_rpc_server_stream_recv_buf:{nullopt}	- Maximum size of the user-space receive buffer. If `null`, this limit is not applied.} {Timestamp:2025-03-31 11:49:49.406342579 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_rpc_server_tcp_recv_buf:{nullopt}	- Size of the Kafka server TCP receive buffer. If `null`, the property is disabled.} {Timestamp:2025-03-31 11:49:49.406344733 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_rpc_server_tcp_send_buf:{nullopt}	- Size of the Kafka server TCP transmit buffer. If `null`, the property is disabled.} {Timestamp:2025-03-31 11:49:49.406348621 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_sasl_max_reauth_ms:{nullopt}	- The maximum time between Kafka client reauthentications. If a client has not reauthenticated a connection within this time frame, that connection is torn down. If this property is not set (or set to `null`), session expiry is disabled, and a connection could live long after the client's credentials are expired or revoked.} {Timestamp:2025-03-31 11:49:49.406350685 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_schema_id_validation_cache_capacity:128	- Per-shard capacity of the cache for validating schema IDs.} {Timestamp:2025-03-31 11:49:49.406353921 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_tcp_keepalive_probe_interval_seconds:60000	- TCP keepalive probe interval in seconds for Kafka connections. This describes the timeout between unacknowledged TCP keepalives. Refers to the TCP_KEEPINTVL socket option. When changed, applies to new connections only.} {Timestamp:2025-03-31 11:49:49.406356716 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_tcp_keepalive_probes:3	- TCP keepalive unacknowledged probes until the connection is considered dead for Kafka connections. Refers to the TCP_KEEPCNT socket option. When changed, applies to new connections only.} {Timestamp:2025-03-31 11:49:49.406360082 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_tcp_keepalive_timeout:120000	- TCP keepalive idle timeout in seconds for Kafka connections. This describes the timeout between TCP keepalive probes that the remote site successfully acknowledged. Refers to the TCP_KEEPIDLE socket option. When changed, applies to new connections only.} {Timestamp:2025-03-31 11:49:49.40636405 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_throughput_control:{}	- List of throughput control groups that define exclusions from node-wide throughput limits. Clients excluded from node-wide throughput limits are still potentially subject to client-specific throughput limits. For more information see https://docs.redpanda.com/current/reference/properties/cluster-properties/#kafka_throughput_control.} {Timestamp:2025-03-31 11:49:49.406366394 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_throughput_controlled_api_keys:{produce, fetch}	- List of Kafka API keys that are subject to cluster-wide and node-wide throughput limit control.} {Timestamp:2025-03-31 11:49:49.40636964 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_throughput_limit_node_in_bps:{nullopt}	- The maximum rate of all ingress Kafka API traffic for a node. Includes all Kafka API traffic (requests, responses, headers, fetched data, produced data, etc.). If `null`, the property is disabled, and traffic is not limited.} {Timestamp:2025-03-31 11:49:49.406372816 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_throughput_limit_node_out_bps:{nullopt}	- The maximum rate of all egress Kafka traffic for a node. Includes all Kafka API traffic (requests, responses, headers, fetched data, produced data, etc.). If `null`, the property is disabled, and traffic is not limited.} {Timestamp:2025-03-31 11:49:49.406378256 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_throughput_replenish_threshold:{nullopt}	- Threshold for refilling the token bucket as part of enforcing throughput limits. This only applies when kafka_throughput_throttling_v2 is `true`. This threshold is evaluated with each request for data. When the number of tokens to replenish exceeds this threshold, then tokens are added to the token bucket. This ensures that the atomic is not being updated for the token count with each request. The range for this threshold is automatically clamped to the corresponding throughput limit for ingress and egress.} {Timestamp:2025-03-31 11:49:49.406383586 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_throughput_throttling_v2:1	- Enables an updated algorithm for enforcing node throughput limits based on a shared token bucket, introduced with Redpanda v23.3.8. Set this property to `false` if you need to use the quota balancing algorithm from Redpanda v23.3.7 and older.  This property defaults to `true` for all new or upgraded Redpanda clusters. Disabling this property is not recommended. It causes your Redpanda cluster to use an outdated throughput throttling mechanism. Only set this to `false` when advised to do so by Redpanda support.} {Timestamp:2025-03-31 11:49:49.40638539 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kvstore_flush_interval:10	- Key-value store flush interval (in milliseconds).} {Timestamp:2025-03-31 11:49:49.406387163 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kvstore_max_segment_size:16777216	- Key-value maximum segment size (in bytes).} {Timestamp:2025-03-31 11:49:49.406408092 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.leader_balancer_idle_timeout:120000	- Leadership rebalancing idle timeout.} {Timestamp:2025-03-31 11:49:49.406409505 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.leader_balancer_mode:	- } {Timestamp:2025-03-31 11:49:49.406411288 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.leader_balancer_mute_timeout:300000	- Leadership rebalancing mute timeout.} {Timestamp:2025-03-31 11:49:49.406413252 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.leader_balancer_transfer_limit_per_shard:512	- Per shard limit for in-progress leadership transfers.} {Timestamp:2025-03-31 11:49:49.406416077 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.legacy_group_offset_retention_enabled:0	- Group offset retention is enabled by default starting in Redpanda version 23.1. To enable offset retention after upgrading from an older version, set this option to true.} {Timestamp:2025-03-31 11:49:49.406420225 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.legacy_permit_unsafe_log_operation:1	- Flag to enable a Redpanda cluster operator to use unsafe control characters within strings, such as consumer group names or user names. This flag applies only for Redpanda clusters that were originally on version 23.1 or earlier and have been upgraded to version 23.2 or later. Starting in version 23.2, newly-created Redpanda clusters ignore this property.} {Timestamp:2025-03-31 11:49:49.406423551 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.legacy_unsafe_log_warn
2025-03-31 11:49:50.845704364  ing_interval_sec:300000	- Period at which to log a warning about using unsafe strings containing control characters. If unsafe strings are permitted by `legacy_permit_unsafe_log_operation`, a warning will be logged at an interval specified by this property.} {Timestamp:2025-03-31 11:49:49.406425976 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.log_cleanup_policy:delete	- Default cleanup policy for topic logs. The topic property `cleanup.policy` overrides the value of `log_cleanup_policy` at the topic level.} {Timestamp:2025-03-31 11:49:49.406427759 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.log_compaction_interval_ms:10000	- How often to trigger background compaction.} {Timestamp:2025-03-31 11:49:49.406429402 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.log_compaction_use_sliding_window:1	- Use sliding window compaction.} {Timestamp:2025-03-31 11:49:49.406431847 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.log_compression_type:producer	- Default topic compression type. The topic property `compression.type` overrides the value of `log_compression_type` at the topic level.} {Timestamp:2025-03-31 11:49:49.406434342 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.log_disable_housekeeping_for_tests:0	- Disables the housekeeping loop for local storage. This property is used to simplify testing, and should not be set in production.} {Timestamp:2025-03-31 11:49:49.406437247 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.log_message_timestamp_alert_after_ms:7200000	- Threshold in milliseconds for alerting on messages with a timestamp after the broker's time, meaning the messages are in the future relative to the broker's clock.} {Timestamp:2025-03-31 11:49:49.406440303 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.log_message_timestamp_alert_before_ms:{nullopt}	- Threshold in milliseconds for alerting on messages with a timestamp before the broker's time, meaning the messages are in the past relative to the broker's clock. To disable this check, set to `null`.} {Timestamp:2025-03-31 11:49:49.406443198 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.log_message_timestamp_type:CreateTime	- Default timestamp type for topic messages (CreateTime or LogAppendTime). The topic property `message.timestamp.type` overrides the value of `log_message_timestamp_type` at the topic level.} {Timestamp:2025-03-31 11:49:49.406446104 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.log_retention_ms:604800000	- The amount of time to keep a log file before deleting it (in milliseconds). If set to `-1`, no time limit is applied. This is a cluster-wide default when a topic does not set or disable `retention.ms`.} {Timestamp:2025-03-31 11:49:49.406450101 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.log_segment_ms:{1209600000}	- Default lifetime of log segments. If `null`, the property is disabled, and no default lifetime is set. Any value under 60 seconds (60000 ms) is rejected. This property can also be set in the Kafka API using the Kafka-compatible alias, `log.roll.ms`. The topic property `segment.ms` overrides the value of `log_segment_ms` at the topic level.} {Timestamp:2025-03-31 11:49:49.406452085 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.log_segment_ms_max:31536000000	- Upper bound on topic `segment.ms`: higher values will be clamped to this value.} {Timestamp:2025-03-31 11:49:49.406454069 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.log_segment_ms_min:600000	- Lower bound on topic `segment.ms`: lower values will be clamped to this value.} {Timestamp:2025-03-31 11:49:49.406456032 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.log_segment_size:134217728	- Default log segment size in bytes for topics which do not set segment.bytes} {Timestamp:2025-03-31 11:49:49.406457936 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.log_segment_size_jitter_percent:5	- Random variation to the segment size limit used for each partition.} {Timestamp:2025-03-31 11:49:49.40645993 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.log_segment_size_max:{nullopt}	- Upper bound on topic `segment.bytes`: higher values will be clamped to this limit.} {Timestamp:2025-03-31 11:49:49.406461953 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.log_segment_size_min:{1048576}	- Lower bound on topic `segment.bytes`: lower values will be clamped to this limit.} {Timestamp:2025-03-31 11:49:49.406463907 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.lz4_decompress_reusable_buffers_disabled:0	- Disable reusable preallocated buffers for LZ4 decompression.} {Timestamp:2025-03-31 11:49:49.406465791 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.max_compacted_log_segment_size:5368709120	- Maximum compacted segment size after consolidation.} {Timestamp:2025-03-31 11:49:49.406469798 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.max_concurrent_producer_ids:18446744073709551615	- Maximum number of the active producers sessions. When the threshold is passed, Redpanda terminates old sessions. When an idle producer corresponding to the terminated session wakes up and produces, its message batches are rejected, and an out of order sequence error is emitted. Consumers don't affect this setting.} {Timestamp:2025-03-31 11:49:49.406471281 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.max_in_flight_pandaproxy_requests_per_shard:500	- Maximum number of in-flight HTTP requests to HTTP Proxy permitted per shard.  Any additional requests above this limit will be rejected with a 429 error.} {Timestamp:2025-03-31 11:49:49.406472633 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.max_in_flight_schema_registry_requests_per_shard:500	- Maximum number of in-flight HTTP requests to Schema Registry permitted per shard.  Any additional requests above this limit will be rejected with a 429 error.} {Timestamp:2025-03-31 11:49:49.406473545 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.max_kafka_throttle_delay_ms:30000	- Fail-safe maximum throttle delay on Kafka requests.} {Timestamp:2025-03-31 11:49:49.406475619 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.max_transactions_per_coordinator:18446744073709551615	- Specifies the maximum number of active transaction sessions per coordinator. When the threshold is passed Redpanda terminates old sessions. When an idle producer corresponding to the terminated session wakes up and produces, it leads to its batches being rejected with invalid producer epoch or invalid_producer_id_mapping error (depends on the transaction execution phase).} {Timestamp:2025-03-31 11:49:49.40647625 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.max_version:	- } {Timestamp:2025-03-31 11:49:49.406477132 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.members_backend_retry_ms:5000	- Time between members backend reconciliation loop retries.} {Timestamp:2025-03-31 11:49:49.406493442 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.
2025-03-31 11:49:50.845761391  memory_abort_on_alloc_failure:1	- If `true`, the Redpanda process will terminate immediately when an allocation cannot be satisfied due to memory exhaustion. If false, an exception is thrown.} {Timestamp:2025-03-31 11:49:49.406496498 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.memory_enable_memory_sampling:1	- When `true`, memory allocations are sampled and tracked. A sampled live set of allocations can then be retrieved from the Admin API. Additionally, Redpanda will periodically log the top-n allocation sites.} {Timestamp:2025-03-31 11:49:49.406498342 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.metadata_dissemination_interval_ms:3000	- Interval for metadata dissemination batching.} {Timestamp:2025-03-31 11:49:49.406504183 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.metadata_dissemination_retries:30	- Number of attempts to look up a topic's metadata-like shard before a request fails. This configuration controls the number of retries that request handlers perform when internal topic metadata (for topics like tx, consumer offsets, etc) is missing. These topics are usually created on demand when users try to use the cluster for the first time and it may take some time for the creation to happen and the metadata to propagate to all the brokers (particularly the broker handling the request). In the mean time Redpanda waits and retry. This configuration controls the number retries.} {Timestamp:2025-03-31 11:49:49.406506297 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.metadata_dissemination_retry_delay_ms:320	- Delay before retrying a topic lookup in a shard or other meta tables.} {Timestamp:2025-03-31 11:49:49.40650835 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.metadata_status_wait_timeout_ms:2000	- Maximum time to wait in metadata request for cluster health to be refreshed.} {Timestamp:2025-03-31 11:49:49.406510174 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.metrics_reporter_report_interval:86400000	- Cluster metrics reporter report interval.} {Timestamp:2025-03-31 11:49:49.406511897 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.metrics_reporter_tick_interval:60000	- Cluster metrics reporter tick interval.} {Timestamp:2025-03-31 11:49:49.406513721 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.metrics_reporter_url:https://m.rp.vectorized.io/v2	- URL of the cluster metrics reporter.} {Timestamp:2025-03-31 11:49:49.406514993 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.min_version:	- } {Timestamp:2025-03-31 11:49:49.406521535 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.minimum_topic_replications:1	- Minimum allowable replication factor for topics in this cluster. The set value must be positive, odd, and equal to or less than the number of available brokers. Changing this parameter only restricts newly-created topics. Redpanda returns an `INVALID_REPLICATION_FACTOR` error on any attempt to create a topic with a replication factor less than this property. If you change the `minimum_topic_replications` setting, the replication factor of existing topics remains unchanged. However, Redpanda will log a warning on start-up with a list of any topics that have fewer replicas than this minimum. For example, you might see a message such as `Topic X has a replication factor less than specified minimum: 1 < 3`.} {Timestamp:2025-03-31 11:49:49.406523829 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.node_isolation_heartbeat_timeout:3000	- How long after the last heartbeat request a node will wait before considering itself to be isolated.} {Timestamp:2025-03-31 11:49:49.406525743 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.node_management_operation_timeout_ms:5000	- Timeout for executing node management operations.} {Timestamp:2025-03-31 11:49:49.406528118 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.node_status_interval:100	- Time interval between two node status messages. Node status messages establish liveness status outside of the Raft protocol.} {Timestamp:2025-03-31 11:49:49.406530422 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.node_status_reconnect_max_backoff_ms:15000	- Maximum backoff (in milliseconds) to reconnect to an unresponsive peer during node status liveness checks.} {Timestamp:2025-03-31 11:49:49.406532516 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.oidc_clock_skew_tolerance:0	- The amount of time (in seconds) to allow for when validating the expiry claim in the token.} {Timestamp:2025-03-31 11:49:49.40653496 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.oidc_discovery_url:https://auth.prd.cloud.redpanda.com/.well-known/openid-configuration	- The URL pointing to the well-known discovery endpoint for the OIDC provider.} {Timestamp:2025-03-31 11:49:49.406537124 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.oidc_keys_refresh_interval:3600000	- The frequency of refreshing the JSON Web Keys (JWKS) used to validate access tokens.} {Timestamp:2025-03-31 11:49:49.406539008 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.oidc_principal_mapping:$.sub	- Rule for mapping JWT payload claim to a Redpanda user principal.} {Timestamp:2025-03-31 11:49:49.406540892 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.oidc_token_audience:redpanda	- A string representing the intended recipient of the token.} {Timestamp:2025-03-31 11:49:49.406542815 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.partition_autobalancing_concurrent_moves:50	- Number of partitions that can be reassigned at once.} {Timestamp:2025-03-31 11:49:49.406545841 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.partition_autobalancing_max_disk_usage_percent:80	- When the disk usage of a node exceeds this threshold, it triggers Redpanda to move partitions off of the node. This property applies only when partition_autobalancing_mode is set to `continuous`.} {Timestamp:2025-03-31 11:49:49.406548786 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.partition_autobalancing_min_size_threshold:{nullopt}	- Minimum size of partition that is going to be prioritized when rebalancing a cluster due to the disk size threshold being breached. This value is calculated automatically by default.} {Timestamp:2025-03-31 11:49:49.406554798 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.partition_autobalancing_mode:node_add	- Mode of partition balancing for a cluster. * `node_add`: partition balancing happens when a node is added. * `continuous`: partition balancing happens automatically to maintain optimal performance and availability, based on continuous monitoring for node changes (same as `node_add`) and also high disk usage. This option requires an Enterprise license, and it is customized by `partition_autobalancing_node_availability_timeout_sec` and `partition_autobalancing_max_disk_usage_percent` properties. * `off`: partition balancing is disabled. This option is not recommended for production clusters.} {Timestamp:2025-03-31 11:49:49.406557362 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.partition_autobal
2025-03-31 11:49:50.845843355  ancing_movement_batch_size_bytes:5368709120	- Total size of partitions that autobalancer is going to move in one batch (deprecated, use partition_autobalancing_concurrent_moves to limit the autobalancer concurrency)} {Timestamp:2025-03-31 11:49:49.406558935 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.partition_autobalancing_node_availability_timeout_sec:900000	- When a node is unavailable for at least this timeout duration, it triggers Redpanda to move partitions off of the node. This property applies only when `partition_autobalancing_mode` is set to `continuous`.      } {Timestamp:2025-03-31 11:49:49.406559837 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.partition_autobalancing_tick_interval_ms:30000	- Partition autobalancer tick interval.} {Timestamp:2025-03-31 11:49:49.4065615 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.partition_autobalancing_tick_moves_drop_threshold:0.2	- If the number of scheduled tick moves drops by this ratio, a new tick is scheduled immediately. Valid values are (0, 1]. For example, with a value of 0.2 and 100 scheduled moves in a tick, a new tick is scheduled when the in-progress moves are fewer than 80.} {Timestamp:2025-03-31 11:49:49.406578843 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.partition_autobalancing_topic_aware:1	- If `true`, Redpanda prioritizes balancing a topicâ€™s partition replica count evenly across all brokers while itâ€™s balancing the clusterâ€™s overall partition count. Because different topics in a cluster can have vastly different load profiles, this better distributes the workload of the most heavily-used topics evenly across brokers.} {Timestamp:2025-03-31 11:49:49.406582119 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.partition_manager_shutdown_watchdog_timeout:30000	- A threshold value to detect partitions which might have been stuck while shutting down. After this threshold, a watchdog in partition manager will log information about partition shutdown not making progress.} {Timestamp:2025-03-31 11:49:49.406585185 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.pp_sr_smp_max_non_local_requests:{nullopt}	- Maximum number of Cross-core(Inter-shard communication) requests pending in HTTP Proxy and Schema Registry seastar::smp group. (For more details, see the `seastar::smp_service_group` documentation).} {Timestamp:2025-03-31 11:49:49.406586898 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.quota_manager_gc_sec:30000	- Quota manager GC frequency in milliseconds.} {Timestamp:2025-03-31 11:49:49.406590555 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.raft_enable_longest_log_detection:1	- Enables an additional step in leader election where a candidate is allowed to wait for all the replies from the broker it requested votes from. This may introduce a small delay when recovering from failure, but it prevents truncation if any of the replicas have more data than the majority.} {Timestamp:2025-03-31 11:49:49.406592258 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.raft_enable_lw_heartbeat:1	- Enables Raft optimization of heartbeats.} {Timestamp:2025-03-31 11:49:49.406594833 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.raft_flush_timer_interval_ms:100	- Interval of checking partition against the `raft_replica_max_pending_flush_bytes`, deprecated started 24.1, use raft_replica_max_flush_delay_ms instead } {Timestamp:2025-03-31 11:49:49.406597377 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.raft_heartbeat_disconnect_failures:3	- The number of failed heartbeats after which an unresponsive TCP connection is forcibly closed. To disable forced disconnection, set to 0.} {Timestamp:2025-03-31 11:49:49.406599181 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.raft_heartbeat_interval_ms:150	- Number of milliseconds for Raft leader heartbeats.} {Timestamp:2025-03-31 11:49:49.40660386 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.raft_heartbeat_timeout_ms:3000	- Raft heartbeat RPC (remote procedure call) timeout. Raft uses a heartbeat mechanism to maintain leadership authority and to trigger leader elections. The `raft_heartbeat_interval_ms` is a periodic heartbeat sent by the partition leader to all followers to declare its leadership. If a follower does not receive a heartbeat within the `raft_heartbeat_timeout_ms`, then it triggers an election to choose a new partition leader.} {Timestamp:2025-03-31 11:49:49.406605453 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.raft_io_timeout_ms:10000	- Raft I/O timeout.} {Timestamp:2025-03-31 11:49:49.406610983 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.raft_learner_recovery_rate:104857600	- Raft learner recovery rate limit. Throttles the rate of data communicated to nodes (learners) that need to catch up to leaders. This rate limit is placed on a node sending data to a recovering node. Each sending node is limited to this rate. The recovering node accepts data as fast as possible according to the combined limits of all healthy nodes in the cluster. For example, if two nodes are sending data to the recovering node, and `raft_learner_recovery_rate` is 100 MB/sec, then the recovering node will recover at a rate of 200 MB/sec.} {Timestamp:2025-03-31 11:49:49.406613227 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.raft_max_concurrent_append_requests_per_follower:16	- Maximum number of concurrent append entry requests sent by the leader to one follower.} {Timestamp:2025-03-31 11:49:49.406615522 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.raft_max_recovery_memory:{nullopt}	- Maximum memory that can be used for reads in Raft recovery process by default 15% of total memory.} {Timestamp:2025-03-31 11:49:49.406618327 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.raft_recovery_concurrency_per_shard:64	- Number of partitions that may simultaneously recover data to a particular shard. This number is limited to avoid overwhelming nodes when they come back online after an outage.} {Timestamp:2025-03-31 11:49:49.406620371 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.raft_recovery_default_read_size:524288	- Specifies the default size of a read issued during Raft follower recovery.} {Timestamp:2025-03-31 11:49:49.406622765 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.raft_recovery_throttle_disable_dynamic_mode:0	- Disables cross shard sharing used to throttle recovery traffic. Should only be used to debug unexpected problems.} {Timestamp:2025-03-31 11:49:49.406624989 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.raft_replica_max_flush_delay_ms:100	- Maximum delay between two subsequent flushes. After this delay, the log is automatically force flushed.} {Timestamp:2025-03-31 11:49:49.406627835 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.raft_replica_max_pending_flush_bytes:{262144}	- Maximum number of bytes that are not flushed per partition. If the configured threshold is reached, the log is automatically flushed even if it has not been explicitly requested.} {Timestamp:2025-03-31 11:49:49.406629718 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - applicatio
2025-03-31 11:49:50.845885244  n.cc:849 - redpanda.raft_replicate_batch_window_size:1048576	- Maximum size of requests cached for replication.} {Timestamp:2025-03-31 11:49:49.406632533 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.raft_smp_max_non_local_requests:{nullopt}	- Maximum number of Cross-core(Inter-shard communication) requests pending in Raft seastar::smp group. For details, refer to the `seastar::smp_service_group` documentation).} {Timestamp:2025-03-31 11:49:49.406634878 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.raft_timeout_now_timeout_ms:1000	- Timeout for Raft's timeout_now RPC. This RPC is used to force a follower to dispatch a round of votes immediately.} {Timestamp:2025-03-31 11:49:49.406636952 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.raft_transfer_leader_recovery_timeout_ms:10000	- Follower recovery timeout waiting period when transferring leadership.} {Timestamp:2025-03-31 11:49:49.406638655 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.readers_cache_eviction_timeout_ms:30000	- Duration after which inactive readers are evicted from cache.} {Timestamp:2025-03-31 11:49:49.406640007 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.readers_cache_target_max_size:200	- Maximum desired number of readers cached per NTP. This a soft limit, meaning that a number of readers in cache may temporarily increase as cleanup is performed in the background.} {Timestamp:2025-03-31 11:49:49.406641029 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.reclaim_batch_cache_min_free:67108864	- Minimum amount of free memory maintained by the batch cache background reclaimer.} {Timestamp:2025-03-31 11:49:49.406642532 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.reclaim_growth_window:3000	- Starting from the last point in time when memory was reclaimed from the batch cache, this is the duration during which the amount of memory to reclaim grows at a significant rate, based on heuristics about the amount of available memory.} {Timestamp:2025-03-31 11:49:49.406643314 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.reclaim_max_size:4194304	- Maximum batch cache reclaim size.} {Timestamp:2025-03-31 11:49:49.406644095 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.reclaim_min_size:131072	- Minimum batch cache reclaim size.} {Timestamp:2025-03-31 11:49:49.406659865 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.reclaim_stable_window:10000	- If the duration since the last time memory was reclaimed is longer than the amount of time specified in this property, the memory usage of the batch cache is considered stable, so only the minimum size (`reclaim_min_size`) is set to be reclaimed.} {Timestamp:2025-03-31 11:49:49.406661979 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.recovery_append_timeout_ms:5000	- Timeout for append entry requests issued while updating a stale follower.} {Timestamp:2025-03-31 11:49:49.406664163 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.release_cache_on_segment_roll:0	- Flag for specifying whether or not to release cache when a full segment is rolled.} {Timestamp:2025-03-31 11:49:49.406666126 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.replicate_append_timeout_ms:3000	- Timeout for append entry requests issued while replicating entries.} {Timestamp:2025-03-31 11:49:49.406669443 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.retention_bytes:{nullopt}	- Default maximum number of bytes per partition on disk before triggering deletion of the oldest messages. If `null` (the default value), no limit is applied. The topic property `retention.bytes` overrides the value of `retention_bytes` at the topic level.} {Timestamp:2025-03-31 11:49:49.406672709 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.retention_local_strict:0	- Flag to allow Tiered Storage topics to expand to consumable retention policy limits. When this flag is enabled, non-local retention settings are used, and local retention settings are used to inform data removal policies in low-disk space scenarios.} {Timestamp:2025-03-31 11:49:49.406676045 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.retention_local_strict_override:1	- Trim log data when a cloud topic reaches its local retention limit. When this option is disabled Redpanda will allow partitions to grow past the local retention limit, and will be trimmed automatically as storage reaches the configured target size.} {Timestamp:2025-03-31 11:49:49.406680654 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.retention_local_target_bytes_default:{nullopt}	- Local retention size target for partitions of topics with object storage write enabled. If `null`, the property is disabled. This property can be overridden on a per-topic basis by setting `retention.local.target.bytes` in each topic enabled for Tiered Storage. Both `retention_local_target_bytes_default` and `retention_local_target_ms_default` can be set. The limit that is reached earlier is applied.} {Timestamp:2025-03-31 11:49:49.406685353 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.retention_local_target_capacity_bytes:{nullopt}	- The target capacity (in bytes) that log storage will try to use before additional retention rules take over to trim data to meet the target. When no target is specified, storage usage is unbounded. Redpanda Data recommends setting only one of `retention_local_target_capacity_bytes` or `retention_local_target_capacity_percent`. If both are set, the minimum of the two is used as the effective target capacity.} {Timestamp:2025-03-31 11:49:49.406690472 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.retention_local_target_capacity_percent:{80}	- The target capacity in percent of unreserved space (`disk_reservation_percent`) that log storage will try to use before additional retention rules will take over to trim data in order to meet the target. When no target is specified storage usage is unbounded. Redpanda Data recommends setting only one of `retention_local_target_capacity_bytes` or `retention_local_target_capacity_percent`. If both are set, the minimum of the two is used as the effective target capacity.} {Timestamp:2025-03-31 11:49:49.4066945 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.retention_local_target_ms_default:86400000	- Local retention time target for partitions of topics with object storage write enabled. This property can be overridden on a per-topic basis by setting `retention.local.target.ms` in each topic enabled for Tiered Storage. Both `retention_local_target_bytes_default` and `retention_local_target_ms_default` can be set. The limit that is reached first is applied.} {Timestamp:2025-03-31 11:49:49.406696804 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.retention_local_trim_interval:30000	- The period during which disk usage is checked for disk pressure, and data is optionally trimmed to meet the target.} {Timestamp:2025-03-31 11:49:49.406699719 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.retention_local_trim_overage_coeff:2	- The space management control loop reclaims the overage multiplied by this this coefficient to compensate for data that is written during 
2025-03-31 11:49:50.845942411  the idle period between control loop invocations.} {Timestamp:2025-03-31 11:49:49.406702665 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.rm_sync_timeout_ms:10000	- Resource manager's synchronization timeout. Specifies the maximum time for this node to wait for the internal state machine to catch up with all events written by previous leaders before rejecting a request.} {Timestamp:2025-03-31 11:49:49.406704098 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.rm_violation_recovery_policy:	- } {Timestamp:2025-03-31 11:49:49.406706142 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.rpc_client_connections_per_peer:128	- The maximum number of connections a broker will open to each of its peers.} {Timestamp:2025-03-31 11:49:49.406708125 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.rpc_server_compress_replies:0	- Enable compression for internal RPC (remote procedure call) server replies.} {Timestamp:2025-03-31 11:49:49.40671058 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.rpc_server_listen_backlog:{nullopt}	- Maximum TCP connection queue length for Kafka server and internal RPC server. If `null` (the default value), no queue length is set.} {Timestamp:2025-03-31 11:49:49.406712844 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.rpc_server_tcp_recv_buf:{nullopt}	- Internal RPC TCP receive buffer size. If `null` (the default value), no buffer size is set by Redpanda.} {Timestamp:2025-03-31 11:49:49.406715128 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.rpc_server_tcp_send_buf:{nullopt}	- Internal RPC TCP send buffer size. If `null` (the default value), then no buffer size is set by Redpanda.} {Timestamp:2025-03-31 11:49:49.406716681 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.rpk_path:"/usr/bin/rpk"	- Path to RPK binary} {Timestamp:2025-03-31 11:49:49.406718585 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.rps_limit_acls_and_users_operations:1000	- Rate limit for controller ACLs and user's operations.} {Timestamp:2025-03-31 11:49:49.406719917 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.rps_limit_configuration_operations:1000	- Rate limit for controller configuration operations.} {Timestamp:2025-03-31 11:49:49.406720789 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.rps_limit_move_operations:1000	- Rate limit for controller move operations.} {Timestamp:2025-03-31 11:49:49.406721721 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.rps_limit_node_management_operations:1000	- Rate limit for controller node management operations.} {Timestamp:2025-03-31 11:49:49.406722562 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.rps_limit_topic_operations:1000	- Rate limit for controller topic operations.} {Timestamp:2025-03-31 11:49:49.406723464 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.sasl_kerberos_config:/etc/krb5.conf	- The location of the Kerberos `krb5.conf` file for Redpanda.} {Timestamp:2025-03-31 11:49:49.406724426 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.sasl_kerberos_keytab:/var/lib/redpanda/redpanda.keytab	- The location of the Kerberos keytab file for Redpanda.} {Timestamp:2025-03-31 11:49:49.406739634 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.sasl_kerberos_principal:redpanda	- The primary of the Kerberos Service Principal Name (SPN) for Redpanda.} {Timestamp:2025-03-31 11:49:49.406741708 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.sasl_kerberos_principal_mapping:{DEFAULT}	- Rules for mapping Kerberos principal names to Redpanda user principals.} {Timestamp:2025-03-31 11:49:49.406743832 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.sasl_mechanisms:{SCRAM}	- A list of supported SASL mechanisms. Accepted values: `SCRAM`, `GSSAPI`, `OAUTHBEARER`.} {Timestamp:2025-03-31 11:49:49.406745786 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.schema_registry_normalize_on_startup:0	- Normalize schemas as they are read from the topic on startup.} {Timestamp:2025-03-31 11:49:49.40674779 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.schema_registry_protobuf_renderer_v2:0	- Enables experimental protobuf renderer to support normalize=true.} {Timestamp:2025-03-31 11:49:49.406749252 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.seed_server_meta_topic_partitions:	- } {Timestamp:2025-03-31 11:49:49.406751096 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.segment_appender_flush_timeout_ms:1000	- Maximum delay until buffered data is written.} {Timestamp:2025-03-31 11:49:49.406752819 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.segment_fallocation_step:33554432	- Size for segments fallocation.} {Timestamp:2025-03-31 11:49:49.406754162 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.seq_table_min_size:	- } {Timestamp:2025-03-31 11:49:49.406756787 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.space_management_enable:1	- Option to explicitly disable automatic disk space management. If this property was explicitly disabled while using v23.2, it will remain disabled following an upgrade.} {Timestamp:2025-03-31 11:49:49.406759031 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.space_management_enable_override:0	- Enable automatic space management. This option is ignored and deprecated in versions >= v23.3.} {Timestamp:2025-03-31 11:49:49.406761014 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.space_management_max_log_concurrency:20	- Maximum parallel logs inspected during space management process.} {Timestamp:2025-03-31 11:49:49.406768388 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.space_management_max_segment_concurrency:10	- Maximum parallel segments inspected during space management process.} {Timestamp:2025-03-31 11:49:49.406770643 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.storage_compaction_index_memory:134217728	- Maximum number of bytes that may be used on each shard by compaction index writers.} {Timestamp:2025-03-31 11:49:49.406773388 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.storage_compaction_key_map_memory:134217728	- Maximum number of bytes that may be used on each shard by compaction key-offset maps. Only applies when `log_compaction_use_sliding_window` is set to `true`.} {Timestamp:2025-03-31 11:49:49.406777375 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.storage_compaction_key_map_memory_limit_percent:12	- Limit on `storage_compaction_key_map_memory`, expressed as a percentage of memory per shard, that bounds the amount of memory used by compaction key-offset maps. Memory per shard is computed after `data_transforms_per_core_memory_reservation`, and only applies when `log_compaction_use_sliding_window` is set to `true`.} {Timestamp:2025-03-31 11:49:49.406779499 +0000 UTC Con
2025-03-31 11:49:50.845983528  tent:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.storage_ignore_cstore_hints:0	- When set, cstore hints are ignored and not used for data access (but are otherwise generated).} {Timestamp:2025-03-31 11:49:49.406793105 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.storage_ignore_timestamps_in_future_sec:{nullopt}	- The maximum number of seconds that a record's timestamp can be ahead of a Redpanda broker's clock and still be used when deciding whether to clean up the record for data retention. This property makes possible the timely cleanup of records from clients with clocks that are drastically unsynchronized relative to Redpanda. When determining whether to clean up a record with timestamp more than `storage_ignore_timestamps_in_future_sec` seconds ahead of the broker, Redpanda ignores the record's timestamp and instead uses a valid timestamp of another record in the same segment, or (if another record's valid timestamp is unavailable) the timestamp of when the segment file was last modified (mtime). By default, `storage_ignore_timestamps_in_future_sec` is disabled (null). To figure out whether to set `storage_ignore_timestamps_in_future_sec` for your system: . Look for logs with segments that are unexpectedly large and not being cleaned up. . In the logs, search for records with unsynchronized timestamps that are further into the future than tolerable by your data retention and storage settings. For example, timestamps 60 seconds or more into the future can be considered to be too unsynchronized. . If you find unsynchronized timestamps throughout your logs, determine the number of seconds that the timestamps are ahead of their actual time, and set `storage_ignore_timestamps_in_future_sec` to that value so data retention can proceed. . If you only find unsynchronized timestamps that are the result of transient behavior, you can disable `storage_ignore_timestamps_in_future_sec`.} {Timestamp:2025-03-31 11:49:49.406795519 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.storage_max_concurrent_replay:1024	- Maximum number of partitions' logs that will be replayed concurrently at startup, or flushed concurrently on shutdown.} {Timestamp:2025-03-31 11:49:49.406797493 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.storage_min_free_bytes:5368709120	- Threshold of minimum bytes free space before rejecting producers.} {Timestamp:2025-03-31 11:49:49.406799417 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.storage_read_buffer_size:131072	- Size of each read buffer (one per in-flight read, per log segment).} {Timestamp:2025-03-31 11:49:49.40680132 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.storage_read_readahead_count:10	- How many additional reads to issue ahead of current read location.} {Timestamp:2025-03-31 11:49:49.406804987 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.storage_reserve_min_segments:2	- The number of segments per partition that the system will attempt to reserve disk capacity for. For example, if the maximum segment size is configured to be 100 MB, and the value of this option is 2, then in a system with 10 partitions Redpanda will attempt to reserve at least 2 GB of disk space.} {Timestamp:2025-03-31 11:49:49.406807071 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.storage_space_alert_free_threshold_bytes:0	- Threshold of minimum bytes free space before setting storage space alert.} {Timestamp:2025-03-31 11:49:49.406808984 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.storage_space_alert_free_threshold_percent:5	- Threshold of minimum percent free space before setting storage space alert.} {Timestamp:2025-03-31 11:49:49.406811499 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.storage_strict_data_init:0	- Requires that an empty file named `.redpanda_data_dir` be present in the broker configuration `data_directory`. If set to `true`, Redpanda will refuse to start if the file is not found in the data directory.} {Timestamp:2025-03-31 11:49:49.406813844 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.storage_target_replay_bytes:10737418240	- Target bytes to replay from disk on startup after clean shutdown: controls frequency of snapshots and checkpoints.} {Timestamp:2025-03-31 11:49:49.406815397 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.superusers:{}	- List of superuser usernames.} {Timestamp:2025-03-31 11:49:49.406837107 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.target_fetch_quota_byte_rate:{nullopt}	- Target fetch size quota byte rate (bytes per second) - disabled default} {Timestamp:2025-03-31 11:49:49.406838961 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.target_quota_byte_rate:0	- Target request size quota byte rate (bytes per second)} {Timestamp:2025-03-31 11:49:49.406841926 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.tls_enable_renegotiation:0	- TLS client-initiated renegotiation is considered unsafe and is by default disabled.  Only re-enable it if you are experiencing issues with your TLS-enabled client.  This option has no effect on TLSv1.3 connections as client-initiated renegotiation was removed.} {Timestamp:2025-03-31 11:49:49.406844551 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.tls_min_version:v1.2	- The minimum TLS version that Redpanda clusters support. This property prevents client applications from negotiating a downgrade to the TLS version when they make a connection to a Redpanda cluster.} {Timestamp:2025-03-31 11:49:49.406846585 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.tm_sync_timeout_ms:10000	- Transaction manager's synchronization timeout. Maximum time to wait for internal state machine to catch up before rejecting a request.} {Timestamp:2025-03-31 11:49:49.406847867 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.tm_violation_recovery_policy:	- } {Timestamp:2025-03-31 11:49:49.406850683 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.tombstone_retention_ms:{nullopt}	- The retention time for tombstone records in a compacted topic. Cannot be enabled at the same time as any of `cloud_storage_enabled`, `cloud_storage_enable_remote_read`, or `cloud_storage_enable_remote_write`.} {Timestamp:2025-03-31 11:49:49.406852336 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.topic_fds_per_partition:{5}	- Required file handles per partition when creating topics.} {Timestamp:2025-03-31 11:49:49.406853969 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.topic_memory_per_partition:{4194304}	- Required memory per partition when creating topics.} {Timestamp:2025-03-31 11:49:49.406855762 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.topic_partitions_per_shard:1000	- Maximum number of partitions which may be allocated to one shard (CPU core).} {Timestamp:2025-03-31 11:49:49.406858016 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.topic_partitions_reserve_shard0:0	- Reserved partition slots on shard (CPU core) 0 on each node.  If this is >= topic_partitions_per_shard, no data partitions will be scheduled on shard 0} {Timestamp:2025-03-31 11:49:49.406860481 +0000 UTC Content:INFO  2025-03-31 11:49:49,40
2025-03-31 11:49:50.846024465  6 [shard  0:main] main - application.cc:849 - redpanda.transaction_coordinator_cleanup_policy:delete	- Cleanup policy for a transaction coordinator topic. Accepted Values: `compact`, `delete`, `["compact","delete"]`, `none`} {Timestamp:2025-03-31 11:49:49.406863517 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.transaction_coordinator_delete_retention_ms:604800000	- Delete segments older than this age. To ensure transaction state is retained as long as the longest-running transaction, make sure this is no less than `transactional_id_expiration_ms`.} {Timestamp:2025-03-31 11:49:49.40686545 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.transaction_coordinator_log_segment_size:1073741824	- The size (in bytes) each log segment should be.} {Timestamp:2025-03-31 11:49:49.406867314 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.transaction_coordinator_partitions:50	- Number of partitions for transactions coordinator.} {Timestamp:2025-03-31 11:49:49.406868757 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.transaction_coordinator_replication:	- } {Timestamp:2025-03-31 11:49:49.406872233 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.transaction_max_timeout_ms:900000	- The maximum allowed timeout for transactions. If a client-requested transaction timeout exceeds this configuration, the broker returns an error during transactional producer initialization. This guardrail prevents hanging transactions from blocking consumer progress.} {Timestamp:2025-03-31 11:49:49.406874598 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.transactional_id_expiration_ms:604800000	- Expiration time of producer IDs. Measured starting from the time of the last write until now for a given ID.} {Timestamp:2025-03-31 11:49:49.406876631 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.tx_log_stats_interval_s:10000	- How often to log per partition tx stats, works only with debug logging enabled.} {Timestamp:2025-03-31 11:49:49.406878064 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.tx_registry_log_capacity:	- } {Timestamp:2025-03-31 11:49:49.406879467 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.tx_registry_sync_timeout_ms:	- } {Timestamp:2025-03-31 11:49:49.40688135 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.tx_timeout_delay_ms:1000	- Delay before scheduling the next check for timed out transactions.} {Timestamp:2025-03-31 11:49:49.406884186 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.unsafe_enable_consumer_offsets_delete_retention:0	- Enables delete retention of consumer offsets topic. This is an internal-only configuration and should be enabled only after consulting with Redpanda support.} {Timestamp:2025-03-31 11:49:49.406886159 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.usage_disk_persistance_interval_sec:300000	- The interval in which all usage stats are written to disk.} {Timestamp:2025-03-31 11:49:49.406887923 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.usage_num_windows:24	- The number of windows to persist in memory and disk.} {Timestamp:2025-03-31 11:49:49.406890117 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.usage_window_width_interval_sec:3600000	- The width of a usage window, tracking cloud and kafka ingress/egress traffic each interval.} {Timestamp:2025-03-31 11:49:49.40689194 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.use_fetch_scheduler_group:1	- Use a separate scheduler group for fetch processing.} {Timestamp:2025-03-31 11:49:49.406893263 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.use_scheduling_groups:	- } {Timestamp:2025-03-31 11:49:49.406895146 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.virtual_cluster_min_producer_ids:18446744073709551615	- Minimum number of active producers per virtual cluster.} {Timestamp:2025-03-31 11:49:49.40689697 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.wait_for_leader_timeout_ms:5000	- Timeout to wait for leadership in metadata cache.} {Timestamp:2025-03-31 11:49:49.406903742 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.write_caching_default:false	- The default write caching mode to apply to user topics. Write caching acknowledges a message as soon as it is received and acknowledged on a majority of brokers, without waiting for it to be written to disk. With `acks=all`, this provides lower latency while still ensuring that a majority of brokers acknowledge the write. Fsyncs follow `raft_replica_max_pending_flush_bytes` and `raft_replica_max_flush_delay_ms`, whichever is reached first. The `write_caching_default` cluster property can be overridden with the `write.caching` topic property. Accepted values: * `true` * `false` * `disabled`: This takes precedence over topic overrides and disables write caching for the entire cluster.} {Timestamp:2025-03-31 11:49:49.406905546 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.zstd_decompress_workspace_bytes:8388608	- Size of the zstd decompression workspace.} {Timestamp:2025-03-31 11:49:49.406906848 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:900 - Node configuration properties:} {Timestamp:2025-03-31 11:49:49.406908381 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:901 - (use `rpk redpanda config set <cfg> <value>` to change)} {Timestamp:2025-03-31 11:49:49.406932677 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.admin:{{:{host: 0.0.0.0, port: 9644}}}	- Network address for the Admin API[] server.} {Timestamp:2025-03-31 11:49:49.40693477 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.admin_api_doc_dir:/usr/share/redpanda/admin-api-doc	- Path to the API specifications for the Admin API.} {Timestamp:2025-03-31 11:49:49.406936434 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.admin_api_tls:{}	- Specifies the TLS configuration for the HTTP Admin API.} {Timestamp:2025-03-31 11:49:49.406938357 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.advertised_kafka_api:{{:{host: 127.0.0.1, port: 9092}}}	- Address of Kafka API published to the clients} {Timestamp:2025-03-31 11:49:49.406940401 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.advertised_rpc_api:{{host: 127.0.0.1, port: 33145}}	- Address of RPC endpoint published to other cluster members} {Timestamp:2025-03-31 11:49:49.406942585 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_cache_directory:{nullopt}	- Directory for archival cache. Should be present when `cloud_storage_enabled` is present} {Timestamp:2025-03-31 11:49:49.406944699 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_hash_path_directory:{nullopt}	- Directory to store inventory report hashes for use by cloud storage scrubber} {Timestamp:2025-03-31 11:49:49.406945931 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.coproc_supervisor_server:	- } {Timest
2025-03-31 11:49:50.846082033  amp:2025-03-31 11:49:49.406949238 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.crash_loop_limit:{5}	- A limit on the number of consecutive times a broker can crash within one hour before its crash-tracking logic is reset. This limit prevents a broker from getting stuck in an infinite cycle of crashes. For more information see https://docs.redpanda.com/current/reference/properties/broker-properties/#crash_loop_limit.} {Timestamp:2025-03-31 11:49:49.406952263 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.crash_loop_sleep_sec:{nullopt}	- The amount of time the broker sleeps before terminating the process when it reaches the number of consecutive times a broker can crash. For more information, see https://docs.redpanda.com/current/reference/properties/broker-properties/#crash_loop_limit.} {Timestamp:2025-03-31 11:49:49.406953415 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.dashboard_dir:	- } {Timestamp:2025-03-31 11:49:49.406955239 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.data_directory:{data_directory="/data/redpanda"}	- Path to the directory for storing Redpanda's streaming data files.} {Timestamp:2025-03-31 11:49:49.406956912 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.developer_mode:1	- Skips most of the checks performed at startup, not recomended for production use} {Timestamp:2025-03-31 11:49:49.406959347 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.emergency_disable_data_transforms:0	- Override the cluster property `data_transforms_enabled` and disable Wasm-powered data transforms. This is an emergency shutoff button.} {Timestamp:2025-03-31 11:49:49.406963054 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.empty_seed_starts_cluster:1	- Controls how a new cluster is formed. All brokers in a cluster must have the same value. See how the `empty_seed_starts_cluster` setting works with the `seed_servers` setting to form a cluster. For backward compatibility, `true` is the default. Redpanda recommends using `false` in production environments to prevent accidental cluster formation.} {Timestamp:2025-03-31 11:49:49.406964296 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.enable_central_config:	- } {Timestamp:2025-03-31 11:49:49.406970928 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.fips_mode:disabled	- Controls whether Redpanda starts in FIPS mode.  This property allows for three values: `disabled` - Redpanda does not start in FIPS mode. `permissive` - Redpanda performs the same check as enabled, but a warning is logged, and Redpanda continues to run. Redpanda loads the OpenSSL FIPS provider into the OpenSSL library. After this completes, Redpanda is operating in FIPS mode, which means that the TLS cipher suites available to users are limited to the TLSv1.2 and TLSv1.3 NIST-approved cryptographic methods. `enabled` - Redpanda verifies that the operating system is enabled for FIPS by checking `/proc/sys/crypto/fips_enabled`. If the file does not exist or does not return `1`, Redpanda immediately exits.} {Timestamp:2025-03-31 11:49:49.406973092 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_api:{{:{host: 0.0.0.0, port: 9092}:{nullopt}}}	- IP address and port of the Kafka API endpoint that handles requests.} {Timestamp:2025-03-31 11:49:49.406974826 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.kafka_api_tls:{}	- Transport Layer Security (TLS) configuration for the Kafka API endpoint.} {Timestamp:2025-03-31 11:49:49.40697699 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.memory_allocation_warning_threshold:{131073}	- Threshold for log messages that contain a larger memory allocation than specified.} {Timestamp:2025-03-31 11:49:49.406980266 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.node_id:{nullopt}	- A number that uniquely identifies the broker within the cluster. If `null` (the default value), Redpanda automatically assigns an ID. If set, it must be non-negative value. The `node_id` property must not be changed after a broker joins the cluster.} {Timestamp:2025-03-31 11:49:49.406983292 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.node_id_overrides:{}	- List of node ID and UUID overrides to be applied at broker startup. Each entry includes the current UUID and desired ID and UUID. Each entry applies to a given node if and only if 'current' matches that node's current UUID.} {Timestamp:2025-03-31 11:49:49.406985245 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.openssl_config_file:{nullopt}	- Path to the configuration file used by OpenSSL to properly load the FIPS-compliant module.} {Timestamp:2025-03-31 11:49:49.406987479 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.openssl_module_directory:{nullopt}	- Path to the directory that contains the OpenSSL FIPS-compliant module. The filename that Redpanda looks for is `fips.so`.} {Timestamp:2025-03-31 11:49:49.406990585 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.rack:{nullopt}	- A label that identifies a failure zone. Apply the same label to all brokers in the same failure zone. When `enable_rack_awareness` is set to `true` at the cluster level, the system uses the rack labels to spread partition replicas across different failure zones.} {Timestamp:2025-03-31 11:49:49.406992719 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.recovery_mode_enabled:0	- If `true`, start Redpanda in recovery mode, where user partitions are not loaded and only administrative operations are allowed.} {Timestamp:2025-03-31 11:49:49.407009541 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.rpc_server:{host: 0.0.0.0, port: 33145}	- IP address and port for the Remote Procedure Call (RPC) server.} {Timestamp:2025-03-31 11:49:49.407011945 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.rpc_server_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} crl file: {nullopt} client_auth_required: 0 }	- TLS configuration for the RPC server.} {Timestamp:2025-03-31 11:49:49.407023206 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.seed_servers:{}	- List of the seed servers used to join current cluster. If the `seed_servers` list is empty the node will be a cluster root and it will form a new cluster. When `empty_seed_starts_cluster` is `true`, Redpanda enables one broker with an empty `seed_servers` list to initiate a new cluster. The broker with an empty `seed_servers` becomes the cluster root, to which other brokers must connect to join the cluster.  Brokers looking to join the cluster should have their `seed_servers` populated with the cluster root's address, facilitating their connection to the cluster. Only one broker, the designated cluster root, should have an empty `seed_servers` list during the initial cluster bootstrapping. This ensures a single initiation point for cluster formation. When `empty_seed_starts_cluster` is `false`, Redpanda requires all brokers to start with a known set of brokers listed in `seed_servers`. The `seed_servers` list must not be empty and should be identical across these initial seed brokers, containing the addresses of all seed brokers. Brokers not included in the `seed_servers` list use it to discover and join the cluster, allowing for expansion bey
2025-03-31 11:49:50.846123260  ond the foundational members. The `seed_servers` list must be consistent across all seed brokers to prevent cluster fragmentation and ensure stable cluster formation.} {Timestamp:2025-03-31 11:49:49.407025391 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.storage_failure_injection_config_path:{nullopt}	- Path to the configuration file used for low level storage failure injection.} {Timestamp:2025-03-31 11:49:49.407027605 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.storage_failure_injection_enabled:0	- If `true`, inject low level storage failures on the write path. Do _not_ use for production instances.} {Timestamp:2025-03-31 11:49:49.407029809 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.upgrade_override_checks:0	- Whether to violate safety checks when starting a Redpanda version newer than the cluster's consensus version.} {Timestamp:2025-03-31 11:49:49.407032925 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - redpanda.verbose_logging_timeout_sec_max:{nullopt}	- Maximum duration in seconds for verbose (`TRACE` or `DEBUG`) logging. Values configured above this will be clamped. If null (the default) there is no limit. Can be overridden in the Admin API on a per-request basis.} {Timestamp:2025-03-31 11:49:49.407034748 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - pandaproxy.advertised_pandaproxy_api:{}	- Network address for the HTTP Proxy API server to publish to clients.} {Timestamp:2025-03-31 11:49:49.407036361 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - pandaproxy.api_doc_dir:/usr/share/redpanda/proxy-api-doc	- Path to the API specifications for the HTTP Proxy API.} {Timestamp:2025-03-31 11:49:49.407039667 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - pandaproxy.client_cache_max_size:10	- The maximum number of Kafka client connections that Redpanda can cache in the LRU (least recently used) cache. The LRU cache helps optimize resource utilization by keeping the most recently used clients in memory, facilitating quicker reconnections for frequent clients while limiting memory usage.} {Timestamp:2025-03-31 11:49:49.407041541 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - pandaproxy.client_keep_alive:300000	- Time, in milliseconds, that an idle client connection may remain open to the HTTP Proxy API.} {Timestamp:2025-03-31 11:49:49.407043845 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - pandaproxy.consumer_instance_timeout_ms:300000	- How long to wait for an idle consumer before removing it. A consumer is considered idle when it's not making requests or heartbeats.} {Timestamp:2025-03-31 11:49:49.407045588 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - pandaproxy.pandaproxy_api:{{:{host: 0.0.0.0, port: 8082}:<nullopt>}}	- Rest API listen address and port} {Timestamp:2025-03-31 11:49:49.407047141 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - pandaproxy.pandaproxy_api_tls:{}	- TLS configuration for Pandaproxy api.} {Timestamp:2025-03-31 11:49:49.407049896 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - pandaproxy_client.broker_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} crl file: {nullopt} client_auth_required: 0 }	- TLS configuration for the Kafka API servers to which the HTTP Proxy client should connect.} {Timestamp:2025-03-31 11:49:49.407052231 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - pandaproxy_client.brokers:{{host: 0.0.0.0, port: 9092}}	- Network addresses of the Kafka API servers to which the HTTP Proxy client should connect.} {Timestamp:2025-03-31 11:49:49.407054846 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - pandaproxy_client.client_identifier:{pandaproxy_client}	- Custom identifier to include in the Kafka request header for the HTTP Proxy client. This identifier can help debug or monitor client activities.} {Timestamp:2025-03-31 11:49:49.407056779 +0000 UTC Content:INFO  2025-03-31 11:49:49,406 [shard  0:main] main - application.cc:849 - pandaproxy_client.consumer_heartbeat_interval_ms:500	- Interval (in milliseconds) for consumer heartbeats.} {Timestamp:2025-03-31 11:49:49.407058693 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - pandaproxy_client.consumer_rebalance_timeout_ms:2000	- Timeout (in milliseconds) for consumer rebalance.} {Timestamp:2025-03-31 11:49:49.407060476 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - pandaproxy_client.consumer_request_max_bytes:1048576	- Maximum bytes to fetch per request.} {Timestamp:2025-03-31 11:49:49.407080764 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - pandaproxy_client.consumer_request_min_bytes:1	- Minimum bytes to fetch per request.} {Timestamp:2025-03-31 11:49:49.407082818 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - pandaproxy_client.consumer_request_timeout_ms:100	- Interval (in milliseconds) for consumer request timeout.} {Timestamp:2025-03-31 11:49:49.407084712 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - pandaproxy_client.consumer_session_timeout_ms:300000	- Timeout (in milliseconds) for consumer session.} {Timestamp:2025-03-31 11:49:49.407087006 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - pandaproxy_client.produce_ack_level:-1	- Number of acknowledgments the producer requires the leader to have received before considering a request complete.} {Timestamp:2025-03-31 11:49:49.40708892 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - pandaproxy_client.produce_batch_delay_ms:100	- Delay (in milliseconds) to wait before sending batch.} {Timestamp:2025-03-31 11:49:49.407090853 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - pandaproxy_client.produce_batch_record_count:1000	- Number of records to batch before sending to broker.} {Timestamp:2025-03-31 11:49:49.407092807 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - pandaproxy_client.produce_batch_size_bytes:1048576	- Number of bytes to batch before sending to broker.} {Timestamp:2025-03-31 11:49:49.407095422 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - pandaproxy_client.produce_compression_type:none	- Enable or disable compression by the Kafka client. Specify `none` to disable compression or one of the supported types [gzip, snappy, lz4, zstd].} {Timestamp:2025-03-31 11:49:49.407097526 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - pandaproxy_client.produce_shutdown_delay_ms:0	- Delay (in milliseconds) to allow for final flush of buffers before shutting down.} {Timestamp:2025-03-31 11:49:49.407099209 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - pandaproxy_client.retries:5	- Number of times to retry a request to a broker.} {Timestamp:2025-03-31 11:49:49.407101042 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - pandaproxy_client.retry_base_backoff_ms:100	- Delay (in milliseconds) for initial retry backoff.} {Timestamp:2025-03-31 11:49:49.407102696 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - pandaproxy_client.sasl_mechanism:	- The SASL mechanism to use when connecting.} {Timestamp:2025-03-31 11:49:49.407104529 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - pandaproxy_client.scram_passw
2025-03-31 11:49:50.846175809  ord:	- Password to use for SCRAM authentication mechanisms.} {Timestamp:2025-03-31 11:49:49.407106332 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - pandaproxy_client.scram_username:	- Username to use for SCRAM authentication mechanisms.} {Timestamp:2025-03-31 11:49:49.407108066 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry.api_doc_dir:/usr/share/redpanda/proxy-api-doc	- API doc directory} {Timestamp:2025-03-31 11:49:49.407111512 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry.mode_mutability:1	- Enable modifications to the read-only `mode` of the Schema Registry.When set to `true`, the entire Schema Registry or its subjects can be switched to `READONLY` or `READWRITE`. This property is useful for preventing unwanted changes to the entire Schema Registry or specific subjects.} {Timestamp:2025-03-31 11:49:49.407113466 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry.schema_registry_api:{{:{host: 0.0.0.0, port: 8081}:<nullopt>}}	- Schema Registry API listener address and port} {Timestamp:2025-03-31 11:49:49.407115139 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry.schema_registry_api_tls:{}	- TLS configuration for Schema Registry API.} {Timestamp:2025-03-31 11:49:49.407117303 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry.schema_registry_replication_factor:{nullopt}	- Replication factor for internal `_schemas` topic.  If unset, defaults to `default_topic_replication`.} {Timestamp:2025-03-31 11:49:49.407119898 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.broker_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} crl file: {nullopt} client_auth_required: 0 }	- TLS configuration for the Kafka API servers to which the HTTP Proxy client should connect.} {Timestamp:2025-03-31 11:49:49.407121982 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.brokers:{{host: 0.0.0.0, port: 9092}}	- Network addresses of the Kafka API servers to which the HTTP Proxy client should connect.} {Timestamp:2025-03-31 11:49:49.407137641 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.client_identifier:{schema_registry_client}	- Custom identifier to include in the Kafka request header for the HTTP Proxy client. This identifier can help debug or monitor client activities.} {Timestamp:2025-03-31 11:49:49.407139625 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.consumer_heartbeat_interval_ms:500	- Interval (in milliseconds) for consumer heartbeats.} {Timestamp:2025-03-31 11:49:49.407141558 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.consumer_rebalance_timeout_ms:2000	- Timeout (in milliseconds) for consumer rebalance.} {Timestamp:2025-03-31 11:49:49.407143402 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.consumer_request_max_bytes:1048576	- Maximum bytes to fetch per request.} {Timestamp:2025-03-31 11:49:49.407145135 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.consumer_request_min_bytes:1	- Minimum bytes to fetch per request.} {Timestamp:2025-03-31 11:49:49.407147109 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.consumer_request_timeout_ms:100	- Interval (in milliseconds) for consumer request timeout.} {Timestamp:2025-03-31 11:49:49.407149032 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.consumer_session_timeout_ms:10000	- Timeout (in milliseconds) for consumer session.} {Timestamp:2025-03-31 11:49:49.407151317 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.produce_ack_level:-1	- Number of acknowledgments the producer requires the leader to have received before considering a request complete.} {Timestamp:2025-03-31 11:49:49.40715319 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.produce_batch_delay_ms:0	- Delay (in milliseconds) to wait before sending batch.} {Timestamp:2025-03-31 11:49:49.407155074 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.produce_batch_record_count:0	- Number of records to batch before sending to broker.} {Timestamp:2025-03-31 11:49:49.407156697 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.produce_batch_size_bytes:0	- Number of bytes to batch before sending to broker.} {Timestamp:2025-03-31 11:49:49.407158691 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.produce_compression_type:none	- Enable or disable compression by the Kafka client. Specify `none` to disable compression or one of the supported types [gzip, snappy, lz4, zstd].} {Timestamp:2025-03-31 11:49:49.407160274 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.produce_shutdown_delay_ms:0	- Delay (in milliseconds) to allow for final flush of buffers before shutting down.} {Timestamp:2025-03-31 11:49:49.407161546 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.retries:5	- Number of times to retry a request to a broker.} {Timestamp:2025-03-31 11:49:49.407163059 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.retry_base_backoff_ms:100	- Delay (in milliseconds) for initial retry backoff.} {Timestamp:2025-03-31 11:49:49.407164562 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.sasl_mechanism:	- The SASL mechanism to use when connecting.} {Timestamp:2025-03-31 11:49:49.407166425 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.scram_password:	- Password to use for SCRAM authentication mechanisms.} {Timestamp:2025-03-31 11:49:49.407168168 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - schema_registry_client.scram_username:	- Username to use for SCRAM authentication mechanisms.} {Timestamp:2025-03-31 11:49:49.407170954 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.broker_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} crl file: {nullopt} client_auth_required: 0 }	- TLS configuration for the Kafka API servers to which the HTTP Proxy client should connect.} {Timestamp:2025-03-31 11:49:49.407186012 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.brokers:{{host: 0.0.0.0, port: 9092}}	- Network addresses of the Kafka API servers to which the HTTP Proxy client should connect.} {Timestamp:2025-03-31 11:49:49.407188707 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.client_identifier:{audit_log_client}	- Custom identifier to include in the Kafka request header for the HTTP Proxy client. This identifier can help debug or monitor client activities.} {Timestamp:2025-03-31 11:49:49.407190621 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.consumer_heartbeat_interval_ms:500	- Interval (in milliseconds) for consumer heartbeats.} {Timestamp:2025-03-31 11:49:49.407192464 +0000 UTC Content:INFO  20
2025-03-31 11:49:50.846222647  25-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.consumer_rebalance_timeout_ms:2000	- Timeout (in milliseconds) for consumer rebalance.} {Timestamp:2025-03-31 11:49:49.407194217 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.consumer_request_max_bytes:1048576	- Maximum bytes to fetch per request.} {Timestamp:2025-03-31 11:49:49.40719588 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.consumer_request_min_bytes:1	- Minimum bytes to fetch per request.} {Timestamp:2025-03-31 11:49:49.407197754 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.consumer_request_timeout_ms:100	- Interval (in milliseconds) for consumer request timeout.} {Timestamp:2025-03-31 11:49:49.407199587 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.consumer_session_timeout_ms:10000	- Timeout (in milliseconds) for consumer session.} {Timestamp:2025-03-31 11:49:49.407201801 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.produce_ack_level:1	- Number of acknowledgments the producer requires the leader to have received before considering a request complete.} {Timestamp:2025-03-31 11:49:49.407216379 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.produce_batch_delay_ms:0	- Delay (in milliseconds) to wait before sending batch.} {Timestamp:2025-03-31 11:49:49.407218373 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.produce_batch_record_count:0	- Number of records to batch before sending to broker.} {Timestamp:2025-03-31 11:49:49.40724387 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.produce_batch_size_bytes:0	- Number of bytes to batch before sending to broker.} {Timestamp:2025-03-31 11:49:49.407272304 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.produce_compression_type:zstd	- Enable or disable compression by the Kafka client. Specify `none` to disable compression or one of the supported types [gzip, snappy, lz4, zstd].} {Timestamp:2025-03-31 11:49:49.407295357 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.produce_shutdown_delay_ms:3000	- Delay (in milliseconds) to allow for final flush of buffers before shutting down.} {Timestamp:2025-03-31 11:49:49.407318871 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.retries:5	- Number of times to retry a request to a broker.} {Timestamp:2025-03-31 11:49:49.407340852 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.retry_base_backoff_ms:100	- Delay (in milliseconds) for initial retry backoff.} {Timestamp:2025-03-31 11:49:49.407342515 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.sasl_mechanism:	- The SASL mechanism to use when connecting.} {Timestamp:2025-03-31 11:49:49.407343928 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.scram_password:	- Password to use for SCRAM authentication mechanisms.} {Timestamp:2025-03-31 11:49:49.407351432 +0000 UTC Content:INFO  2025-03-31 11:49:49,407 [shard  0:main] main - application.cc:849 - audit_log_client.scram_username:	- Username to use for SCRAM authentication mechanisms.} {Timestamp:2025-03-31 11:49:49.408219831 +0000 UTC Content:INFO  2025-03-31 11:49:49,408 [shard  0:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.410344147 +0000 UTC Content:INFO  2025-03-31 11:49:49,410 [shard  6:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.410640563 +0000 UTC Content:INFO  2025-03-31 11:49:49,410 [shard 11:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.410732456 +0000 UTC Content:INFO  2025-03-31 11:49:49,410 [shard 10:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411312915 +0000 UTC Content:INFO  2025-03-31 11:49:49,410 [shard 17:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411314638 +0000 UTC Content:INFO  2025-03-31 11:49:49,410 [shard 16:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.4113159 +0000 UTC Content:INFO  2025-03-31 11:49:49,410 [shard 21:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411317103 +0000 UTC Content:INFO  2025-03-31 11:49:49,410 [shard 20:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411318365 +0000 UTC Content:INFO  2025-03-31 11:49:49,411 [shard  3:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411319577 +0000 UTC Content:INFO  2025-03-31 11:49:49,411 [shard  1:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411320779 +0000 UTC Content:INFO  2025-03-31 11:49:49,411 [shard 14:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411321982 +0000 UTC Content:INFO  2025-03-31 11:49:49,411 [shard 15:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411323204 +0000 UTC Content:INFO  2025-03-31 11:49:49,411 [shard  2:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411324406 +0000 UTC Content:INFO  2025-03-31 11:49:49,411 [shard  4:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411325598 +0000 UTC Content:INFO  2025-03-31 11:49:49,411 [shard  5:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411326801 +0000 UTC Content:INFO  2025-03-31 11:49:49,411 [shard 12:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411327983 +0000 UTC Content:INFO  2025-03-31 11:49:49,411 [shard  9:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411366295 +0000 UTC Content:INFO  2025-03-31 11:49:49,411 [shard 19:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411367667 +0000 UTC Content:INFO  2025-03-31 11:49:49,411 [shard 18:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411393556 +0000 UTC Content:INFO  2025-03-31 11:49:49,411 [shard 13:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411411109 +0000 UTC Content:INFO  2025-03-31 11:49:49,411 [shard  8:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411459991 +0000 UTC Content:INFO  2025-03-31 11:49:49,411 [shard  7:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411527828 +0000 UTC Content:INFO  2025-03-31 11:49:49,411 [shard 22:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411599703 +0000 UTC Content:INFO  2025-03-31 11:49:49,411 [shard 23:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate} {Timestamp:2025-03-31 11:49:49.411600725 +0000 UTC Content:INFO  2025-03-31 11:49:49,411 [shard  0:main] main - application.cc:563 - Setting abort_on_allocation_failure (abort on OOM): true} {Timestamp:2025-03-31 11:49:49.420953
2025-03-31 11:49:50.846224029  332 +0000 UTC Content:INFO  2025-03-31 11:49:49,420 [shard  0:main] syschecks - Writing pid file "/data/redpanda/pid.lock"}]}} ObservedRedpandaServiceConfig:{DefaultTopicRetentionMs:0 DefaultTopicRetentionBytes:0}}
2025-03-31 11:49:50.846267150   [DEBUG]	[ControlLoop]			Updated system snapshot at tick 1015
2025-03-31 11:49:50.930359473   [INFO]	[redpanda]			Setting desired state of FSM redpanda to stopped
2025-03-31 11:49:50.930361236   [INFO]	[RedpandaManagerCore]			Created instance redpanda
2025-03-31 11:49:50.930416640   [DEBUG]	[ControlLoop]			Updated system snapshot at tick 1016
2025-03-31 11:49:51.030099574   [DEBUG]	[redpanda]			Service status: {LastStateChange:0 ServiceInfo:{Status: Uptime:0 DownTime:0 ReadyTime:0 Pid:0 Pgid:0 ExitCode:0 WantUp:false IsPaused:false IsFinishing:false IsWantingUp:false IsReady:false ExitHistory:[] LastChangedAt:0001-01-01 00:00:00 +0000 UTC LastReadyAt:0001-01-01 00:00:00 +0000 UTC} ObservedS6ServiceConfig:{Command:[] Env:map[] ConfigFiles:map[] MemoryLimit:0}}
2025-03-31 11:49:51.030101327   [DEBUG]	[redpanda]			Error: service does not exist
2025-03-31 11:49:51.030102099   [DEBUG]	[redpanda]			Starting Action: Adding Redpanda service redpanda to S6 manager ...
2025-03-31 11:49:51.030121836   [DEBUG]	[redpanda]			Redpanda service redpanda added to S6 manager
2025-03-31 11:49:51.030127647   [DEBUG]	[redpanda]			Entering creating state for FSM redpanda
2025-03-31 11:49:51.030154567   [INFO]	[redpanda]			Setting desired state of FSM redpanda to running
2025-03-31 11:49:51.030155228   [INFO]	[S6ManagerRedpandaServiceredpanda]			Created instance redpanda
2025-03-31 11:49:51.030209390   [DEBUG]	[ControlLoop]			Updated system snapshot at tick 1017
2025-03-31 11:49:51.128704124   [INFO]	[S6ManagerCore]			Updated desired state of instance sleepy-7 from running to stopped
2025-03-31 11:49:51.128705687   [INFO]	[sleepy-7]			Setting desired state of FSM sleepy-7 to stopped
2025-03-31 11:49:51.128783082   [DEBUG]	[ControlLoop]			Updated system snapshot at tick 1018
2025-03-31 11:49:51.229560229   [DEBUG]	[sleepy-7]			Starting Action: Stopping S6 service sleepy-7 ...
2025-03-31 11:49:51.229561882   [DEBUG]	[S6Service]			Stopping S6 service /run/service/sleepy-7
2025-03-31 11:49:51.229882394   [DEBUG]	[S6Service]			Stopped S6 service /run/service/sleepy-7
2025-03-31 11:49:51.229884678   [DEBUG]	[sleepy-7]			S6 service sleepy-7 stop command executed
2025-03-31 11:49:51.229897662   [INFO]	[sleepy-7]			Entering stopping state for sleepy-7
2025-03-31 11:49:51.229957765   [DEBUG]	[ControlLoop]			Updated system snapshot at tick 1019
2025-03-31 11:49:51.328210736   [INFO]	[StarveCheck]			Control loop is healthy, last reconcile was 0.70 seconds ago
2025-03-31 11:49:51.329361185   [INFO]	[sleepy-7]			Entering stopped state for sleepy-7
2025-03-31 11:49:51.329427589   [DEBUG]	[ControlLoop]			Updated system snapshot at tick 1020
2025-03-31 11:49:51.430907485   [INFO]	[Core]			======= Container Instance State: Core (tick: 800) =======
2025-03-31 11:49:51.430909418   [INFO]	[Core]			FSM States: Current=degraded, Desired=active
2025-03-31 11:49:51.430910831   [INFO]	[Core]			Health: Overall=Degraded, CPU=Active, Memory=Active, Disk=Degraded
2025-03-31 11:49:51.430911733   [INFO]	[Core]			CPU: Usage=5113.92m cores, Cores=24
2025-03-31 11:49:51.430912764   [INFO]	[Core]			Memory: Used=13201.68 MB, Total=128729.00 MB, Usage=10.26%
2025-03-31 11:49:51.430913736   [INFO]	[Core]			Disk: Used=897.28 GB, Total=930.51 GB, Usage=96.43%
2025-03-31 11:49:51.430915029   [INFO]	[Core]			Architecture: amd64, HWID: f8f88cc406c9095f57ceae0e2bde3a41f6d67093b9c15b9012155068d8408a0b
2025-03-31 11:49:51.430916051   [INFO]	[Core]			=================================================
2025-03-31 11:49:51.431479428   [DEBUG]	[RedpandaServiceredpanda]			Request for public_metrics took 384.922Âµs
2025-03-31 11:49:51.431503463   [DEBUG]	[redpanda]			Service status: {LastStateChange:0 ServiceInfo:{Status: Uptime:0 DownTime:0 ReadyTime:0 Pid:0 Pgid:0 ExitCode:0 WantUp:false IsPaused:false IsFinishing:false IsWantingUp:false IsReady:false ExitHistory:[] LastChangedAt:0001-01-01 00:00:00 +0000 UTC LastReadyAt:0001-01-01 00:00:00 +0000 UTC} ObservedS6ServiceConfig:{Command:[] Env:map[] ConfigFiles:map[] MemoryLimit:0}}
2025-03-31 11:49:51.431504334   [DEBUG]	[redpanda]			Error: health check connection refused
2025-03-31 11:49:51.431510496   [DEBUG]	[redpanda]			Health check refused connection for service redpanda, returning ServiceInfo with failed health checks
2025-03-31 11:49:51.431681537   [INFO]	[redpanda]			Entering stopped state for redpanda
2025-03-31 11:49:51.431897031   [DEBUG]	[redpanda]			Starting Action: Creating S6 service redpanda ...
2025-03-31 11:49:51.431898263   [DEBUG]	[S6Service]			Creating S6 service /run/service/redpanda
2025-03-31 11:49:51.432185132   [DEBUG]	[S6Service]			S6 service /run/service/redpanda created with logging to /data/logs/redpanda
2025-03-31 11:49:51.432186895   [DEBUG]	[redpanda]			S6 service redpanda directory structure created
2025-03-31 11:49:51.432188027   [DEBUG]	[redpanda]			Entering creating state for FSM redpanda
2025-03-31 11:49:51.432279629   [DEBUG]	[ControlLoop]			Updated system snapshot at tick 1021
2025-03-31 11:49:51.530701968   [DEBUG]	[RedpandaServiceredpanda]			Request for public_metrics took 295.094Âµs
2025-03-31 11:49:51.530757231   [DEBUG]	[redpanda]			Service status: {LastStateChange:1743421791 ServiceInfo:{Status:down Uptime:0 DownTime:0 ReadyTime:0 Pid:0 Pgid:0 ExitCode:1 WantUp:false IsPaused:false IsFinishing:false IsWantingUp:true IsReady:true ExitHistory:[{Timestamp:2025-03-31 11:48:15.394869032 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:18.381323959 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:21.454108845 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:24.349570448 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:27.271394348 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:30.136743364 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:33.029690745 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:35.94873716 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:38.780066259 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:41.666966838 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:44.562601326 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:47.474821049 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:50.351683348 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:53.228880648 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:56.087776568 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:58.90743484 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:01.764619923 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:04.610268496 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:07.484995091 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:10.470455098 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:13.306413786 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:16.230537829 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:19.162818115 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:22.067429665 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:24.999969643 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:27.94693202 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:30.860684384 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:33.711746083 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:36.552564274 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:39.435492518 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:42.322903603 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:45.205946231 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:48.140896557 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:50.925845223 +0000 UTC ExitCode:1 Signal:0}] LastChangedAt:2025-03-31 11:49:50.925845223 +0000 UTC LastReadyAt:2025-03-31 11:49:50.926408079 +0000 UTC} ObservedS6ServiceConfig:{Command:[/opt/redpanda/bin/redpanda --redpanda-cfg /run/service/redpanda/config/redpanda.yaml] Env:map[] ConfigFiles:map[redpanda.yaml:# Redpanda configuration file
2025-03-31 11:49:51.530758874  
2025-03-31 11:49:51.530759285  redpanda:
2025-03-31 11:49:51.530759786    data_directory: "/data/redpanda"
2025-03-31 11:49:51.530760317  
2025-03-31 11:49:51.530760678    seed_servers: []
2025-03-31 11:49:51.530761109  
2025-03-31 11:49:51.530761529    rpc_server:
2025-03-31 11:49:51.530761930      address: "0.0.0.0"
2025-03-31 11:49:51.530762351      port: 33145
2025-03-31 11:49:51.530771748  
2025-03-31 11:49:51.530772480    advertised_rpc_api:
2025-03-31 11:49:51.530773141      address: "127.0.0.1"
2025-03-31 11:49:51.530773742      port: 33145
2025-03-31 11:49:51.530774253  
2025-03-31 11:49:51.530774794    kafka_api:
2025-03-31 11:49:51.530775686    - address: "0.0.0.0"
2025-03-31 11:49:51.530776157      port: 9092
2025-03-31 11:49:51.530776517  
2025-03-31 11:49:51.530776878    advertised_kafka_api:
2025-03-31 11:49:51.530777339    - address: "127.0.0.1"
2025-03-31 11:49:51.530777740      port: 9092
2025-03-31 11:49:51.530778130  
2025-03-31 11:49:51.530778431    admin:
2025-03-31 11:49:51.530778892      address: "0.0.0.0"
2025-03-31 11:49:51.530779343      port: 9644
2025-03-31 11:49:51.530779633  
2025-03-31 11:49:51.530780054    developer_mode: true
2025-03-31 11:49:51.530780385  
2025-03-31 11:49:51.530780815    # Default topic retention configuration:
2025-03-31 11:49:51.530781256    log_retention_ms: -1
2025-03-31 11:49:51.530781817    retention_bytes: null
2025-03-31 11:49:51.530782158  
2025-03-31 11:49:51.530782709    # Auto topic creation configuration:
2025-03-31 11:49:51.530783300    auto_create_topics_enabled: true  # Enable automatic topic creation
2025-03-31 11:49:51.530783621  
2025-03-31 11:49:51.530784011  pandaproxy: {}
2025-03-31 11:49:51.530784272  
2025-03-31 11:49:51.530784673  schema_registry: {}
2025-03-31 11:49:51.530785003  
2025-03-31 11:49:51.530785304  rpk:
2025-03-31 11:49:51.530785785    coredump_dir: "/data/redpanda/coredump"
2025-03-31 11:49:51.530786196  ] MemoryLimit:0}}
2025-03-31 11:49:51.530786817   [DEBUG]	[redpanda]			Error: health check connection refused
2025-03-31 11:49:51.531021868   [INFO]	[redpanda]			Entering stopped state for redpanda
2025-03-31 11:49:51.531097219   [DEBUG]	[ControlLoop]			Updated system snapshot at tick 1022
2025-03-31 11:49:51.630540203   [DEBUG]	[RedpandaServiceredpanda]			Request for public_metrics took 284.123Âµs
2025-03-31 11:49:51.630624481   [DEBUG]	[redpanda]			Service status: {LastStateChange:1743421791 ServiceInfo:{Status:down Uptime:0 DownTime:0 ReadyTime:0 Pid:0 Pgid:0 ExitCode:1 WantUp:false IsPaused:false IsFinishing:false IsWantingUp:true IsReady:true ExitHistory:[{Timestamp:2025-03-31 11:48:15.394869032 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:18.381323959 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:21.454108845 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:24.349570448 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:27.271394348 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:30.136743364 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:33.029690745 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:35.94873716 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:38.780066259 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:41.666966838 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:44.562601326 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:47.474821049 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:50.351683348 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:53.228880648 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:56.087776568 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:48:58.90743484 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:01.764619923 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:04.610268496 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:07.484995091 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:10.470455098 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:13.306413786 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:16.230537829 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:19.162818115 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:22.067429665 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:24.999969643 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:27.94693202 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:30.860684384 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:33.711746083 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:36.552564274 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:39.435492518 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:42.322903603 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:45.205946231 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:48.140896557 +0000 UTC ExitCode:1 Signal:0} {Timestamp:2025-03-31 11:49:50.925845223 +0000 UTC ExitCode:1 Signal:0}] LastChangedAt:2025-03-31 11:49:50.925845223 +0000 UTC LastReadyAt:2025-03-31 11:49:50.926408079 +0000 UTC} ObservedS6ServiceConfig:{Command:[/opt/redpanda/bin/redpanda --redpanda-cfg /run/service/redpanda/config/redpanda.yaml] Env:map[] ConfigFiles:map[redpanda.yaml:# Redpanda configuration file
2025-03-31 11:49:51.630626064  
2025-03-31 11:49:51.630626675  redpanda:
2025-03-31 11:49:51.630627486    data_directory: "/data/redpanda"
2025-03-31 11:49:51.630627977  
2025-03-31 11:49:51.630628458    seed_servers: []
2025-03-31 11:49:51.630628879  
2025-03-31 11:49:51.630629450    rpc_server:
2025-03-31 11:49:51.630630141      address: "0.0.0.0"
2025-03-31 11:49:51.630630743      port: 33145
2025-03-31 11:49:51.630631233  
2025-03-31 11:49:51.630631865    advertised_rpc_api:
2025-03-31 11:49:51.630632516      address: "127.0.0.1"
2025-03-31 11:49:51.630633107      port: 33145
2025-03-31 11:49:51.630633518  
2025-03-31 11:49:51.630634009    kafka_api:
2025-03-31 11:49:51.630634640    - address: "0.0.0.0"
2025-03-31 11:49:51.630635231      port: 9092
2025-03-31 11:49:51.630635672  
2025-03-31 11:49:51.630636293    advertised_kafka_api:
2025-03-31 11:49:51.630636944    - address: "127.0.0.1"
2025-03-31 11:49:51.630637565      port: 9092
2025-03-31 11:49:51.630637966  
2025-03-31 11:49:51.630638457    admin:
2025-03-31 11:49:51.630639088      address: "0.0.0.0"
2025-03-31 11:49:51.630639579      port: 9644
2025-03-31 11:49:51.630639890  
2025-03-31 11:49:51.630640260    developer_mode: true
2025-03-31 11:49:51.630640541  
2025-03-31 11:49:51.630640952    # Default topic retention configuration:
2025-03-31 11:49:51.630641342    log_retention_ms: -1
2025-03-31 11:49:51.630641693    retention_bytes: null
2025-03-31 11:49:51.630642004  
2025-03-31 11:49:51.630642414    # Auto topic creation configuration:
2025-03-31 11:49:51.630642975    auto_create_topics_enabled: true  # Enable automatic topic creation
2025-03-31 11:49:51.630643296  
2025-03-31 11:49:51.630643667  pandaproxy: {}
2025-03-31 11:49:51.630643927  
2025-03-31 11:49:51.630644328  schema_registry: {}
2025-03-31 11:49:51.630644629  
2025-03-31 11:49:51.630644929  rpk:
2025-03-31 11:49:51.630645400    coredump_dir: "/data/redpanda/coredump"
2025-03-31 11:49:51.630645791  ] MemoryLimit:0}}
2025-03-31 11:49:51.630646312   [DEBUG]	[redpanda]			Error: health check connection refused
2025-03-31 11:49:51.631091908   [DEBUG]	[redpanda]			Starting Action: Starting S6 service redpanda ...
2025-03-31 11:49:51.631092840   [DEBUG]	[S6Service]			Starting S6 service /run/service/redpanda
2025-03-31 11:49:51.631520031   [DEBUG]	[S6Service]			Started S6 service /run/service/redpanda
2025-03-31 11:49:51.631521824   [DEBUG]	[redpanda]			S6 service redpanda start command executed
2025-03-31 11:49:51.631522856   [INFO]	[redpanda]			Entering starting state for redpanda
2025-03-31 11:49:51.631639515   [DEBUG]	[ControlLoop]			Updated system snapshot at tick 1023


=== GOLDEN SERVICE INTERNAL LOGS ===
2025-03-31 11:48:10.396083365  level=info msg="Running main config from specified file" @service=benthos benthos_version="" path=/run/service/golden-service/config/golden-service.yaml
2025-03-31 11:48:10.397603797  level=info msg="Listening for HTTP requests at: http://0.0.0.0:4195" @service=benthos
2025-03-31 11:48:10.397670873  level=info msg="Launching a Benthos instance, use CTRL+C to close" @service=benthos
2025-03-31 11:48:10.397761393  level=info msg="Receiving HTTP messages at: http://0.0.0.0:8082/" @service=benthos label="" path=root.input
2025-03-31 11:48:10.397788514  level=info msg="Output type stdout is now active" @service=benthos label="" path=root.output
2025-03-31 11:48:11.329270083  {"message": "test"}
2025-03-31 11:48:19.048812513  {"message": "test"}
2025-03-31 11:48:21.233394400  {"message": "test"}
2025-03-31 11:48:22.206351053  {"message": "test"}
2025-03-31 11:48:23.604470231  {"message": "test"}
2025-03-31 11:48:24.709604309  {"message": "test"}
2025-03-31 11:48:25.495141514  {"message": "test"}
2025-03-31 11:48:27.142873180  {"message": "test"}
2025-03-31 11:48:27.807358646  {"message": "test"}
2025-03-31 11:48:29.108281448  {"message": "test"}
2025-03-31 11:48:30.084655363  {"message": "test"}
2025-03-31 11:48:30.802431434  {"message": "test"}
2025-03-31 11:48:31.770352810  {"message": "test"}
2025-03-31 11:48:32.986402791  {"message": "test"}
2025-03-31 11:48:33.565789658  {"message": "test"}
2025-03-31 11:48:34.944840661  {"message": "test"}
2025-03-31 11:48:35.924287584  {"message": "test"}
2025-03-31 11:48:36.475562583  {"message": "test"}
2025-03-31 11:48:38.305409896  {"message": "test"}
2025-03-31 11:48:39.327506690  {"message": "test"}
2025-03-31 11:48:41.130657403  {"message": "test"}
2025-03-31 11:48:42.029045079  {"message": "test"}
2025-03-31 11:48:43.203407158  {"message": "test"}
2025-03-31 11:48:44.576583215  {"message": "test"}
2025-03-31 11:48:45.259988440  {"message": "test"}
2025-03-31 11:48:47.180635122  {"message": "test"}
2025-03-31 11:48:48.072617164  {"message": "test"}
2025-03-31 11:48:49.301735381  {"message": "test"}
2025-03-31 11:48:50.375243516  {"message": "test"}
2025-03-31 11:48:50.946333448  {"message": "test"}
2025-03-31 11:48:52.768253386  {"message": "test"}
2025-03-31 11:48:53.441810959  {"message": "test"}
2025-03-31 11:48:54.082998309  {"message": "test"}
2025-03-31 11:48:55.680092100  {"message": "test"}
2025-03-31 11:48:56.487910983  {"message": "test"}
2025-03-31 11:48:56.948011654  {"message": "test"}
2025-03-31 11:48:57.997406736  {"message": "test"}
2025-03-31 11:48:58.903303229  {"message": "test"}
2025-03-31 11:48:59.709850397  {"message": "test"}
2025-03-31 11:49:01.589090941  {"message": "test"}
2025-03-31 11:49:02.406054628  {"message": "test"}
2025-03-31 11:49:04.222879108  {"message": "test"}
2025-03-31 11:49:04.789187861  {"message": "test"}
2025-03-31 11:49:05.275921061  {"message": "test"}
2025-03-31 11:49:07.046257879  {"message": "test"}
2025-03-31 11:49:07.555627419  {"message": "test"}
2025-03-31 11:49:08.109057128  {"message": "test"}
2025-03-31 11:49:08.619987609  {"message": "test"}
2025-03-31 11:49:10.012533900  {"message": "test"}
2025-03-31 11:49:10.687921713  {"message": "test"}
2025-03-31 11:49:11.615232395  {"message": "test"}
2025-03-31 11:49:13.181850359  {"message": "test"}
2025-03-31 11:49:14.089692536  {"message": "test"}
2025-03-31 11:49:15.732849507  {"message": "test"}
2025-03-31 11:49:16.284727883  {"message": "test"}
2025-03-31 11:49:16.808657726  {"message": "test"}
2025-03-31 11:49:17.505603470  {"message": "test"}
2025-03-31 11:49:18.899854577  {"message": "test"}
2025-03-31 11:49:19.667116675  {"message": "test"}
2025-03-31 11:49:21.129152543  {"message": "test"}
2025-03-31 11:49:22.051254509  {"message": "test"}
2025-03-31 11:49:22.670063726  {"message": "test"}
2025-03-31 11:49:23.332939317  {"message": "test"}
2025-03-31 11:49:24.782412916  {"message": "test"}
2025-03-31 11:49:25.259886982  {"message": "test"}
2025-03-31 11:49:26.227614971  {"message": "test"}
2025-03-31 11:49:27.626638807  {"message": "test"}
2025-03-31 11:49:28.382080313  {"message": "test"}
2025-03-31 11:49:29.064327170  {"message": "test"}
2025-03-31 11:49:30.615199877  {"message": "test"}
2025-03-31 11:49:31.356270330  {"message": "test"}
2025-03-31 11:49:31.948115991  {"message": "test"}
2025-03-31 11:49:33.642834947  {"message": "test"}
2025-03-31 11:49:34.325544646  {"message": "test"}
2025-03-31 11:49:34.933549510  {"message": "test"}
2025-03-31 11:49:36.433642380  {"message": "test"}
2025-03-31 11:49:37.099991551  {"message": "test"}
2025-03-31 11:49:37.791033971  {"message": "test"}
2025-03-31 11:49:39.188051523  {"message": "test"}
2025-03-31 11:49:39.692108877  {"message": "test"}
2025-03-31 11:49:40.425756167  {"message": "test"}
2025-03-31 11:49:42.071018998  {"message": "test"}
2025-03-31 11:49:42.924246846  {"message": "test"}
2025-03-31 11:49:43.546654622  {"message": "test"}
2025-03-31 11:49:44.976398682  {"message": "test"}
2025-03-31 11:49:45.722969122  {"message": "test"}
2025-03-31 11:49:46.870042686  {"message": "test"}
2025-03-31 11:49:47.953665028  {"message": "test"}
2025-03-31 11:49:48.718343400  {"message": "test"}
2025-03-31 11:49:50.471984718  {"message": "test"}


=== AVAILABLE LOG FILES ===
/data/logs/umh-core/lock
/data/logs/umh-core/state
/data/logs/umh-core/@4000000067ea814d31b64c23.s
/data/logs/umh-core/@4000000067ea816e2c984e9c.s
/data/logs/umh-core/current
/data/logs/umh-core/@4000000067ea8183322a7289.s
/data/logs/redpanda/lock
/data/logs/redpanda/state
/data/logs/redpanda/@4000000067ea81282c37ee9e.s
/data/logs/redpanda/@4000000067ea812e26539698.s
/data/logs/redpanda/@4000000067ea813419b36bde.s
/data/logs/redpanda/@4000000067ea81382a592414.s
/data/logs/redpanda/@4000000067ea813e1c12c9ea.s
/data/logs/redpanda/@4000000067ea81452bf7c9f5.s
/data/logs/redpanda/@4000000067ea814a00736a9e.s
/data/logs/redpanda/@4000000067ea814f2a3ecb99.s
/data/logs/redpanda/@4000000067ea8155188e0e84.s
/data/logs/redpanda/@4000000067ea815c2cb964ac.s
/data/logs/redpanda/@4000000067ea816220477f96.s
/data/logs/redpanda/@4000000067ea8168155849c0.s
/data/logs/redpanda/@4000000067ea816e0f4e99ab.s
/data/logs/redpanda/@4000000067ea81792b9cf12c.s
/data/logs/redpanda/current
/data/logs/redpanda/@4000000067ea818327e75f5f.s
/data/logs/golden-service/lock
/data/logs/golden-service/state
/data/logs/golden-service/current
/data/logs/sleepy-0/lock
/data/logs/sleepy-0/state
/data/logs/sleepy-0/current
/data/logs/sleepy-1/lock
/data/logs/sleepy-1/state
/data/logs/sleepy-1/current
/data/logs/sleepy-2/lock
/data/logs/sleepy-2/state
/data/logs/sleepy-2/current
/data/logs/sleepy-3/lock
/data/logs/sleepy-3/state
/data/logs/sleepy-3/current
/data/logs/sleepy-4/lock
/data/logs/sleepy-4/state
/data/logs/sleepy-4/current
/data/logs/sleepy-5/lock
/data/logs/sleepy-5/state
/data/logs/sleepy-5/current
/data/logs/sleepy-6/lock
/data/logs/sleepy-6/state
/data/logs/sleepy-6/current
/data/logs/sleepy-7/lock
/data/logs/sleepy-7/state
/data/logs/sleepy-7/current
/data/logs/sleepy-8/lock
/data/logs/sleepy-8/state
/data/logs/sleepy-8/current
/data/logs/sleepy-9/lock
/data/logs/sleepy-9/state
/data/logs/sleepy-9/current


Latest YAML config at time of failure:
agent:
    metricsPort: 8080
    communicator:
        apiUrl: ""
        authToken: ""
    releaseChannel: ""
services:
    - name: golden-service
      desiredState: running
      s6ServiceConfig:
        command:
            - /usr/local/bin/benthos
            - -c
            - /run/service/golden-service/config/golden-service.yaml
        env:
            LOG_LEVEL: DEBUG
        configFiles:
            golden-service.yaml: |
                ---
                input:
                  http_server:
                    path: /
                    address: 0.0.0.0:8082
                output:
                  stdout: {}
        memoryLimit: 0
    - name: sleepy-0
      desiredState: running
      s6ServiceConfig:
        command:
            - sleep
            - "600"
        env: {}
        configFiles: {}
        memoryLimit: 0
    - name: sleepy-1
      desiredState: stopped
      s6ServiceConfig:
        command:
            - sleep
            - "600"
        env: {}
        configFiles: {}
        memoryLimit: 0
    - name: sleepy-2
      desiredState: stopped
      s6ServiceConfig:
        command:
            - sleep
            - "600"
        env: {}
        configFiles: {}
        memoryLimit: 0
    - name: sleepy-3
      desiredState: stopped
      s6ServiceConfig:
        command:
            - sleep
            - "600"
        env: {}
        configFiles: {}
        memoryLimit: 0
    - name: sleepy-4
      desiredState: stopped
      s6ServiceConfig:
        command:
            - sleep
            - "600"
        env: {}
        configFiles: {}
        memoryLimit: 0
    - name: sleepy-5
      desiredState: stopped
      s6ServiceConfig:
        command:
            - sleep
            - "600"
        env: {}
        configFiles: {}
        memoryLimit: 0
    - name: sleepy-6
      desiredState: running
      s6ServiceConfig:
        command:
            - sleep
            - "600"
        env: {}
        configFiles: {}
        memoryLimit: 0
    - name: sleepy-7
      desiredState: stopped
      s6ServiceConfig:
        command:
            - sleep
            - "600"
        env: {}
        configFiles: {}
        memoryLimit: 0
    - name: sleepy-8
      desiredState: stopped
      s6ServiceConfig:
        command:
            - sleep
            - "600"
        env: {}
        configFiles: {}
        memoryLimit: 0
    - name: sleepy-9
      desiredState: stopped
      s6ServiceConfig:
        command:
            - sleep
            - "600"
        env: {}
        configFiles: {}
        memoryLimit: 0
benthos: []
nmap: []
redpanda:
    name: redpanda
    desiredState: stopped
    redpandaServiceConfig:
        defaultTopicRetentionMs: 0
        defaultTopicRetentionBytes: 0


Test failed. Container name: umh-core-fa6e64b9
[38;5;9mâ€¢ [FAILED] [178.724 seconds][0m
[0mUMH Container Integration [38;5;243mwith service scaling test [38;5;9m[1m[It] should scale up to multiple services while maintaining healthy metrics[0m [38;5;204m[integration, scaling][0m
[38;5;243m/home/scarjit/Git/united-manufacturing-hub/umh-core/integration/integration_test.go:209[0m

  [38;5;9m[FAILED] Error counter (1) exceeded 0: umh_core_errors_total{component="base_fsm_manager",instance="RedpandaManagerCore.instances.redpanda"} 1
  Expected
      <float64>: 1
  to be <=
      <float64>: 0[0m
  [38;5;9mIn [1m[It][0m[38;5;9m at: [1m/home/scarjit/Git/united-manufacturing-hub/umh-core/integration/metrics_parsing.go:155[0m [38;5;243m@ 03/31/25 13:49:51.354[0m
[38;5;243m------------------------------[0m
[38;5;14mS[0m[38;5;14mS[0m

[38;5;9m[1mSummarizing 1 Failure:[0m
  [38;5;9m[FAIL][0m [0mUMH Container Integration [38;5;243mwith service scaling test [38;5;9m[1m[It] should scale up to multiple services while maintaining healthy metrics[0m [38;5;204m[integration, scaling][0m
  [38;5;243m/home/scarjit/Git/united-manufacturing-hub/umh-core/integration/metrics_parsing.go:155[0m

[38;5;9m[1mRan 1 of 12 Specs in 178.724 seconds[0m
[38;5;9m[1mFAIL![0m -- [38;5;10m[1m0 Passed[0m | [38;5;9m[1m1 Failed[0m | [38;5;11m[1m0 Pending[0m | [38;5;14m[1m11 Skipped[0m
--- FAIL: TestIntegration (178.72s)
FAIL

Ginkgo ran 31 suites in 2m59.466654289s

There were failures detected in the following suites:
  [38;5;9mintegration [38;5;243m./integration[0m

Test Suite Failed
