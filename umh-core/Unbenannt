diff --git a/umh-core/pkg/constants/s6.go b/umh-core/pkg/constants/s6.go
index 191f4e955..dc0541edd 100644
--- a/umh-core/pkg/constants/s6.go
+++ b/umh-core/pkg/constants/s6.go
@@ -32,6 +32,18 @@ const (
 	S6UpdateObservedStateTimeout = time.Millisecond * 3
 	S6RemoveTimeout              = time.Millisecond * 3
 	S6MaxLines                   = 10000
+
+	// S6FileReadTimeBuffer is the minimum time buffer required before attempting to read a file chunk
+	// This is half of DefaultTickerTime to ensure graceful early exit from file operations
+	// WHY HALF: Provides safety margin to complete current chunk + cleanup before context deadline
+	// BUSINESS LOGIC: Prevents timeout failures by returning partial success instead of total failure
+	S6FileReadTimeBuffer = DefaultTickerTime / 2
+
+	// S6FileReadChunkSize is the buffer size used for reading files in chunks
+	// Set to 1MB for optimal I/O performance while maintaining memory efficiency
+	// WHY 1MB: Balance between I/O throughput (fewer syscalls) and memory usage (bounded allocation)
+	// PERFORMANCE: Large enough to amortize syscall overhead, small enough to avoid memory pressure
+	S6FileReadChunkSize = 1024 * 1024
 )
 
 const (
diff --git a/umh-core/pkg/service/filesystem/filesystem.go b/umh-core/pkg/service/filesystem/filesystem.go
index cb2bd3f97..83c6522ee 100644
--- a/umh-core/pkg/service/filesystem/filesystem.go
+++ b/umh-core/pkg/service/filesystem/filesystem.go
@@ -21,13 +21,25 @@ import (
 	"os"
 	"os/exec"
 	"path/filepath"
+	"sync"
 	"time"
 
+	"github.com/united-manufacturing-hub/united-manufacturing-hub/umh-core/pkg/constants"
 	"github.com/united-manufacturing-hub/united-manufacturing-hub/umh-core/pkg/logger"
 	"github.com/united-manufacturing-hub/united-manufacturing-hub/umh-core/pkg/metrics"
 	"github.com/united-manufacturing-hub/united-manufacturing-hub/umh-core/pkg/sentry"
 )
 
+// chunkBufferPool provides reusable buffers for efficient file reading
+// CONCURRENCY SAFETY: sync.Pool is thread-safe and handles concurrent Get/Put operations
+// MEMORY EFFICIENCY: Reuses 1MB buffers across goroutines to reduce GC pressure
+// BUFFER REUSE: Each buffer is completely overwritten by io.ReadFull before being appended to result
+var chunkBufferPool = sync.Pool{
+	New: func() interface{} {
+		return make([]byte, constants.S6FileReadChunkSize)
+	},
+}
+
 // Service provides an interface for filesystem operations
 // This allows for easier testing and separation of concerns
 type Service interface {
@@ -37,7 +49,7 @@ type Service interface {
 	// ReadFile reads a file's contents respecting the context
 	ReadFile(ctx context.Context, path string) ([]byte, error)
 
-	// ReadFileRange reads the file starting at byte offset “from” and returns:
+	// ReadFileRange reads the file starting at byte offset "from" and returns:
 	//   - chunk   – the data that was read (nil if nothing new)
 	//   - newSize – the file size **after** the read (use it as next offset)
 	ReadFileRange(ctx context.Context, path string, from int64) ([]byte, int64, error)
@@ -156,9 +168,21 @@ func (s *DefaultService) ReadFile(ctx context.Context, path string) ([]byte, err
 	}
 }
 
-// ReadFileRange reads the file starting at byte offset “from” and returns:
+// ReadFileRange reads the file starting at byte offset "from" and returns:
 //   - chunk   – the data that was read (nil if nothing new)
 //   - newSize – the file size **after** the read (use it as next offset)
+//
+// PERFORMANCE OPTIMIZATIONS:
+// 1. MEMORY EFFICIENT: Pre-allocates result buffer to avoid repeated allocations during append()
+// 2. BUFFER POOLING: Reuses 1MB working buffers via sync.Pool to reduce GC pressure
+// 3. CONTEXT AWARE: Checks deadline before each chunk, returns SUCCESS (not timeout) if insufficient time
+// 4. CHUNKED I/O: Reads in 1MB chunks for optimal I/O performance while maintaining memory control
+//
+// CORRECTNESS GUARANTEES:
+// - Result buffer contains ONLY actual file data (no padding/zeros)
+// - newSize is always the correct next read offset (original_from + bytes_read)
+// - Graceful degradation: partial reads return success, allowing incremental progress
+// - Thread-safe: Buffer pool handles concurrent access safely
 func (s *DefaultService) ReadFileRange(
 	ctx context.Context,
 	path string,
@@ -209,13 +233,53 @@ func (s *DefaultService) ReadFileRange(
 			return
 		}
 
-		buf := make([]byte, size-from)
-		if _, err = io.ReadFull(f, buf); err != nil {
-			resCh <- result{nil, 0, err}
-			return
+		// CONTEXT-AWARE TIMING: Check remaining time before each chunk to avoid timeout failures
+		// If less than timeBuffer remains, gracefully exit with partial data instead of timing out
+		timeBuffer := constants.S6FileReadTimeBuffer
+
+		// MEMORY PREALLOCATION: Pre-allocate capacity but start with zero length
+		// This avoids repeated allocations during append() while not wasting memory
+		expectedSize := size - from
+		buf := make([]byte, 0, expectedSize)
+
+		// BUFFER POOL USAGE: Get reusable 1MB buffer to minimize allocations
+		// The buffer gets completely overwritten by io.ReadFull each time
+		smallBuf := chunkBufferPool.Get().([]byte)
+		defer chunkBufferPool.Put(smallBuf)
+
+		for {
+			// GRACEFUL EARLY EXIT: Check if enough time remains for another chunk
+			// Returns SUCCESS with partial data instead of TIMEOUT failure
+			if deadline, ok := ctx.Deadline(); ok {
+				if remaining := time.Until(deadline); remaining < timeBuffer {
+					// NEWSIZE CALCULATION: from + bytes_read = next offset to read from
+					resCh <- result{buf, from + int64(len(buf)), nil}
+					return
+				}
+			}
+
+			// Read chunk: io.ReadFull either reads exactly len(smallBuf) bytes OR returns error
+			n, err := io.ReadFull(f, smallBuf)
+			if err == io.EOF || err == io.ErrUnexpectedEOF {
+				if n > 0 {
+					// PARTIAL READ: Use smallBuf[:n] to avoid appending garbage data
+					// This is why buf contains NO EXTRA ZEROS - we only append actual file data
+					buf = append(buf, smallBuf[:n]...)
+				}
+				break
+			}
+			if err != nil {
+				resCh <- result{nil, 0, err}
+				return
+			}
+
+			// FULL READ SUCCESS: io.ReadFull guarantees smallBuf contains exactly 1MB of file data
+			// No slicing needed here - the entire buffer contains valid data
+			buf = append(buf, smallBuf...)
 		}
 
-		resCh <- result{buf, size, nil}
+		// FINAL RESULT: buf contains only actual file data, newSize = next read offset
+		resCh <- result{buf, from + int64(len(buf)), nil}
 	}()
 
 	select {
