# Copyright 2023 UMH Systems GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
{{if or .Values.bridges.mqtttokafka.enabled .Values._000_commonConfig.datamodel_v2.enabled }}

apiVersion: v1
kind: ConfigMap
metadata:
  # This is the name of the component, it should be unique within the namespace (63 chars max as per RFC 1035/1123)
  name: dfc-1db88f9f-789d-45fa-b095-cf579cc9074d-mqtt-t-kafka-historian
  # To be scheduled in the open-source namespace
  namespace: united-manufacturing-hub
  labels:
    # This does not change over time
    data-flow-component-uuid: 1db88f9f-789d-45fa-b095-cf579cc9074d
    # This is the random UUID of the version of the component
    data-flow-component-version-uuid: 83bd6ace-58f7-4572-9ab4-f8a9237a47dd
    # Timestamp of last change to the component in unix milliseconds since epoch
    data-flow-version: '1724661195274'
    # Generic label to indicate that this is a data flow component
    is-data-flow-component: 'true'
    # Required for mgmtcompanion to manage the component
    managed-by: mgmtcompanion
    # Do not allow mgmtcompanion to modify the component
    data-flow-component-is-read-only: true
    # The type of the component
    data-flow-component-type: bridge
data:
  benthos.yaml: |
    ## Input
    #
    # This configuration bridges _historian messages from MQTT to Kafka
    # According to our _historian schema, these can occur in the following topics:
    # umh/v1/+/_historian/# (enterprise)
    # umh/v1/+/+/_historian/# (enterprise, site)
    # umh/v1/+/+/+/_historian/# (enterprise, site, area)
    # umh/v1/+/+/+/+/_historian/# (enterprise, site, area, productionLine)
    # umh/v1/+/+/+/+/+/_historian/# (enterprise, site, area, productionLine, workCell)
    # umh/v1/+/+/+/+/+/+/_historian/# (enterprise, site, area, productionLine, workCell, originID)
    # We will use a HiveMQ shared subscription ($share/<GROUPID>/TOPIC) to allow scaling the number of consumers

    input:
      mqtt:
        urls:
          - united-manufacturing-hub-mqtt.united-manufacturing-hub.svc.cluster.local:1883
        client_id: "benthos_mqtt_to_kafka"
        dynamic_client_id_suffix: "nanoid"
        auto_replay_nacks: false
        topics:
          - $share/benthos_mqtt_to_kafka/umh/v1/+/_historian/#
          - $share/benthos_mqtt_to_kafka/umh/v1/+/+/_historian/#
          - $share/benthos_mqtt_to_kafka/umh/v1/+/+/+/_historian/#
          - $share/benthos_mqtt_to_kafka/umh/v1/+/+/+/+/_historian/#
          - $share/benthos_mqtt_to_kafka/umh/v1/+/+/+/+/+/_historian/#
          - $share/benthos_mqtt_to_kafka/umh/v1/+/+/+/+/+/+/_historian/#

    ## Processors
    #
    # This part does the following:
    # 0. We check if a bridged-by header already exists, if not we create it.
    # 1. We check if the message was either bridged by us, or the reverse of us (benthos-kafka-to-mqtt-<INSTANCE_NAME>), in any of these cases we discard the message.
    # 2. We convert the MQTT topic name into a Kafka topic name (replace / with .)
    # 3. We slice the topic name into a topic and a key, by splitting it at the 5th dot (so after umh.v1.enterprise.site.area) [This is the same behaviour as the old databridge]
    # 4. Check if the message is a valid JSON object
    # 4a. If the message is not a valid JSON, we still bridge it, but we set a kafka header (parsing-failure="invalid-json").
    # 4b. If the message is a valid JSON, we bridge it as is.
    # 5. In both cases we add a new header to indicate that it was bridged by this benthos instance.
    #    This key also needs to include our instance name, so we can bridge on multiple devices (We get that from Helm).
    #    The key should be an json array of bridges the data was passed through.
    #    bridged-by: ["benthos-mqtt-to-kafka-<INSTANCE_NAME>"]
    # 6. We can finally bridge the message to Kafka

    pipeline:
      processors:
        - bloblang: |
            # Step 2
            let topic = meta("mqtt_topic")
            let converted_topic = $topic.replace("/", ".")
            ## Trim leading or trailing dots
            let trimmed_topic = $converted_topic.trim(".")

            # Step 3
            let topic_parts = $trimmed_topic.split(".")
            let kafka_topic = $topic_parts.slice(0, {{.Values._000_commonConfig.datamodel_v2.merge_point}}).join(".")
            let kafka_key = $topic_parts.slice({{.Values._000_commonConfig.datamodel_v2.merge_point}}).join(".")

            meta "kafka_topic" = $kafka_topic
            meta "kafka_key" = $kafka_key

            let invalid_json = this.parse_json().catch(this.format_json().catch(true))

            ## If not valid json build a new payload
            let encoded = this.format_json().catch(this).encode("base64")

            root = if $invalid_json == true {
              {
                "parsing_failure": "invalid-json",
                "payload": $encoded
              }
            } else {
              root
            }


            # Step 0
            ## Check if bridged-by header exists, if not create it
            let bridged_by = match {
              this.bridged_by => this.bridged_by
              _ => []
            }

            # Step 1
            ## Check if the message was bridged by us, or the reverse of us
            root = if $bridged_by.contains("benthos-kafka-to-mqtt-{{.Values._000_commonConfig.serialNumber}}") || $bridged_by.contains("benthos-mqtt-to-kafka-{{.Values._000_commonConfig.serialNumber}}") {
              deleted()
            } else{
              root
            }

            # Step 4 (a/b)
            let valid_schema = root.json_schema("""{
              "$schema": "http://json-schema.org/draft-07/schema#",
              "type": "object",
              "required": ["timestamp_ms"],
              "properties": {
                "timestamp_ms": {
                  "type": "integer"
                }
              },
              "minProperties": 2,
              "additionalProperties": true
            }
            """).catch(false)

            let parsing_failure = match {
              $valid_schema == false => true
              _ => false
            }

            meta "parsing_failure" = $parsing_failure


            # Step 5

            ## Append our bridge to the bridged-by header
            let bridged_by = $bridged_by.append("benthos-mqtt-to-kafka-{{.Values._000_commonConfig.serialNumber}}")

            ### Transform bridged_by to meta as we are transfering to kafka
            meta "bridged_by" = $bridged_by

    output:
      broker:
        pattern: fan_out
        outputs:
          - kafka_franz:
              topic: ${! meta("kafka_topic") }
              seed_brokers:
                - united-manufacturing-hub-kafka.united-manufacturing-hub.svc.cluster.local:9092
              client_id: mqtt_to_kafka_historian_bridge_{{.Values._000_commonConfig.serialNumber}}
              key: ${! meta("kafka_key") }
              metadata:
                include_prefixes:
                  - kafka_
                  - bridged_
                  - parsing_
  {{ if and (.Values.bridges.mqtttokafka.debug | default false) (.Values.bridges.mqtttokafka.debug.enableStdout | default false) }}
          - stdout:
              codec: lines
  {{ end }}
{{end}}