## Input
#
# This configuration bridges _historian messages from MQTT to Kafka
# According to our _historian schema, these can occur in the following topics:
# umh/v1/+/_historian/# (enterprise)
# umh/v1/+/+/_historian/# (enterprise, site)
# umh/v1/+/+/+/_historian/# (enterprise, site, area)
# umh/v1/+/+/+/+/_historian/# (enterprise, site, area, productionLine)
# umh/v1/+/+/+/+/+/_historian/# (enterprise, site, area, productionLine, workCell)
# umh/v1/+/+/+/+/+/+/_historian/# (enterprise, site, area, productionLine, workCell, originID)
# We will use a HiveMQ shared subscription ($share/<GROUPID>/TOPIC) to allow scaling the number of consumers

input:
  mqtt:
    urls:
      - united-manufacturing-hub-mqtt.united-manufacturing-hub.svc.cluster.local:1883
    client_id: "benthos_mqtt_to_kafka"
    dynamic_client_id_suffix: "nanoid"
    topics:
      - $share/benthos_mqtt_to_kafka/umh/v1/+/_historian/#
      - $share/benthos_mqtt_to_kafka/umh/v1/+/+/_historian/#
      - $share/benthos_mqtt_to_kafka/umh/v1/+/+/+/_historian/#
      - $share/benthos_mqtt_to_kafka/umh/v1/+/+/+/+/_historian/#
      - $share/benthos_mqtt_to_kafka/umh/v1/+/+/+/+/+/_historian/#
      - $share/benthos_mqtt_to_kafka/umh/v1/+/+/+/+/+/+/_historian/#

## Processors
#
# This part does the following:
# 0. We check if a bridged-by header already exists, if not we create it.
# 1. We check if the message was either bridged by us, or the reverse of us (benthos-kafka-to-mqtt-<INSTANCE_NAME>), in any of these cases we discard the message.
# 2. We convert the MQTT topic name into a Kafka topic name (replace / with .)
# 3. We slice the topic name into a topic and a key, by splitting it at the 5th dot (so after umh.v1.enterprise.site.area) [This is the same behaviour as the old databridge]
# 4. Check if the message is a valid JSON object
# 4a. If the message is not a valid JSON, we still bridge it, but we set a kafka header (parsing-failure="invalid-json").
# 4b. If the message is a valid JSON, we bridge it as is.
# 5. In both cases we add a new header to indicate that it was bridged by this benthos instance.
#    This key also needs to include our instance name, so we can bridge on multiple devices (We get that from Helm).
#    The key should be an json array of bridges the data was passed through.
#    bridged-by: ["benthos-mqtt-to-kafka-<INSTANCE_NAME>"]
# 6. We can finally bridge the message to Kafka

pipeline:
  processors:
    - bloblang: |
        # Step 0 processor
        message = this
        message.bridged_by = message.bridged_by.catch([])
        # Check if we are in the bridged_by array
        match {
          this.bridged_by == "benthos-mqtt-to-kafka-<INSTANCE_NAME>" => drop()
        }
