// Copyright 2025 UMH Systems GmbH
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package redpanda_test

const redpandaLogs = `2025-04-14 16:44:30.814074090  
2025-04-14 16:44:30.814083840  
2025-04-14 16:44:30.814085340  Welcome to the Redpanda community!
2025-04-14 16:44:30.814085965  
2025-04-14 16:44:30.814086840  Documentation: https://docs.redpanda.com - Product documentation site
2025-04-14 16:44:30.814087882  GitHub Discussion: https://github.com/redpanda-data/redpanda/discussions - Longer, more involved discussions
2025-04-14 16:44:30.814088674  GitHub Issues: https://github.com/redpanda-data/redpanda/issues - Report and track issues with the codebase
2025-04-14 16:44:30.814089757  Support: https://support.redpanda.com - Contact the support team privately
2025-04-14 16:44:30.814090632  Product Feedback: https://redpanda.com/feedback - Let us know how we can improve your experience
2025-04-14 16:44:30.814091257  Slack: https://redpanda.com/slack - Chat about all things Redpanda. Join the conversation!
2025-04-14 16:44:30.814100174  Twitter: https://twitter.com/redpandadata - All the latest Redpanda news!
2025-04-14 16:44:30.814100465  
2025-04-14 16:44:30.814100715  
2025-04-14 16:44:30.880836965  WARNING: unable to mbind shard memory; performance may suffer: Function not implemented
2025-04-14 16:44:30.881279799  INFO  2025-04-14 16:44:30,880 seastar - Reactor backend: linux-aio
2025-04-14 16:44:30.883923132  INFO  2025-04-14 16:44:30,883 seastar - Perf-based stall detector creation failed (EACCESS), try setting /proc/sys/kernel/perf_event_paranoid to 1 or less to enable kernel backtraces: falling back to posix timer.
2025-04-14 16:44:30.885516590  INFO  2025-04-14 16:44:30,885 cpu_profiler - Perf-based cpu profiler creation failed (EACCESS), try setting /proc/sys/kernel/perf_event_paranoid to 1 or less to enable kernel backtraces: falling back to posix timer.
2025-04-14 16:44:30.902515465  INFO  2025-04-14 16:44:30,902 [shard 0:main] main - application.cc:480 - Redpanda v24.3.8 - b1dd9f54ab1fcd31110608ff214d0937bf30fdb1
2025-04-14 16:44:30.902518424  INFO  2025-04-14 16:44:30,902 [shard 0:main] main - application.cc:481 - Command line: /opt/redpanda/bin/redpanda --redpanda-cfg /run/service/redpanda/config/redpanda.yaml --memory 2G --smp 1
2025-04-14 16:44:30.902519340  INFO  2025-04-14 16:44:30,902 [shard 0:main] main - application.cc:489 - kernel=6.10.14-linuxkit, nodename=1fe89860001d, machine=aarch64
2025-04-14 16:44:30.902790715  INFO  2025-04-14 16:44:30,902 [shard 0:main] main - application.cc:400 - System resources: { cpus: 1, available memory: 2.000GiB, reserved memory: 1.500GiB}
2025-04-14 16:44:30.902792257  INFO  2025-04-14 16:44:30,902 [shard 0:main] main - application.cc:408 - File handle limit: 1048576/1048576
2025-04-14 16:44:30.924548215  INFO  2025-04-14 16:44:30,924 [shard 0:main] cluster - config_manager.cc:510 - Can't load config cache: std::__1::__fs::filesystem::filesystem_error (error system:2, filesystem error: open failed: No such file or directory ["/data/redpanda/config_cache.yaml"])
2025-04-14 16:44:30.925064799  INFO  2025-04-14 16:44:30,925 [shard 0:main] cluster - config_manager.cc:450 - Can't load config bootstrap file: std::__1::__fs::filesystem::filesystem_error (error system:2, filesystem error: open failed: No such file or directory ["/run/service/redpanda/config/.bootstrap.yaml"])
2025-04-14 16:44:30.925311632  INFO  2025-04-14 16:44:30,925 [shard 0:main] main - application.cc:896 - Cluster configuration properties:
2025-04-14 16:44:30.925312632  INFO  2025-04-14 16:44:30,925 [shard 0:main] main - application.cc:897 - (use ` + "`" + `rpk cluster config edit` + "`" + ` to change)
2025-04-14 16:44:30.926667007  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.abort_index_segment_size:50000	- Capacity (in number of txns) of an abort index segment. Each partition tracks the aborted transaction offset ranges to help service client requests.If the number transactions increase beyond this threshold, they are flushed to disk to easy memory pressure.Then they're loaded on demand. This configuration controls the maximum number of aborted transactions  before they are flushed to disk.
2025-04-14 16:44:30.926668715  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.abort_timed_out_transactions_interval_ms:10000	- Interval, in milliseconds, at which Redpanda looks for inactive transactions and aborts them.
2025-04-14 16:44:30.926669715  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.admin_api_require_auth:0	- Whether Admin API clients must provide HTTP basic authentication headers.
2025-04-14 16:44:30.926671507  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.aggregate_metrics:0	- Enable aggregation of metrics returned by the ` + "`" + `/metrics` + "`" + ` endpoint. Aggregation can simplify monitoring by providing summarized data instead of raw, per-instance metrics. Metric aggregation is performed by summing the values of samples by labels and is done when it makes sense by the shard and/or partition labels.
2025-04-14 16:44:30.926682882  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.alive_timeout_ms:5000	- The amount of time since the last broker status heartbeat. After this time, a broker is considered offline and not alive.
2025-04-14 16:44:30.926684632  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.alter_topic_cfg_timeout_ms:5000	- The duration, in milliseconds, that Redpanda waits for the replication of entries in the controller log when executing a request to alter topic configurations. This timeout ensures that configuration changes are replicated across the cluster before the alteration request is considered complete.
2025-04-14 16:44:30.926695465  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.append_chunk_size:16384	- Size of direct write operations to disk in bytes. A larger chunk size can improve performance for write-heavy workloads, but increase latency for these writes as more data is collected before each write operation. A smaller chunk size can decrease write latency, but potentially increase the number of disk I/O operations.
2025-04-14 16:44:30.926703090  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.audit_client_max_buffer_size:16777216	- Defines the number of bytes allocated by the internal audit client for audit messages. When changing this, you must disable audit logging and then re-enable it for the change to take effect. Consider increasing this if your system generates a very large number of audit records in a short amount of time.
2025-04-14 16:44:30.926704424  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.audit_enabled:0	- Enables or disables audit logging. When you set this to true, Redpanda checks for an existing topic named ` + "`" + `_redpanda.audit_log` + "`" + `. If none is found, Redpanda automatically creates one for you.
2025-04-14 16:44:30.926705965  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.audit_enabled_event_types:{management, authenticate, admin}	- List of strings in JSON style identifying the event types to include in the audit log. This may include any of the following: ` + "`" + `management, produce, consume, describe, heartbeat, authenticate, schema_registry, admin` + "`" + `.
2025-04-14 16:44:30.926717090  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.audit_excluded_principals:{}	- List of user principals to exclude from auditing.
2025-04-14 16:44:30.926717924  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.audit_excluded_topics:{}	- List of topics to exclude from auditing.
2025-04-14 16:44:30.926719424  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.audit_log_num_partitions:12	- Defines the number of partitions used by a newly-created audit topic. This configuration applies only to the audit log topic and may be different from the cluster or other topic configurations. This cannot be altered for existing audit log topics.
2025-04-14 16:44:30.926721590  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.audit_log_replication_factor:{nullopt}	- Defines the replication factor for a newly-created audit log topic. This configuration applies only to the audit log topic and may be different from the cluster or other topic configurations. This cannot be altered for existing audit log topics. Setting this value is optional. If a value is not provided, Redpanda will use the value specified for ` + "`" + `internal_topic_replication_factor` + "`" + `.
2025-04-14 16:44:30.926723299  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.audit_queue_drain_interval_ms:500	- Interval, in milliseconds, at which Redpanda flushes the queued audit log messages to the audit log topic. Longer intervals may help prevent duplicate messages, especially in high throughput scenarios, but they also increase the risk of data loss during shutdowns where the queue is lost.
2025-04-14 16:44:30.926730424  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.audit_queue_max_buffer_size_per_shard:1048576	- Defines the maximum amount of memory in bytes used by the audit buffer in each shard. Once this size is reached, requests to log additional audit messages will return a non-retryable error. Limiting the buffer size per shard helps prevent any single shard from consuming excessive memory due to audit log messages.
2025-04-14 16:44:30.927062465  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.auto_create_topics_enabled:1	- Allow automatic topic creation. To prevent excess topics, this property is not supported on Redpanda Cloud BYOC and Dedicated clusters. You should explicitly manage topic creation for these Redpanda Cloud clusters. If you produce to a topic that doesn't exist, the topic will be created with defaults if this property is enabled.
2025-04-14 16:44:30.927064549  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_access_key:{nullopt}	- AWS or GCP access key. This access key is part of the credentials that Redpanda requires to authenticate with object storage services for Tiered Storage. This access key is used with the <<cloud_storage_secret_key>> to form the complete credentials required for authentication. To authenticate using IAM roles, see cloud_storage_credentials_source.
2025-04-14 16:44:30.927066049  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_api_endpoint:{nullopt}	- Optional API endpoint. - AWS: When blank, this is automatically generated using <<cloud_storage_region,region>> and <<cloud_storage_bucket,bucket>>. Otherwise, this uses the value assigned. - GCP: Uses ` + "`" + `storage.googleapis.com` + "`" + `.
2025-04-14 16:44:30.927066757  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_api_endpoint_port:443	- TLS port override.
2025-04-14 16:44:30.927068674  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_attempt_cluster_restore_on_bootstrap:0	- When set to ` + "`" + `true` + "`" + `, Redpanda automatically retrieves cluster metadata from a specified object storage bucket at the cluster's first startup. This option is ideal for orchestrated deployments, such as Kubernetes. Ensure any previous cluster linked to the bucket is fully decommissioned to prevent conflicts between Tiered Storage subsystems.
2025-04-14 16:44:30.927069882  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_azure_adls_endpoint:{nullopt}	- Azure Data Lake Storage v2 endpoint override. Use when hierarchical namespaces are enabled on your storage account and you have set up a custom endpoint.
2025-04-14 16:44:30.927071257  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_azure_adls_port:{nullopt}	- Azure Data Lake Storage v2 port override. See also ` + "`" + `cloud_storage_azure_adls_endpoint` + "`" + `. Use when Hierarchical Namespaces are enabled on your storage account and you have set up a custom endpoint.
2025-04-14 16:44:30.927072465  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_azure_container:{nullopt}	- The name of the Azure container to use with Tiered Storage. If ` + "`" + `null` + "`" + `, the property is disabled. The container must belong to cloud_storage_azure_storage_account.
2025-04-14 16:44:30.927075424  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_azure_hierarchical_namespace_enabled:{nullopt}	- Whether or not an Azure hierarchical namespace is enabled on the ` + "`" + `cloud_storage_azure_storage_account` + "`" + `. If this property is not set, ´cloud_storage_azure_shared_key` + "`" + ` must be set, and each node tries to determine at startup if a hierarchical namespace is enabled. Setting this property to ` + "`" + `true` + "`" + ` disables the check and treats a hierarchical namespace as active. Setting to ` + "`" + `false` + "`" + ` disables the check and treats a hierarchical namespace as not active.
2025-04-14 16:44:30.927076799  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_azure_managed_identity_id:{nullopt}	- The managed identity ID to use for access to the Azure storage account. To use Azure managed identities, you must set ` + "`" + `cloud_storage_credentials_source` + "`" + ` to ` + "`" + `azure_vm_instance_metadata` + "`" + `.
2025-04-14 16:44:30.927078340  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_azure_shared_key:{nullopt}	- The shared key to be used for Azure Shared Key authentication with the Azure storage account configured by ` + "`" + `cloud_storage_azure_storage_account` + "`" + `.  If ` + "`" + `null` + "`" + `, the property is disabled. Redpanda expects this key string to be Base64 encoded.
2025-04-14 16:44:30.927079382  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_azure_storage_account:{nullopt}	- The name of the Azure storage account to use with Tiered Storage. If ` + "`" + `null` + "`" + `, the property is disabled.
2025-04-14 16:44:30.927080799  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_backend:unknown	- Optional object storage backend variant used to select API capabilities. If not supplied, this will be inferred from other configuration properties. Accepted values: [` + "`" + `unknown` + "`" + `, ` + "`" + `aws` + "`" + `, ` + "`" + `google_s3_compat` + "`" + `, ` + "`" + `azure` + "`" + `, ` + "`" + `minio` + "`" + `]
2025-04-14 16:44:30.927082257  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_background_jobs_quota:5000	- The total number of requests the object storage background jobs can make during one background housekeeping run. This is a per-shard limit. Adjusting this limit can optimize object storage traffic and impact shard performance.
2025-04-14 16:44:30.927083090  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_bucket:{nullopt}	- AWS or GCP bucket or container that should be used to store data.
2025-04-14 16:44:30.927085382  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_cache_check_interval:5000	- Minimum interval between Tiered Storage cache trims, measured in milliseconds. This setting dictates the cooldown period after a cache trim operation before another trim can occur. If a cache fetch operation requests a trim but the interval since the last trim has not yet passed, the trim will be postponed until this cooldown expires. Adjusting this interval helps manage the balance between cache size and retrieval performance.
2025-04-14 16:44:30.927086549  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_cache_chunk_size:16777216	- Size of chunks of segments downloaded into object storage cache. Reduces space usage by only downloading the necessary chunk from a segment.
2025-04-14 16:44:30.927087965  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_cache_max_objects:100000	- Maximum number of objects that may be held in the Tiered Storage cache.  This applies simultaneously with ` + "`" + `cloud_storage_cache_size` + "`" + `, and whichever limit is hit first will trigger trimming of the cache.
2025-04-14 16:44:30.927089257  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_cache_num_buckets:0	- Divide the object storage cache across the specified number of buckets. This only works for objects with randomized prefixes. The names are not changed when the value is set to zero.
2025-04-14 16:44:30.927090340  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_cache_size:0	- Maximum size of object storage cache. If both this property and cloud_storage_cache_size_percent are set, Redpanda uses the minimum of the two.
2025-04-14 16:44:30.927123215  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_cache_size_percent:{20}	- Maximum size of the cloud cache as a percentage of unreserved disk space disk_reservation_percent. The default value for this option is tuned for a shared disk configuration. Consider increasing the value if using a dedicated cache disk. The property <<cloud_storage_cache_size,` + "`" + `cloud_storage_cache_size` + "`" + `>> controls the same limit expressed as a fixed number of bytes. If both ` + "`" + `cloud_storage_cache_size` + "`" + ` and ` + "`" + `cloud_storage_cache_size_percent` + "`" + ` are set, Redpanda uses the minimum of the two.
2025-04-14 16:44:30.927125299  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_cache_trim_carryover_bytes:0	- The cache performs a recursive directory inspection during the cache trim. The information obtained during the inspection can be carried over to the next trim operation. This parameter sets a limit on the memory occupied by objects that can be carried over from one trim to next, and allows cache to quickly unblock readers before starting the directory inspection (deprecated)
2025-04-14 16:44:30.927220382  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_cache_trim_threshold_percent_objects:{nullopt}	- Trim is triggered when the cache reaches this percent of the maximum object count. If this is unset, the default behavioris to start trim when the cache is about 100% full.
2025-04-14 16:44:30.927221840  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_cache_trim_threshold_percent_size:{nullopt}	- Trim is triggered when the cache reaches this percent of the maximum cache size. If this is unset, the default behavioris to start trim when the cache is about 100% full.
2025-04-14 16:44:30.927223340  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_cache_trim_walk_concurrency:1	- The maximum number of concurrent tasks launched for directory walk during cache trimming. A higher number allows cache trimming to run faster but can cause latency spikes due to increased pressure on I/O subsystem and syscall threads.
2025-04-14 16:44:30.927224174  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_chunk_eviction_strategy:eager	- Selects a strategy for evicting unused cache chunks.
2025-04-14 16:44:30.927225007  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_chunk_prefetch:0	- Number of chunks to prefetch ahead of every downloaded chunk
2025-04-14 16:44:30.927226340  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_cluster_metadata_num_consumer_groups_per_upload:1000	- Number of groups to upload in a single snapshot object during consumer offsets upload. Setting a lower value will mean a larger number of smaller snapshots are uploaded.
2025-04-14 16:44:30.927227174  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_cluster_metadata_retries:5	- Number of attempts metadata operations may be retried.
2025-04-14 16:44:30.927228049  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_cluster_metadata_upload_interval_ms:3600000	- Time interval to wait between cluster metadata uploads.
2025-04-14 16:44:30.927228882  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_cluster_metadata_upload_timeout_ms:60000	- Timeout for cluster metadata uploads.
2025-04-14 16:44:30.927230382  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_credentials_host:{nullopt}	- The hostname to connect to for retrieving role based credentials. Derived from cloud_storage_credentials_source if not set. Only required when using IAM role based access. To authenticate using access keys, see ` + "`" + `cloud_storage_access_key` + "`" + `.
2025-04-14 16:44:30.927232299  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_credentials_source:config_file	- The source of credentials used to authenticate to object storage services. Required for cluster provider authentication with IAM roles. To authenticate using access keys, see cloud_storage_access_key` + "`" + `. Accepted values: ` + "`" + `config_file` + "`" + `, ` + "`" + `aws_instance_metadata` + "`" + `, ` + "`" + `sts, gcp_instance_metadata` + "`" + `, ` + "`" + `azure_vm_instance_metadata` + "`" + `, ` + "`" + `azure_aks_oidc_federation` + "`" + ` 
2025-04-14 16:44:30.927233132  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_crl_file:{nullopt}	- Path to certificate revocation list for ` + "`" + `cloud_storage_trust_file` + "`" + `.
2025-04-14 16:44:30.927234007  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_disable_archiver_manager:1	- Use legacy upload mode and do not start archiver_manager.
2025-04-14 16:44:30.927234924  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_disable_chunk_reads:0	- Disable chunk reads and switch back to legacy mode where full segments are downloaded.
2025-04-14 16:44:30.927236174  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_disable_metadata_consistency_checks:1	- Disable all metadata consistency checks. This will allow redpanda to replay logs with inconsistent tiered-storage metadata. Normally, this option should be disabled.
2025-04-14 16:44:30.927237382  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_disable_read_replica_loop_for_tests:0	- Begins the read replica sync loop in tiered-storage-enabled topic partitions. The property exists to simplify testing and shouldn't be set in production.
2025-04-14 16:44:30.927238715  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_disable_remote_labels_for_tests:0	- If 'true', Redpanda disables remote labels and falls back on the hash-based object naming scheme for new topics. This property exists to simplify testing and shouldn't be set in production.
2025-04-14 16:44:30.927239465  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_disable_tls:0	- Disable TLS for all object storage connections.
2025-04-14 16:44:30.927240799  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_disable_upload_consistency_checks:0	- Disable all upload consistency checks. This will allow redpanda to upload logs with gaps and replicate metadata with consistency violations. Normally, this options should be disabled.
2025-04-14 16:44:30.927241965  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_disable_upload_loop_for_tests:0	- Begins the upload loop in tiered-storage-enabled topic partitions. The property exists to simplify testing and shouldn't be set in production.
2025-04-14 16:44:30.927242799  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_enable_compacted_topic_reupload:1	- Enable re-uploading data for compacted topics
2025-04-14 16:44:30.927243590  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_enable_remote_read:0	- Default remote read config value for new topics
2025-04-14 16:44:30.927244340  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_enable_remote_write:0	- Default remote write value for new topics
2025-04-14 16:44:30.927245424  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_enable_scrubbing:0	- Enable scrubbing of cloud storage partitions. The scrubber validates the integrity of data and metadata uploaded to cloud storage.
2025-04-14 16:44:30.927246590  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_enable_segment_merging:1	- Enables adjacent segment merging. The segments are reuploaded if there is an opportunity for that and if it will improve the tiered-storage performance
2025-04-14 16:44:30.927247507  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_enabled:0	- Enable object storage. Must be set to ` + "`" + `true` + "`" + ` to use Tiered Storage or Remote Read Replicas.
2025-04-14 16:44:30.927248299  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_full_scrub_interval_ms:43200000	- Time interval between a final scrub and the next.
2025-04-14 16:44:30.927249174  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_garbage_collect_timeout_ms:30000	- Timeout for running the cloud storage garbage collection (ms).
2025-04-14 16:44:30.927250424  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_graceful_transfer_timeout_ms:{5000}	- Time limit on waiting for uploads to complete before a leadership transfer.  If this is null, leadership transfers will proceed without waiting.
2025-04-14 16:44:30.927251215  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_housekeeping_interval_ms:300000	- Interval for cloud storage housekeeping tasks.
2025-04-14 16:44:30.927252340  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_hydrated_chunks_per_segment_ratio:0.7	- The maximum number of chunks per segment that can be hydrated at a time. Above this number, unused chunks will be trimmed.
2025-04-14 16:44:30.927253549  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_hydration_timeout_ms:600000	- Duration to wait for a hydration request to be fulfilled, if hydration is not completed within this time, the consumer will be notified with a timeout error.
2025-04-14 16:44:30.927254882  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_idle_threshold_rps:10	- The cloud storage request rate threshold for idle state detection. If the average request rate for the configured period is lower than this threshold the cloud storage is considered being idle.
2025-04-14 16:44:30.927325465  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_idle_timeout_ms:10000	- Timeout used to detect idle state of the cloud storage API. If the average cloud storage request rate is below this threshold for a configured amount of time the cloud storage is considered idle and the housekeeping jobs are started.
2025-04-14 16:44:30.927326424  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_initial_backoff_ms:100	- Initial backoff time for exponential backoff algorithm (ms)
2025-04-14 16:44:30.927327549  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_based_scrub_enabled:0	- Scrubber uses the latest cloud storage inventory report, if available, to check if the required objects exist in the bucket or container.
2025-04-14 16:44:30.927328590  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_id:redpanda_scrubber_inventory	- The name of the scheduled inventory job created by Redpanda to generate bucket or container inventory reports.
2025-04-14 16:44:30.927330257  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_max_hash_size_during_parse:67108864	- Maximum bytes of hashes which will be held in memory before writing data to disk during inventory report parsing. Affects the number of files written by inventory service to disk during report parsing, as when this limit is reached new files are written to disk.
2025-04-14 16:44:30.927331299  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_report_check_interval_ms:21600000	- Time interval between checks for a new inventory report in the cloud storage bucket or container.
2025-04-14 16:44:30.927332424  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_reports_prefix:redpanda_scrubber_inventory	- The prefix to the path in the cloud storage bucket or container where inventory reports will be placed.
2025-04-14 16:44:30.927334340  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_self_managed_report_config:0	- If enabled, Redpanda will not attempt to create the scheduled report configuration using cloud storage APIs. The scrubbing process will look for reports in the expected paths in the bucket or container, and use the latest report found. Primarily intended for use in testing and on backends where scheduled inventory reports are not supported.
2025-04-14 16:44:30.927335215  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_manifest_cache_size:1048576	- Amount of memory that can be used to handle tiered-storage metadata
2025-04-14 16:44:30.927336674  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_manifest_max_upload_interval_sec:{60000}	- Wait at least this long between partition manifest uploads. Actual time between uploads may be greater than this interval. If this property is not set, or null, metadata will be updated after each segment upload.
2025-04-14 16:44:30.927337465  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_manifest_upload_timeout_ms:10000	- Manifest upload timeout (ms).
2025-04-14 16:44:30.927340090  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_materialized_manifest_ttl_ms:10000	- The time interval that determines how long the materialized manifest can stay in cache under contention. This parameter is used for performance tuning. When the spillover manifest is materialized and stored in cache and the cache needs to evict it it will use 'cloud_storage_materialized_manifest_ttl_ms' value as a timeout. The cursor that uses the spillover manifest uses this value as a TTL interval after which it stops referencing the manifest making it available for eviction. This only affects spillover manifests under contention.
2025-04-14 16:44:30.927353132  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_max_concurrent_hydrations_per_shard:{nullopt}	- Maximum concurrent segment hydrations of remote data per CPU core.  If unset, value of ` + "`" + `cloud_storage_max_connections / 2` + "`" + ` is used, which means that half of available S3 bandwidth could be used to download data from S3. If the cloud storage cache is empty every new segment reader will require a download. This will lead to 1:1 mapping between number of partitions scanned by the fetch request and number of parallel downloads. If this value is too large the downloads can affect other workloads. In case of any problem caused by the tiered-storage reads this value can be lowered. This will only affect segment hydrations (downloads) but won't affect cached segments. If fetch request is reading from the tiered-storage cache its concurrency will only be limited by available memory.
2025-04-14 16:44:30.927354132  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_max_connection_idle_time_ms:5000	- Max https connection idle time (ms)
2025-04-14 16:44:30.927355132  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_max_connections:20	- Maximum simultaneous object storage connections per shard, applicable to upload and download activities.
2025-04-14 16:44:30.927356299  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_max_materialized_segments_per_shard:{nullopt}	- Maximum concurrent readers of remote data per CPU core.  If unset, value of ` + "`" + `topic_partitions_per_shard` + "`" + ` multiplied by 2 is used.
2025-04-14 16:44:30.927357174  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_max_partition_readers_per_shard:{nullopt}	- Maximum partition readers per shard (deprecated)
2025-04-14 16:44:30.927359007  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_max_segment_readers_per_shard:{nullopt}	- Maximum concurrent I/O cursors of materialized remote segments per CPU core.  If unset, value of ` + "`" + `topic_partitions_per_shard` + "`" + ` is used, i.e. one segment reader per partition if the shard is at its maximum partition capacity.  These readers are cachedacross Kafka consume requests and store a readahead buffer.
2025-04-14 16:44:30.927360674  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_max_segments_pending_deletion_per_partition:5000	- The per-partition limit for the number of segments pending deletion from the cloud. Segments can be deleted due to retention or compaction. If this limit is breached and deletion fails, then segments will be orphaned in the cloud and will have to be removed manually
2025-04-14 16:44:30.927362840  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_max_throughput_per_shard:{1073741824}	- Max throughput used by tiered-storage per shard in bytes per second. This value is an upper bound of the throughput available to the tiered-storage subsystem. This parameter is intended to be used as a safeguard and in tests when we need to set precise throughput value independent of actual storage media. Please use 'cloud_storage_throughput_limit_percent' instead of this parameter in the production environment.
2025-04-14 16:44:30.927363632  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_metadata_sync_timeout_ms:10000	- Timeout for SI metadata synchronization.
2025-04-14 16:44:30.927365090  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_min_chunks_per_segment_threshold:5	- The minimum number of chunks per segment for trimming to be enabled. If the number of chunks in a segment is below this threshold, the segment is small enough that all chunks in it can be hydrated at any given time
2025-04-14 16:44:30.927365965  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_partial_scrub_interval_ms:3600000	- Time interval between two partial scrubs of the same partition.
2025-04-14 16:44:30.927366965  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_readreplica_manifest_sync_timeout_ms:30000	- Timeout to check if new data is available for partition in S3 for read replica.
2025-04-14 16:44:30.927367590  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_reconciliation_interval_ms:	- 
2025-04-14 16:44:30.927368590  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_recovery_temporary_retention_bytes_default:1073741824	- Retention in bytes for topics created during automated recovery
2025-04-14 16:44:30.927460257  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_recovery_topic_validation_depth:10	- Number of metadata segments to validate, from newest to oldest, when ` + "`" + `cloud_storage_recovery_topic_validation_mode` + "`" + ` is set to ` + "`" + `check_manifest_and_segment_metadata` + "`" + `.
2025-04-14 16:44:30.927464257  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_recovery_topic_validation_mode:check_manifest_existence	- Validation performed before recovering a topic from object storage. In case of failure, the reason for the failure appears as ` + "`" + `ERROR` + "`" + ` lines in the Redpanda application log. For each topic, this reports errors for all partitions, but for each partition, only the first error is reported. This property accepts the following parameters: ` + "`" + `no_check` + "`" + `: Skips the checks for topic recovery. ` + "`" + `check_manifest_existence` + "`" + `:  Runs an existence check on each ` + "`" + `partition_manifest` + "`" + `. Fails if there are connection issues to the object storage. ` + "`" + `check_manifest_and_segment_metadata` + "`" + `: Downloads the manifest and runs a consistency check, comparing the metadata with the cloud storage objects. The process fails if metadata references any missing cloud storage objects.
2025-04-14 16:44:30.927465340  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_region:{nullopt}	- Cloud provider region that houses the bucket or container used for storage.
2025-04-14 16:44:30.927466174  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_roles_operation_timeout_ms:30000	- Timeout for IAM role related operations (ms)
2025-04-14 16:44:30.927467090  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_scrubbing_interval_jitter_ms:600000	- Jitter applied to the cloud storage scrubbing interval.
2025-04-14 16:44:30.927467840  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_secret_key:{nullopt}	- Cloud provider secret key.
2025-04-14 16:44:30.927468882  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_segment_max_upload_interval_sec:{3600000}	- Time that segment can be kept locally without uploading it to the remote storage (sec).
2025-04-14 16:44:30.927469924  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_segment_size_min:{nullopt}	- Smallest acceptable segment size in the cloud storage. Default: cloud_storage_segment_size_target/2
2025-04-14 16:44:30.927470799  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_segment_size_target:{nullopt}	- Desired segment size in the cloud storage. Default: segment.bytes
2025-04-14 16:44:30.927471590  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_segment_upload_timeout_ms:30000	- Log segment upload timeout (ms)
2025-04-14 16:44:30.927473590  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_spillover_manifest_max_segments:{nullopt}	- Maximum number of elements in the spillover manifest that can be offloaded to the cloud storage. This property is similar to 'cloud_storage_spillover_manifest_size' but it triggers spillover based on number of segments instead of the size of the manifest in bytes. The property exists to simplify testing and shouldn't be set in the production environment
2025-04-14 16:44:30.927475257  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_spillover_manifest_size:{65536}	- The size of the manifest which can be offloaded to the cloud. If the size of the local manifest stored in redpanda exceeds cloud_storage_spillover_manifest_size x2 the spillover mechanism will split the manifest into two parts and one of them will be uploaded to S3.
2025-04-14 16:44:30.927477965  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_throughput_limit_percent:{50}	- Max throughput used by tiered-storage per node expressed as a percentage of the disk bandwidth. If the server has several disks Redpanda will take into account only the one which is used to store tiered-storage cache. Note that even if the tiered-storage is allowed to use full bandwidth of the disk (100%) it won't necessary use it in full. The actual usage depend on your workload and the state of the tiered-storage cache. This parameter is a safeguard that prevents tiered-storage from using too many system resources and not a performance tuning knob.
2025-04-14 16:44:30.927478882  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_topic_purge_grace_period_ms:30000	- Grace period during which the purger will refuse to purge the topic.
2025-04-14 16:44:30.927479882  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_trust_file:{nullopt}	- Path to certificate that should be used to validate server certificate during TLS handshake.
2025-04-14 16:44:30.927480632  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_upload_ctrl_d_coeff:0	- derivative coefficient for upload PID controller.
2025-04-14 16:44:30.927481507  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_upload_ctrl_max_shares:1000	- maximum number of IO and CPU shares that archival upload can use
2025-04-14 16:44:30.927482424  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_upload_ctrl_min_shares:100	- minimum number of IO and CPU shares that archival upload can use
2025-04-14 16:44:30.927483257  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_upload_ctrl_p_coeff:-2	- proportional coefficient for upload PID controller
2025-04-14 16:44:30.927483924  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_upload_ctrl_update_interval_ms:60000	- 
2025-04-14 16:44:30.927484924  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_upload_loop_initial_backoff_ms:100	- Initial backoff interval when there is nothing to upload for a partition (ms).
2025-04-14 16:44:30.927485840  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_upload_loop_max_backoff_ms:10000	- Max backoff interval when there is nothing to upload for a partition (ms).
2025-04-14 16:44:30.927488840  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_url_style:{nullopt}	- Specifies the addressing style to use for Amazon S3 requests. This configuration determines how S3 bucket URLs are formatted. You can choose between: ` + "`" + `virtual_host` + "`" + `, (for example, ` + "`" + `<bucket-name>.s3.amazonaws.com` + "`" + `), ` + "`" + `path` + "`" + `, (for example, ` + "`" + `s3.amazonaws.com/<bucket-name>` + "`" + `), and ` + "`" + `null` + "`" + `. Path style is supported for backward compatibility with legacy systems. When this property is not set or is ` + "`" + `null` + "`" + `, the client tries to use ` + "`" + `virtual_host` + "`" + ` addressing. If the initial request fails, the client automatically tries the ` + "`" + `path` + "`" + ` style. If neither addressing style works, Redpanda terminates the startup, requiring manual configuration to proceed.
2025-04-14 16:44:30.927489507  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cluster_id:{nullopt}	- Cluster identifier.
2025-04-14 16:44:30.927490340  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.compacted_log_segment_size:268435456	- Size (in bytes) for each compacted log segment.
2025-04-14 16:44:30.927491424  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.compaction_ctrl_backlog_size:{nullopt}	- Target backlog size for compaction controller. If not set the max backlog size is configured to 80% of total disk space available.
2025-04-14 16:44:30.927492257  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.compaction_ctrl_d_coeff:0.2	- Derivative coefficient for compaction PID controller.
2025-04-14 16:44:30.927493007  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.compaction_ctrl_i_coeff:0	- Integral coefficient for compaction PID controller.
2025-04-14 16:44:30.927493882  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.compaction_ctrl_max_shares:1000	- Maximum number of I/O and CPU shares that compaction process can use.
2025-04-14 16:44:30.927494799  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.compaction_ctrl_min_shares:10	- Minimum number of I/O and CPU shares that compaction process can use.
2025-04-14 16:44:30.927496049  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.compaction_ctrl_p_coeff:-12.5	- Proportional coefficient for compaction PID controller. This must be negative, because the compaction backlog should decrease when the number of compaction shares increases.
2025-04-14 16:44:30.927578299  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.compaction_ctrl_update_interval_ms:30000	- 
2025-04-14 16:44:30.927579424  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.controller_backend_housekeeping_interval_ms:1000	- Interval between iterations of controller backend housekeeping loop.
2025-04-14 16:44:30.927580549  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.controller_log_accummulation_rps_capacity_acls_and_users_operations:{nullopt}	- Maximum capacity of rate limit accumulation in controller ACLs and users operations limit.
2025-04-14 16:44:30.927581632  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.controller_log_accummulation_rps_capacity_configuration_operations:{nullopt}	- Maximum capacity of rate limit accumulation in controller configuration operations limit.
2025-04-14 16:44:30.927582674  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.controller_log_accummulation_rps_capacity_move_operations:{nullopt}	- Maximum capacity of rate limit accumulation in controller move operations limit.
2025-04-14 16:44:30.927583799  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.controller_log_accummulation_rps_capacity_node_management_operations:{nullopt}	- Maximum capacity of rate limit accumulation in controller node management operations limit.
2025-04-14 16:44:30.927584799  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.controller_log_accummulation_rps_capacity_topic_operations:{nullopt}	- Maximum capacity of rate limit accumulationin controller topic operations limit
2025-04-14 16:44:30.927585882  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.controller_snapshot_max_age_sec:60000	- Maximum amount of time before Redpanda attempts to create a controller snapshot after a new controller command appears.
2025-04-14 16:44:30.927586465  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.coproc_max_batch_size:	- 
2025-04-14 16:44:30.927587049  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.coproc_max_inflight_bytes:	- 
2025-04-14 16:44:30.927587632  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.coproc_max_ingest_bytes:	- 
2025-04-14 16:44:30.927588257  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.coproc_offset_flush_interval_ms:	- 
2025-04-14 16:44:30.927589215  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.core_balancing_continuous:0	- If set to ` + "`" + `true` + "`" + `, move partitions between cores in runtime to maintain balanced partition distribution.
2025-04-14 16:44:30.927590132  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.core_balancing_debounce_timeout:10000	- Interval, in milliseconds, between trigger and invocation of core balancing.
2025-04-14 16:44:30.927591340  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.core_balancing_on_core_count_change:1	- If set to ` + "`" + `true` + "`" + `, and if after a restart the number of cores changes, Redpanda will move partitions between cores to maintain balanced partition distribution.
2025-04-14 16:44:30.927592049  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cpu_profiler_enabled:0	- Enables CPU profiling for Redpanda.
2025-04-14 16:44:30.927592840  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.cpu_profiler_sample_period_ms:100	- The sample period for the CPU profiler.
2025-04-14 16:44:30.927593674  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.create_topic_timeout_ms:2000	- Timeout, in milliseconds, to wait for new topic creation.
2025-04-14 16:44:30.927594590  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.data_transforms_binary_max_size:10485760	- The maximum size for a deployable WebAssembly binary that the broker can store.
2025-04-14 16:44:30.927595424  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.data_transforms_commit_interval_ms:3000	- The commit interval at which data transforms progress.
2025-04-14 16:44:30.927597507  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.data_transforms_enabled:0	- Enables WebAssembly-powered data transforms directly in the broker. When ` + "`" + `data_transforms_enabled` + "`" + ` is set to ` + "`" + `true` + "`" + `, Redpanda reserves memory for data transforms, even if no transform functions are currently deployed. This memory reservation ensures that adequate resources are available for transform functions when they are needed, but it also means that some memory is allocated regardless of usage.
2025-04-14 16:44:30.927598799  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.data_transforms_logging_buffer_capacity_bytes:512000	- Buffer capacity for transform logs, per shard. Buffer occupancy is calculated as the total size of buffered log messages; that is, logs emitted but not yet produced.
2025-04-14 16:44:30.927599965  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.data_transforms_logging_flush_interval_ms:500	- Flush interval for transform logs. When a timer expires, pending logs are collected and published to the ` + "`" + `transform_logs` + "`" + ` topic.
2025-04-14 16:44:30.927600924  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.data_transforms_logging_line_max_bytes:1024	- Transform log lines truncate to this length. Truncation occurs after any character escaping.
2025-04-14 16:44:30.927602632  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.data_transforms_per_core_memory_reservation:20971520	- The amount of memory to reserve per core for data transform (Wasm) virtual machines. Memory is reserved on boot. The maximum number of functions that can be deployed to a cluster is equal to ` + "`" + `data_transforms_per_core_memory_reservation` + "`" + ` / ` + "`" + `data_transforms_per_function_memory_limit` + "`" + `.
2025-04-14 16:44:30.927604215  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.data_transforms_per_function_memory_limit:2097152	- The amount of memory to give an instance of a data transform (Wasm) virtual machine. The maximum number of functions that can be deployed to a cluster is equal to ` + "`" + `data_transforms_per_core_memory_reservation` + "`" + ` / ` + "`" + `data_transforms_per_function_memory_limit` + "`" + `.
2025-04-14 16:44:30.927605215  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.data_transforms_read_buffer_memory_percentage:45	- The percentage of available memory in the transform subsystem to use for read buffers.
2025-04-14 16:44:30.927606299  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.data_transforms_runtime_limit_ms:3000	- The maximum amount of runtime to start up a data transform, and the time it takes for a single record to be transformed.
2025-04-14 16:44:30.927607257  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.data_transforms_write_buffer_memory_percentage:45	- The percentage of available memory in the transform subsystem to use for write buffers.
2025-04-14 16:44:30.927608299  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.datalake_coordinator_snapshot_max_delay_secs:900000	- Maximum amount of time the coordinator waits to snapshot after a command appears in the log.
2025-04-14 16:44:30.927609465  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.debug_bundle_auto_removal_seconds:{nullopt}	- If set, how long debug bundles are kept in the debug bundle storage directory after they are created. If not set, debug bundles are kept indefinitely.
2025-04-14 16:44:30.927610965  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.debug_bundle_storage_dir:{nullopt}	- Path to the debug bundle storage directory. Note: Changing this path does not clean up existing debug bundles. If not set, the debug bundle is stored in the Redpanda data directory specified in the redpanda.yaml broker configuration file.
2025-04-14 16:44:30.927611965  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.debug_load_slice_warning_depth:{nullopt}	- The recursion depth after which debug logging is enabled automatically for the log reader.
2025-04-14 16:44:30.927613257  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.default_leaders_preference:none	- Default settings for preferred location of topic partition leaders. It can be either "none" (no preference), or "racks:<rack1>,<rack2>,..." (prefer brokers with rack id from the list).
2025-04-14 16:44:30.927613965  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.default_num_windows:10	- Default number of quota tracking windows.
2025-04-14 16:44:30.927689924  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.default_topic_partitions:1	- Default number of partitions per topic.
2025-04-14 16:44:30.927690840  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.default_topic_replications:1	- Default replication factor for new topics.
2025-04-14 16:44:30.927691590  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.default_window_sec:1000	- Default quota tracking window size in milliseconds.
2025-04-14 16:44:30.927692299  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.development_enable_cloud_topics:0	- Enable cloud topics.
2025-04-14 16:44:30.927693132  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.development_feature_property_testing_only:0	- Development feature property for testing only.
2025-04-14 16:44:30.927693840  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.disable_batch_cache:0	- Disable batch cache in log manager.
2025-04-14 16:44:30.927694882  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.disable_cluster_recovery_loop_for_tests:0	- Disables the cluster recovery loop. This property is used to simplify testing and should not be set in production.
2025-04-14 16:44:30.927695757  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.disable_metrics:0	- Disable registering the metrics exposed on the internal ` + "`" + `/metrics` + "`" + ` endpoint.
2025-04-14 16:44:30.927696590  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.disable_public_metrics:0	- Disable registering the metrics exposed on the ` + "`" + `/public_metrics` + "`" + ` endpoint.
2025-04-14 16:44:30.927698507  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.disk_reservation_percent:25	- The percentage of total disk capacity that Redpanda will avoid using. This applies both when cloud cache and log data share a disk, as well as when cloud cache uses a dedicated disk. It is recommended to not run disks near capacity to avoid blocking I/O due to low disk space, as well as avoiding performance issues associated with SSD garbage collection.
2025-04-14 16:44:30.927699215  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.election_timeout_ms:1500	- Election timeout expressed in milliseconds.
2025-04-14 16:44:30.927699840  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.enable_admin_api:	- 
2025-04-14 16:44:30.927700840  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.enable_auto_rebalance_on_node_add:0	- Enable automatic partition rebalancing when new nodes are added
2025-04-14 16:44:30.927701799  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.enable_cluster_metadata_upload_loop:1	- Enables cluster metadata uploads. Required for whole cluster restore.
2025-04-14 16:44:30.927704007  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.enable_controller_log_rate_limiting:0	- Limits the write rate for the controller log.
2025-04-14 16:44:30.927704590  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.enable_coproc:	- 
2025-04-14 16:44:30.927706757  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.enable_developmental_unrecoverable_data_corrupting_features:	- Development features should never be enabled in a production cluster, or any cluster where stability, data loss, or the ability to upgrade are a concern. To enable experimental features, set the value of this configuration option to the current unix epoch expressed in seconds. The value must be within one hour of the current time on the broker.Once experimental features are enabled they cannot be disabled
2025-04-14 16:44:30.927707424  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.enable_idempotence:1	- Enable idempotent producers.
2025-04-14 16:44:30.927708174  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.enable_leader_balancer:1	- Enable automatic leadership rebalancing.
2025-04-14 16:44:30.927710340  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.enable_metrics_reporter:1	- Enable the cluster metrics reporter. If ` + "`" + `true` + "`" + `, the metrics reporter collects and exports to Redpanda Data a set of customer usage metrics at the interval set by ` + "`" + `metrics_reporter_report_interval` + "`" + `. The cluster metrics of the metrics reporter are different from the monitoring metrics. * The metrics reporter exports customer usage metrics for consumption by Redpanda Data.* Monitoring metrics are exported for consumption by Redpanda users.
2025-04-14 16:44:30.927711049  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.enable_mpx_extensions:0	- Enable Redpanda extensions for MPX.
2025-04-14 16:44:30.927711799  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.enable_pid_file:1	- Enable PID file. You should not need to change.
2025-04-14 16:44:30.927712632  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.enable_rack_awareness:0	- Enable rack-aware replica assignment.
2025-04-14 16:44:30.927713924  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.enable_sasl:0	- Enable SASL authentication for Kafka connections. Authorization is required to modify this property. See also ` + "`" + `kafka_enable_authorization` + "`" + `.
2025-04-14 16:44:30.927716382  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.enable_schema_id_validation:none	- Mode to enable server-side schema ID validation. Accepted Values: * ` + "`" + `none` + "`" + `: Schema validation is disabled (no schema ID checks are done). Associated topic properties cannot be modified. * ` + "`" + `redpanda` + "`" + `: Schema validation is enabled. Only Redpanda topic properties are accepted. * ` + "`" + `compat` + "`" + `: Schema validation is enabled. Both Redpanda and compatible topic properties are accepted.
2025-04-14 16:44:30.927717257  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.enable_transactions:1	- Enable transactions (atomic writes).
2025-04-14 16:44:30.927718382  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.enable_usage:0	- Enables the usage tracking mechanism, storing windowed history of kafka/cloud_storage metrics over time.
2025-04-14 16:44:30.927719632  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.features_auto_enable:1	- Whether new feature flags auto-activate after upgrades (true) or must wait for manual activation via the Admin API (false).
2025-04-14 16:44:30.927720590  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.fetch_max_bytes:57671680	- Maximum number of bytes returned in a fetch request.
2025-04-14 16:44:30.927721507  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.fetch_pid_d_coeff:0	- Derivative coefficient for fetch PID controller.
2025-04-14 16:44:30.927722382  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.fetch_pid_i_coeff:0.01	- Integral coefficient for fetch PID controller.
2025-04-14 16:44:30.927723465  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.fetch_pid_max_debounce_ms:100	- The maximum debounce time the fetch PID controller will apply, in milliseconds.
2025-04-14 16:44:30.927724382  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.fetch_pid_p_coeff:100	- Proportional coefficient for fetch PID controller.
2025-04-14 16:44:30.927725632  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.fetch_pid_target_utilization_fraction:0.2	- A fraction, between 0 and 1, for the target reactor utilization of the fetch scheduling group.
2025-04-14 16:44:30.927730715  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.fetch_read_strategy:non_polling	- The strategy used to fulfill fetch requests. * ` + "`" + `polling` + "`" + `: Repeatedly polls every partition in the request for new data. The polling interval is set by ` + "`" + `fetch_reads_debounce_timeout` + "`" + ` (deprecated). * ` + "`" + `non_polling` + "`" + `: The backend is signaled when a partition has new data, so Redpanda does not need to repeatedly read from every partition in the fetch. Redpanda Data recommends using this value for most workloads, because it can improve fetch latency and CPU utilization. * ` + "`" + `non_polling_with_debounce` + "`" + `: This option behaves like ` + "`" + `non_polling` + "`" + `, but it includes a debounce mechanism with a fixed delay specified by ` + "`" + `fetch_reads_debounce_timeout` + "`" + ` at the start of each fetch. By introducing this delay, Redpanda can accumulate more data before processing, leading to fewer fetch operations and returning larger amounts of data. Enabling this option reduces reactor utilization, but it may also increase end-to-end latency.
2025-04-14 16:44:30.927806799  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.fetch_reads_debounce_timeout:1	- Time to wait for the next read in fetch requests when the requested minimum bytes was not reached.
2025-04-14 16:44:30.927808632  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.fetch_session_eviction_timeout_ms:60000	- Time duration after which the inactive fetch session is removed from the fetch session cache. Fetch sessions are used to implement the incremental fetch requests where a consumer does not send all requested partitions to the server but the server tracks them for the consumer.
2025-04-14 16:44:30.927809257  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.find_coordinator_timeout_ms:	- 
2025-04-14 16:44:30.927809882  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.full_raft_configuration_recovery_pattern:	- 
2025-04-14 16:44:30.927810715  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.group_initial_rebalance_delay:3000	- Delay added to the rebalance phase to wait for new members.
2025-04-14 16:44:30.927812049  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.group_max_session_timeout_ms:300000	- The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.
2025-04-14 16:44:30.927813465  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.group_min_session_timeout_ms:6000	- The minimum allowed session timeout for registered consumers. Shorter timeouts result in quicker failure detection at the cost of more frequent consumer heartbeating, which can overwhelm broker resources.
2025-04-14 16:44:30.927814174  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.group_new_member_join_timeout:30000	- Timeout for new member joins.
2025-04-14 16:44:30.927815090  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.group_offset_retention_check_ms:600000	- Frequency rate at which the system should check for expired group offsets.
2025-04-14 16:44:30.927816049  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.group_offset_retention_sec:{604800000}	- Consumer group offset retention seconds. To disable offset retention, set this to null.
2025-04-14 16:44:30.927816840  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.group_topic_partitions:16	- Number of partitions in the internal group membership topic.
2025-04-14 16:44:30.927817590  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.health_manager_tick_interval:180000	- How often the health manager runs.
2025-04-14 16:44:30.927818507  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.health_monitor_max_metadata_age:10000	- Maximum age of the metadata cached in the health monitor of a non-controller broker.
2025-04-14 16:44:30.927819299  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.health_monitor_tick_interval:10000	- How often health monitor refresh cluster state
2025-04-14 16:44:30.927820174  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.http_authentication:{BASIC}	- A list of supported HTTP authentication mechanisms. Accepted Values: ` + "`" + `BASIC` + "`" + `, ` + "`" + `OIDC` + "`" + `
2025-04-14 16:44:30.927821299  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.iceberg_catalog_base_location:redpanda-iceberg-catalog	- Base path for the cloud object storage-backed Iceberg catalog. After Iceberg is enabled, do not change this value.
2025-04-14 16:44:30.927822757  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.iceberg_catalog_commit_interval_ms:60000	- The frequency at which the Iceberg coordinator commits topic files to the catalog. This is the interval between commit transactions across all topics monitored by the coordinator, not the interval between individual commits.
2025-04-14 16:44:30.927823840  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.iceberg_catalog_type:object_storage	- Iceberg catalog type that Redpanda will use to commit table metadata updates. Supported types: 'rest', 'object_storage'
2025-04-14 16:44:30.927824924  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.iceberg_delete:1	- Default value for the redpanda.iceberg.delete topic property that determines if the corresponding Iceberg table is deleted upon deleting the topic.
2025-04-14 16:44:30.927826840  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.iceberg_enabled:0	- Enables the translation of topic data into Iceberg tables. Setting iceberg_enabled to true activates the feature at the cluster level, but each topic must also set the redpanda.iceberg.enabled topic-level property to true to use it. If iceberg_enabled is set to false, the feature is disabled for all topics in the cluster, overriding any topic-level settings.
2025-04-14 16:44:30.927827965  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_client_id:{nullopt}	- Iceberg REST catalog user ID. This ID is used to query the catalog API for the OAuth token. Required if catalog type is set to ` + "`" + `rest` + "`" + `
2025-04-14 16:44:30.927828965  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_client_secret:{nullopt}	- Secret to authenticate against Iceberg REST catalog. Required if catalog type is set to ` + "`" + `rest` + "`" + `
2025-04-14 16:44:30.927829882  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_crl_file:{nullopt}	- Path to certificate revocation list for ` + "`" + `iceberg_rest_catalog_trust_file` + "`" + `.
2025-04-14 16:44:30.927830632  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_endpoint:{nullopt}	- URL of Iceberg REST catalog endpoint
2025-04-14 16:44:30.927831715  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_prefix:{nullopt}	- Prefix part of the Iceberg REST catalog URL. Prefix is appended to the catalog path f.e. '/v1/{prefix}/namespaces'
2025-04-14 16:44:30.927832757  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_request_timeout_ms:10000	- Maximum length of time that Redpanda waits for a response from the REST catalog before aborting the request
2025-04-14 16:44:30.927834090  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_token:{nullopt}	- Token used to access the REST Iceberg catalog. If the token is present, Redpanda ignores credentials stored in the properties iceberg_rest_catalog_client_id and iceberg_rest_catalog_client_secret
2025-04-14 16:44:30.927835049  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.iceberg_rest_catalog_trust_file:{nullopt}	- Path to a file containing a certificate chain to trust for the REST Iceberg catalog
2025-04-14 16:44:30.927836257  INFO  2025-04-14 16:44:30,926 [shard 0:main] main - application.cc:849 - redpanda.id_allocator_batch_size:1000	- The ID allocator allocates messages in batches (each batch is a one log record) and then serves requests from memory without touching the log until the batch is exhausted.
2025-04-14 16:44:30.927837299  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.id_allocator_log_capacity:100	- Capacity of the ` + "`" + `id_allocator` + "`" + ` log in number of batches. After it reaches ` + "`" + `id_allocator_stm` + "`" + `, it truncates the log's prefix.
2025-04-14 16:44:30.927837882  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.id_allocator_replication:	- 
2025-04-14 16:44:30.927839465  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.initial_retention_local_target_bytes_default:{nullopt}	- Initial local retention size target for partitions of topics with Tiered Storage enabled. If no initial local target retention is configured all locally retained data will be delivered to learner when joining partition replica set.
2025-04-14 16:44:30.927841007  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.initial_retention_local_target_ms_default:{nullopt}	- Initial local retention time target for partitions of topics with Tiered Storage enabled. If no initial local target retention is configured all locally retained data will be delivered to learner when joining partition replica set.
2025-04-14 16:44:30.927841799  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.internal_topic_replication_factor:3	- Target replication factor for internal topics.
2025-04-14 16:44:30.927842549  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.join_retry_timeout_ms:5000	- Time between cluster join retries in milliseconds.
2025-04-14 16:44:30.927917215  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_admin_topic_api_rate:{nullopt}	- Target quota rate (partition mutations per default_window_sec)
2025-04-14 16:44:30.927918340  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_batch_max_bytes:1048576	- Maximum size of a batch processed by the server. If the batch is compressed, the limit applies to the compressed batch size.
2025-04-14 16:44:30.927919507  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_client_group_byte_rate_quota:{}	- Per-group target produce quota byte rate (bytes per second). Client is considered part of the group if client_id contains clients_prefix.
2025-04-14 16:44:30.927920632  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_client_group_fetch_byte_rate_quota:{}	- Per-group target fetch quota byte rate (bytes per second). Client is considered part of the group if client_id contains clients_prefix
2025-04-14 16:44:30.927921715  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_connection_rate_limit:{nullopt}	- Maximum connections per second for one core. If ` + "`" + `null` + "`" + ` (the default), then the number of connections per second is unlimited.
2025-04-14 16:44:30.927922882  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_connection_rate_limit_overrides:{}	- Overrides the maximum connections per second for one core for the specified IP addresses (for example, ` + "`" + `['127.0.0.1:90', '50.20.1.1:40']` + "`" + `)
2025-04-14 16:44:30.927923799  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_connections_max:{nullopt}	- Maximum number of Kafka client connections per broker. If ` + "`" + `null` + "`" + `, the property is disabled.
2025-04-14 16:44:30.927924965  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_connections_max_overrides:{}	- A list of IP addresses for which Kafka client connection limits are overridden and don't apply. For example, ` + "`" + `(['127.0.0.1:90', '50.20.1.1:40']).` + "`" + `
2025-04-14 16:44:30.927926007  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_connections_max_per_ip:{nullopt}	- Maximum number of Kafka client connections per IP address, per broker. If ` + "`" + `null` + "`" + `, the property is disabled.
2025-04-14 16:44:30.927927674  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_enable_authorization:{nullopt}	- Flag to require authorization for Kafka connections. If ` + "`" + `null` + "`" + `, the property is disabled, and authorization is instead enabled by enable_sasl. * ` + "`" + `null` + "`" + `: Ignored. Authorization is enabled with ` + "`" + `enable_sasl` + "`" + `: ` + "`" + `true` + "`" + ` * ` + "`" + `true` + "`" + `: authorization is required. * ` + "`" + `false` + "`" + `: authorization is disabled.
2025-04-14 16:44:30.927928715  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_enable_describe_log_dirs_remote_storage:1	- Whether to include Tiered Storage as a special remote:// directory in ` + "`" + `DescribeLogDirs Kafka` + "`" + ` API requests.
2025-04-14 16:44:30.927929507  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_enable_partition_reassignment:1	- Enable the Kafka partition reassignment API.
2025-04-14 16:44:30.927930257  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_group_recovery_timeout_ms:30000	- Kafka group recovery timeout.
2025-04-14 16:44:30.927931215  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_max_bytes_per_fetch:67108864	- Limit fetch responses to this many bytes, even if the total of partition bytes limits is higher.
2025-04-14 16:44:30.927932799  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_memory_batch_size_estimate_for_fetch:1048576	- The size of the batch used to estimate memory consumption for fetch requests, in bytes. Smaller sizes allow more concurrent fetch requests per shard. Larger sizes prevent running out of memory because of too many concurrent fetch requests.
2025-04-14 16:44:30.927933882  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_memory_share_for_fetch:0.5	- The share of Kafka subsystem memory that can be used for fetch read buffers, as a fraction of the Kafka subsystem memory amount.
2025-04-14 16:44:30.927934882  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_mtls_principal_mapping_rules:{nullopt}	- Principal mapping rules for mTLS authentication on the Kafka API. If ` + "`" + `null` + "`" + `, the property is disabled.
2025-04-14 16:44:30.927936174  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_nodelete_topics:{_redpanda.audit_log, __consumer_offsets, _schemas}	- A list of topics that are protected from deletion and configuration changes by Kafka clients. Set by default to a list of Redpanda internal topics.
2025-04-14 16:44:30.927937257  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_noproduce_topics:{}	- A list of topics that are protected from being produced to by Kafka clients. Set by default to a list of Redpanda internal topics.
2025-04-14 16:44:30.927938049  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_qdc_depth_alpha:0.8	- Smoothing factor for Kafka queue depth control depth tracking.
2025-04-14 16:44:30.927938882  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_qdc_depth_update_ms:7000	- Update frequency for Kafka queue depth control.
2025-04-14 16:44:30.927939590  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_qdc_enable:0	- Enable kafka queue depth control.
2025-04-14 16:44:30.927940382  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_qdc_idle_depth:10	- Queue depth when idleness is detected in Kafka queue depth control.
2025-04-14 16:44:30.927941257  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_qdc_latency_alpha:0.002	- Smoothing parameter for Kafka queue depth control latency tracking.
2025-04-14 16:44:30.927942007  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_qdc_max_depth:100	- Maximum queue depth used in kafka queue depth control.
2025-04-14 16:44:30.927942882  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_qdc_max_latency_ms:80	- Maximum latency threshold for Kafka queue depth control depth tracking.
2025-04-14 16:44:30.927943632  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_qdc_min_depth:1	- Minimum queue depth used in Kafka queue depth control.
2025-04-14 16:44:30.927944465  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_qdc_window_count:12	- Number of windows used in kafka queue depth control latency tracking.
2025-04-14 16:44:30.927945257  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_qdc_window_size_ms:1500	- Window size for Kafka queue depth control latency tracking.
2025-04-14 16:44:30.927949132  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_quota_balancer_min_shard_throughput_bps:256	- The minimum value of the throughput quota a shard can get in the process of quota balancing, expressed in bytes per second. The value applies equally to ingress and egress traffic. ` + "`" + `kafka_quota_balancer_min_shard_throughput_bps` + "`" + ` doesn't override the limit settings, ` + "`" + `kafka_throughput_limit_node_in_bps` + "`" + ` and ` + "`" + `kafka_throughput_limit_node_out_bps` + "`" + `. Consequently, the value of ` + "`" + `kafka_throughput_limit_node_in_bps` + "`" + ` or ` + "`" + `kafka_throughput_limit_node_out_bps` + "`" + ` can result in lesser throughput than kafka_quota_balancer_min_shard_throughput_bps. Both ` + "`" + `kafka_quota_balancer_min_shard_throughput_ratio` + "`" + ` and ` + "`" + `kafka_quota_balancer_min_shard_throughput_bps` + "`" + ` can be specified at the same time. In this case, the balancer will not decrease the effective shard quota below the largest bytes-per-second (bps) value of each of these two properties. If set to ` + "`" + `0` + "`" + `, no minimum is enforced.
2025-04-14 16:44:30.928022132  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_quota_balancer_min_shard_throughput_ratio:0.01	- The minimum value of the throughput quota a shard can get in the process of quota balancing, expressed as a ratio of default shard quota. While the value applies equally to ingress and egress traffic, the default shard quota can be different for ingress and egress and therefore result in different minimum throughput bytes-per-second (bps) values. Both ` + "`" + `kafka_quota_balancer_min_shard_throughput_ratio` + "`" + ` and ` + "`" + `kafka_quota_balancer_min_shard_throughput_bps` + "`" + ` can be specified at the same time. In this case, the balancer will not decrease the effective shard quota below the largest bps value of each of these two properties. If set to ` + "`" + `0.0` + "`" + `, the minimum is disabled. If set to ` + "`" + `1.0` + "`" + `, the balancer won't be able to rebalance quota without violating this ratio, preventing the balancer from adjusting shards' quotas.
2025-04-14 16:44:30.928023549  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_quota_balancer_node_period_ms:0	- Intra-node throughput quota balancer invocation period, in milliseconds. When set to 0, the balancer is disabled and makes all the throughput quotas immutable.
2025-04-14 16:44:30.928024549  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_quota_balancer_window_ms:5000	- Time window used to average current throughput measurement for quota balancer, in milliseconds.
2025-04-14 16:44:30.928025382  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_request_max_bytes:104857600	- Maximum size of a single request processed using the Kafka API.
2025-04-14 16:44:30.928026340  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_rpc_server_stream_recv_buf:{nullopt}	- Maximum size of the user-space receive buffer. If ` + "`" + `null` + "`" + `, this limit is not applied.
2025-04-14 16:44:30.928027257  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_rpc_server_tcp_recv_buf:{nullopt}	- Size of the Kafka server TCP receive buffer. If ` + "`" + `null` + "`" + `, the property is disabled.
2025-04-14 16:44:30.928028215  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_rpc_server_tcp_send_buf:{nullopt}	- Size of the Kafka server TCP transmit buffer. If ` + "`" + `null` + "`" + `, the property is disabled.
2025-04-14 16:44:30.928029965  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_sasl_max_reauth_ms:{nullopt}	- The maximum time between Kafka client reauthentications. If a client has not reauthenticated a connection within this time frame, that connection is torn down. If this property is not set (or set to ` + "`" + `null` + "`" + `), session expiry is disabled, and a connection could live long after the client's credentials are expired or revoked.
2025-04-14 16:44:30.928030840  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_schema_id_validation_cache_capacity:128	- Per-shard capacity of the cache for validating schema IDs.
2025-04-14 16:44:30.928032340  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_tcp_keepalive_probe_interval_seconds:60000	- TCP keepalive probe interval in seconds for Kafka connections. This describes the timeout between unacknowledged TCP keepalives. Refers to the TCP_KEEPINTVL socket option. When changed, applies to new connections only.
2025-04-14 16:44:30.928033632  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_tcp_keepalive_probes:3	- TCP keepalive unacknowledged probes until the connection is considered dead for Kafka connections. Refers to the TCP_KEEPCNT socket option. When changed, applies to new connections only.
2025-04-14 16:44:30.928035174  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_tcp_keepalive_timeout:120000	- TCP keepalive idle timeout in seconds for Kafka connections. This describes the timeout between TCP keepalive probes that the remote site successfully acknowledged. Refers to the TCP_KEEPIDLE socket option. When changed, applies to new connections only.
2025-04-14 16:44:30.928037007  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_throughput_control:{}	- List of throughput control groups that define exclusions from node-wide throughput limits. Clients excluded from node-wide throughput limits are still potentially subject to client-specific throughput limits. For more information see https://docs.redpanda.com/current/reference/properties/cluster-properties/#kafka_throughput_control.
2025-04-14 16:44:30.928038049  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_throughput_controlled_api_keys:{produce, fetch}	- List of Kafka API keys that are subject to cluster-wide and node-wide throughput limit control.
2025-04-14 16:44:30.928039507  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_throughput_limit_node_in_bps:{nullopt}	- The maximum rate of all ingress Kafka API traffic for a node. Includes all Kafka API traffic (requests, responses, headers, fetched data, produced data, etc.). If ` + "`" + `null` + "`" + `, the property is disabled, and traffic is not limited.
2025-04-14 16:44:30.928040965  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_throughput_limit_node_out_bps:{nullopt}	- The maximum rate of all egress Kafka traffic for a node. Includes all Kafka API traffic (requests, responses, headers, fetched data, produced data, etc.). If ` + "`" + `null` + "`" + `, the property is disabled, and traffic is not limited.
2025-04-14 16:44:30.928043507  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_throughput_replenish_threshold:{nullopt}	- Threshold for refilling the token bucket as part of enforcing throughput limits. This only applies when kafka_throughput_throttling_v2 is ` + "`" + `true` + "`" + `. This threshold is evaluated with each request for data. When the number of tokens to replenish exceeds this threshold, then tokens are added to the token bucket. This ensures that the atomic is not being updated for the token count with each request. The range for this threshold is automatically clamped to the corresponding throughput limit for ingress and egress.
2025-04-14 16:44:30.928046007  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kafka_throughput_throttling_v2:1	- Enables an updated algorithm for enforcing node throughput limits based on a shared token bucket, introduced with Redpanda v23.3.8. Set this property to ` + "`" + `false` + "`" + ` if you need to use the quota balancing algorithm from Redpanda v23.3.7 and older.  This property defaults to ` + "`" + `true` + "`" + ` for all new or upgraded Redpanda clusters. Disabling this property is not recommended. It causes your Redpanda cluster to use an outdated throughput throttling mechanism. Only set this to ` + "`" + `false` + "`" + ` when advised to do so by Redpanda support.
2025-04-14 16:44:30.928046799  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kvstore_flush_interval:10	- Key-value store flush interval (in milliseconds).
2025-04-14 16:44:30.928047549  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.kvstore_max_segment_size:16777216	- Key-value maximum segment size (in bytes).
2025-04-14 16:44:30.928048299  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.leader_balancer_idle_timeout:120000	- Leadership rebalancing idle timeout.
2025-04-14 16:44:30.928048882  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.leader_balancer_mode:	- 
2025-04-14 16:44:30.928049632  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.leader_balancer_mute_timeout:300000	- Leadership rebalancing mute timeout.
2025-04-14 16:44:30.928050507  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.leader_balancer_transfer_limit_per_shard:512	- Per shard limit for in-progress leadership transfers.
2025-04-14 16:44:30.928051757  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.legacy_group_offset_retention_enabled:0	- Group offset retention is enabled by default starting in Redpanda version 23.1. To enable offset retention after upgrading from an older version, set this option to true.
2025-04-14 16:44:30.928053715  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.legacy_permit_unsafe_log_operation:1	- Flag to enable a Redpanda cluster operator to use unsafe control characters within strings, such as consumer group names or user names. This flag applies only for Redpanda clusters that were originally on version 23.1 or earlier and have been upgraded to version 23.2 or later. Starting in version 23.2, newly-created Redpanda clusters ignore this property.
2025-04-14 16:44:30.928055215  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.legacy_unsafe_log_warning_interval_sec:300000	- Period at which to log a warning about using unsafe strings containing control characters. If unsafe strings are permitted by ` + "`" + `legacy_permit_unsafe_log_operation` + "`" + `, a warning will be logged at an interval specified by this property.
2025-04-14 16:44:30.928056299  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.log_cleanup_policy:delete	- Default cleanup policy for topic logs. The topic property ` + "`" + `cleanup.policy` + "`" + ` overrides the value of ` + "`" + `log_cleanup_policy` + "`" + ` at the topic level.
2025-04-14 16:44:30.928071424  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.log_compaction_interval_ms:10000	- How often to trigger background compaction.
2025-04-14 16:44:30.928139257  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.log_compaction_use_sliding_window:1	- Use sliding window compaction.
2025-04-14 16:44:30.928140465  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.log_compression_type:producer	- Default topic compression type. The topic property ` + "`" + `compression.type` + "`" + ` overrides the value of ` + "`" + `log_compression_type` + "`" + ` at the topic level.
2025-04-14 16:44:30.928141590  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.log_disable_housekeeping_for_tests:0	- Disables the housekeeping loop for local storage. This property is used to simplify testing, and should not be set in production.
2025-04-14 16:44:30.928142840  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.log_message_timestamp_alert_after_ms:7200000	- Threshold in milliseconds for alerting on messages with a timestamp after the broker's time, meaning the messages are in the future relative to the broker's clock.
2025-04-14 16:44:30.928144257  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.log_message_timestamp_alert_before_ms:{nullopt}	- Threshold in milliseconds for alerting on messages with a timestamp before the broker's time, meaning the messages are in the past relative to the broker's clock. To disable this check, set to ` + "`" + `null` + "`" + `.
2025-04-14 16:44:30.928145590  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.log_message_timestamp_type:CreateTime	- Default timestamp type for topic messages (CreateTime or LogAppendTime). The topic property ` + "`" + `message.timestamp.type` + "`" + ` overrides the value of ` + "`" + `log_message_timestamp_type` + "`" + ` at the topic level.
2025-04-14 16:44:30.928146882  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.log_retention_ms:604800000	- The amount of time to keep a log file before deleting it (in milliseconds). If set to ` + "`" + `-1` + "`" + `, no time limit is applied. This is a cluster-wide default when a topic does not set or disable ` + "`" + `retention.ms` + "`" + `.
2025-04-14 16:44:30.928148757  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.log_segment_ms:{1209600000}	- Default lifetime of log segments. If ` + "`" + `null` + "`" + `, the property is disabled, and no default lifetime is set. Any value under 60 seconds (60000 ms) is rejected. This property can also be set in the Kafka API using the Kafka-compatible alias, ` + "`" + `log.roll.ms` + "`" + `. The topic property ` + "`" + `segment.ms` + "`" + ` overrides the value of ` + "`" + `log_segment_ms` + "`" + ` at the topic level.
2025-04-14 16:44:30.928149674  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.log_segment_ms_max:31536000000	- Upper bound on topic ` + "`" + `segment.ms` + "`" + `: higher values will be clamped to this value.
2025-04-14 16:44:30.928150549  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.log_segment_ms_min:600000	- Lower bound on topic ` + "`" + `segment.ms` + "`" + `: lower values will be clamped to this value.
2025-04-14 16:44:30.928151382  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.log_segment_size:134217728	- Default log segment size in bytes for topics which do not set segment.bytes
2025-04-14 16:44:30.928152299  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.log_segment_size_jitter_percent:5	- Random variation to the segment size limit used for each partition.
2025-04-14 16:44:30.928153215  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.log_segment_size_max:{nullopt}	- Upper bound on topic ` + "`" + `segment.bytes` + "`" + `: higher values will be clamped to this limit.
2025-04-14 16:44:30.928154132  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.log_segment_size_min:{1048576}	- Lower bound on topic ` + "`" + `segment.bytes` + "`" + `: lower values will be clamped to this limit.
2025-04-14 16:44:30.928155007  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.lz4_decompress_reusable_buffers_disabled:0	- Disable reusable preallocated buffers for LZ4 decompression.
2025-04-14 16:44:30.928155840  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.max_compacted_log_segment_size:5368709120	- Maximum compacted segment size after consolidation.
2025-04-14 16:44:30.928157632  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.max_concurrent_producer_ids:18446744073709551615	- Maximum number of the active producers sessions. When the threshold is passed, Redpanda terminates old sessions. When an idle producer corresponding to the terminated session wakes up and produces, its message batches are rejected, and an out of order sequence error is emitted. Consumers don't affect this setting.
2025-04-14 16:44:30.928158882  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.max_in_flight_pandaproxy_requests_per_shard:500	- Maximum number of in-flight HTTP requests to HTTP Proxy permitted per shard.  Any additional requests above this limit will be rejected with a 429 error.
2025-04-14 16:44:30.928160132  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.max_in_flight_schema_registry_requests_per_shard:500	- Maximum number of in-flight HTTP requests to Schema Registry permitted per shard.  Any additional requests above this limit will be rejected with a 429 error.
2025-04-14 16:44:30.928160965  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.max_kafka_throttle_delay_ms:30000	- Fail-safe maximum throttle delay on Kafka requests.
2025-04-14 16:44:30.928163007  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.max_transactions_per_coordinator:18446744073709551615	- Specifies the maximum number of active transaction sessions per coordinator. When the threshold is passed Redpanda terminates old sessions. When an idle producer corresponding to the terminated session wakes up and produces, it leads to its batches being rejected with invalid producer epoch or invalid_producer_id_mapping error (depends on the transaction execution phase).
2025-04-14 16:44:30.928163590  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.max_version:	- 
2025-04-14 16:44:30.928164382  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.members_backend_retry_ms:5000	- Time between members backend reconciliation loop retries.
2025-04-14 16:44:30.928165549  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.memory_abort_on_alloc_failure:1	- If ` + "`" + `true` + "`" + `, the Redpanda process will terminate immediately when an allocation cannot be satisfied due to memory exhaustion. If false, an exception is thrown.
2025-04-14 16:44:30.928166924  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.memory_enable_memory_sampling:1	- When ` + "`" + `true` + "`" + `, memory allocations are sampled and tracked. A sampled live set of allocations can then be retrieved from the Admin API. Additionally, Redpanda will periodically log the top-n allocation sites.
2025-04-14 16:44:30.928167715  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.metadata_dissemination_interval_ms:3000	- Interval for metadata dissemination batching.
2025-04-14 16:44:30.928170465  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.metadata_dissemination_retries:30	- Number of attempts to look up a topic's metadata-like shard before a request fails. This configuration controls the number of retries that request handlers perform when internal topic metadata (for topics like tx, consumer offsets, etc) is missing. These topics are usually created on demand when users try to use the cluster for the first time and it may take some time for the creation to happen and the metadata to propagate to all the brokers (particularly the broker handling the request). In the mean time Redpanda waits and retry. This configuration controls the number retries.
2025-04-14 16:44:30.928171382  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.metadata_dissemination_retry_delay_ms:320	- Delay before retrying a topic lookup in a shard or other meta tables.
2025-04-14 16:44:30.928172299  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.metadata_status_wait_timeout_ms:2000	- Maximum time to wait in metadata request for cluster health to be refreshed.
2025-04-14 16:44:30.928173090  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.metrics_reporter_report_interval:86400000	- Cluster metrics reporter report interval.
2025-04-14 16:44:30.928173840  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.metrics_reporter_tick_interval:60000	- Cluster metrics reporter tick interval.
2025-04-14 16:44:30.928347799  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.metrics_reporter_url:https://m.rp.vectorized.io/v2	- URL of the cluster metrics reporter.
2025-04-14 16:44:30.928348465  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.min_version:	- 
2025-04-14 16:44:30.928351715  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.minimum_topic_replications:1	- Minimum allowable replication factor for topics in this cluster. The set value must be positive, odd, and equal to or less than the number of available brokers. Changing this parameter only restricts newly-created topics. Redpanda returns an ` + "`" + `INVALID_REPLICATION_FACTOR` + "`" + ` error on any attempt to create a topic with a replication factor less than this property. If you change the ` + "`" + `minimum_topic_replications` + "`" + ` setting, the replication factor of existing topics remains unchanged. However, Redpanda will log a warning on start-up with a list of any topics that have fewer replicas than this minimum. For example, you might see a message such as ` + "`" + `Topic X has a replication factor less than specified minimum: 1 < 3` + "`" + `.
2025-04-14 16:44:30.928352715  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.node_isolation_heartbeat_timeout:3000	- How long after the last heartbeat request a node will wait before considering itself to be isolated.
2025-04-14 16:44:30.928353549  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.node_management_operation_timeout_ms:5000	- Timeout for executing node management operations.
2025-04-14 16:44:30.928354590  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.node_status_interval:100	- Time interval between two node status messages. Node status messages establish liveness status outside of the Raft protocol.
2025-04-14 16:44:30.928355632  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.node_status_reconnect_max_backoff_ms:15000	- Maximum backoff (in milliseconds) to reconnect to an unresponsive peer during node status liveness checks.
2025-04-14 16:44:30.928356590  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.oidc_clock_skew_tolerance:0	- The amount of time (in seconds) to allow for when validating the expiry claim in the token.
2025-04-14 16:44:30.928357674  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.oidc_discovery_url:https://auth.prd.cloud.redpanda.com/.well-known/openid-configuration	- The URL pointing to the well-known discovery endpoint for the OIDC provider.
2025-04-14 16:44:30.928358590  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.oidc_keys_refresh_interval:3600000	- The frequency of refreshing the JSON Web Keys (JWKS) used to validate access tokens.
2025-04-14 16:44:30.928359424  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.oidc_principal_mapping:$.sub	- Rule for mapping JWT payload claim to a Redpanda user principal.
2025-04-14 16:44:30.928360215  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.oidc_token_audience:redpanda	- A string representing the intended recipient of the token.
2025-04-14 16:44:30.928361049  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.partition_autobalancing_concurrent_moves:50	- Number of partitions that can be reassigned at once.
2025-04-14 16:44:30.928362465  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.partition_autobalancing_max_disk_usage_percent:80	- When the disk usage of a node exceeds this threshold, it triggers Redpanda to move partitions off of the node. This property applies only when partition_autobalancing_mode is set to ` + "`" + `continuous` + "`" + `.
2025-04-14 16:44:30.928363799  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.partition_autobalancing_min_size_threshold:{nullopt}	- Minimum size of partition that is going to be prioritized when rebalancing a cluster due to the disk size threshold being breached. This value is calculated automatically by default.
2025-04-14 16:44:30.928366674  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.partition_autobalancing_mode:node_add	- Mode of partition balancing for a cluster. * ` + "`" + `node_add` + "`" + `: partition balancing happens when a node is added. * ` + "`" + `continuous` + "`" + `: partition balancing happens automatically to maintain optimal performance and availability, based on continuous monitoring for node changes (same as ` + "`" + `node_add` + "`" + `) and also high disk usage. This option requires an Enterprise license, and it is customized by ` + "`" + `partition_autobalancing_node_availability_timeout_sec` + "`" + ` and ` + "`" + `partition_autobalancing_max_disk_usage_percent` + "`" + ` properties. * ` + "`" + `off` + "`" + `: partition balancing is disabled. This option is not recommended for production clusters.
2025-04-14 16:44:30.928368049  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.partition_autobalancing_movement_batch_size_bytes:5368709120	- Total size of partitions that autobalancer is going to move in one batch (deprecated, use partition_autobalancing_concurrent_moves to limit the autobalancer concurrency)
2025-04-14 16:44:30.928369549  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.partition_autobalancing_node_availability_timeout_sec:900000	- When a node is unavailable for at least this timeout duration, it triggers Redpanda to move partitions off of the node. This property applies only when ` + "`" + `partition_autobalancing_mode` + "`" + ` is set to ` + "`" + `continuous` + "`" + `.      
2025-04-14 16:44:30.928370340  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.partition_autobalancing_tick_interval_ms:30000	- Partition autobalancer tick interval.
2025-04-14 16:44:30.928371965  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.partition_autobalancing_tick_moves_drop_threshold:0.2	- If the number of scheduled tick moves drops by this ratio, a new tick is scheduled immediately. Valid values are (0, 1]. For example, with a value of 0.2 and 100 scheduled moves in a tick, a new tick is scheduled when the in-progress moves are fewer than 80.
2025-04-14 16:44:30.928373840  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.partition_autobalancing_topic_aware:1	- If ` + "`" + `true` + "`" + `, Redpanda prioritizes balancing a topic’s partition replica count evenly across all brokers while it’s balancing the cluster’s overall partition count. Because different topics in a cluster can have vastly different load profiles, this better distributes the workload of the most heavily-used topics evenly across brokers.
2025-04-14 16:44:30.928375299  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.partition_manager_shutdown_watchdog_timeout:30000	- A threshold value to detect partitions which might have been stuck while shutting down. After this threshold, a watchdog in partition manager will log information about partition shutdown not making progress.
2025-04-14 16:44:30.928376632  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.pp_sr_smp_max_non_local_requests:{nullopt}	- Maximum number of Cross-core(Inter-shard communication) requests pending in HTTP Proxy and Schema Registry seastar::smp group. (For more details, see the ` + "`" + `seastar::smp_service_group` + "`" + ` documentation).
2025-04-14 16:44:30.928377382  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.quota_manager_gc_sec:30000	- Quota manager GC frequency in milliseconds.
2025-04-14 16:44:30.928379090  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_enable_longest_log_detection:1	- Enables an additional step in leader election where a candidate is allowed to wait for all the replies from the broker it requested votes from. This may introduce a small delay when recovering from failure, but it prevents truncation if any of the replicas have more data than the majority.
2025-04-14 16:44:30.928379840  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_enable_lw_heartbeat:1	- Enables Raft optimization of heartbeats.
2025-04-14 16:44:30.928381007  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_flush_timer_interval_ms:100	- Interval of checking partition against the ` + "`" + `raft_replica_max_pending_flush_bytes` + "`" + `, deprecated started 24.1, use raft_replica_max_flush_delay_ms instead 
2025-04-14 16:44:30.928382132  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_heartbeat_disconnect_failures:3	- The number of failed heartbeats after which an unresponsive TCP connection is forcibly closed. To disable forced disconnection, set to 0.
2025-04-14 16:44:30.928382924  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_heartbeat_interval_ms:150	- Number of milliseconds for Raft leader heartbeats.
2025-04-14 16:44:30.928469882  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_heartbeat_timeout_ms:3000	- Raft heartbeat RPC (remote procedure call) timeout. Raft uses a heartbeat mechanism to maintain leadership authority and to trigger leader elections. The ` + "`" + `raft_heartbeat_interval_ms` + "`" + ` is a periodic heartbeat sent by the partition leader to all followers to declare its leadership. If a follower does not receive a heartbeat within the ` + "`" + `raft_heartbeat_timeout_ms` + "`" + `, then it triggers an election to choose a new partition leader.
2025-04-14 16:44:30.928470799  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_io_timeout_ms:10000	- Raft I/O timeout.
2025-04-14 16:44:30.928473424  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_learner_recovery_rate:104857600	- Raft learner recovery rate limit. Throttles the rate of data communicated to nodes (learners) that need to catch up to leaders. This rate limit is placed on a node sending data to a recovering node. Each sending node is limited to this rate. The recovering node accepts data as fast as possible according to the combined limits of all healthy nodes in the cluster. For example, if two nodes are sending data to the recovering node, and ` + "`" + `raft_learner_recovery_rate` + "`" + ` is 100 MB/sec, then the recovering node will recover at a rate of 200 MB/sec.
2025-04-14 16:44:30.928474465  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_max_concurrent_append_requests_per_follower:16	- Maximum number of concurrent append entry requests sent by the leader to one follower.
2025-04-14 16:44:30.928475465  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_max_recovery_memory:{nullopt}	- Maximum memory that can be used for reads in Raft recovery process by default 15% of total memory.
2025-04-14 16:44:30.928476757  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_recovery_concurrency_per_shard:64	- Number of partitions that may simultaneously recover data to a particular shard. This number is limited to avoid overwhelming nodes when they come back online after an outage.
2025-04-14 16:44:30.928477674  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_recovery_default_read_size:524288	- Specifies the default size of a read issued during Raft follower recovery.
2025-04-14 16:44:30.928478799  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_recovery_throttle_disable_dynamic_mode:0	- Disables cross shard sharing used to throttle recovery traffic. Should only be used to debug unexpected problems.
2025-04-14 16:44:30.928479799  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_replica_max_flush_delay_ms:100	- Maximum delay between two subsequent flushes. After this delay, the log is automatically force flushed.
2025-04-14 16:44:30.928481132  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_replica_max_pending_flush_bytes:{262144}	- Maximum number of bytes that are not flushed per partition. If the configured threshold is reached, the log is automatically flushed even if it has not been explicitly requested.
2025-04-14 16:44:30.928481965  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_replicate_batch_window_size:1048576	- Maximum size of requests cached for replication.
2025-04-14 16:44:30.928483299  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_smp_max_non_local_requests:{nullopt}	- Maximum number of Cross-core(Inter-shard communication) requests pending in Raft seastar::smp group. For details, refer to the ` + "`" + `seastar::smp_service_group` + "`" + ` documentation).
2025-04-14 16:44:30.928484340  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_timeout_now_timeout_ms:1000	- Timeout for Raft's timeout_now RPC. This RPC is used to force a follower to dispatch a round of votes immediately.
2025-04-14 16:44:30.928485299  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.raft_transfer_leader_recovery_timeout_ms:10000	- Follower recovery timeout waiting period when transferring leadership.
2025-04-14 16:44:30.928486174  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.readers_cache_eviction_timeout_ms:30000	- Duration after which inactive readers are evicted from cache.
2025-04-14 16:44:30.928487465  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.readers_cache_target_max_size:200	- Maximum desired number of readers cached per NTP. This a soft limit, meaning that a number of readers in cache may temporarily increase as cleanup is performed in the background.
2025-04-14 16:44:30.928488424  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.reclaim_batch_cache_min_free:67108864	- Minimum amount of free memory maintained by the batch cache background reclaimer.
2025-04-14 16:44:30.928489882  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.reclaim_growth_window:3000	- Starting from the last point in time when memory was reclaimed from the batch cache, this is the duration during which the amount of memory to reclaim grows at a significant rate, based on heuristics about the amount of available memory.
2025-04-14 16:44:30.928490632  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.reclaim_max_size:4194304	- Maximum batch cache reclaim size.
2025-04-14 16:44:30.928491299  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.reclaim_min_size:131072	- Minimum batch cache reclaim size.
2025-04-14 16:44:30.928492840  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.reclaim_stable_window:10000	- If the duration since the last time memory was reclaimed is longer than the amount of time specified in this property, the memory usage of the batch cache is considered stable, so only the minimum size (` + "`" + `reclaim_min_size` + "`" + `) is set to be reclaimed.
2025-04-14 16:44:30.928493715  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.recovery_append_timeout_ms:5000	- Timeout for append entry requests issued while updating a stale follower.
2025-04-14 16:44:30.928494674  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.release_cache_on_segment_roll:0	- Flag for specifying whether or not to release cache when a full segment is rolled.
2025-04-14 16:44:30.928495549  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.replicate_append_timeout_ms:3000	- Timeout for append entry requests issued while replicating entries.
2025-04-14 16:44:30.928497090  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.retention_bytes:{nullopt}	- Default maximum number of bytes per partition on disk before triggering deletion of the oldest messages. If ` + "`" + `null` + "`" + ` (the default value), no limit is applied. The topic property ` + "`" + `retention.bytes` + "`" + ` overrides the value of ` + "`" + `retention_bytes` + "`" + ` at the topic level.
2025-04-14 16:44:30.928498590  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.retention_local_strict:0	- Flag to allow Tiered Storage topics to expand to consumable retention policy limits. When this flag is enabled, non-local retention settings are used, and local retention settings are used to inform data removal policies in low-disk space scenarios.
2025-04-14 16:44:30.928500132  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.retention_local_strict_override:1	- Trim log data when a cloud topic reaches its local retention limit. When this option is disabled Redpanda will allow partitions to grow past the local retention limit, and will be trimmed automatically as storage reaches the configured target size.
2025-04-14 16:44:30.928502299  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.retention_local_target_bytes_default:{nullopt}	- Local retention size target for partitions of topics with object storage write enabled. If ` + "`" + `null` + "`" + `, the property is disabled. This property can be overridden on a per-topic basis by setting ` + "`" + `retention.local.target.bytes` + "`" + ` in each topic enabled for Tiered Storage. Both ` + "`" + `retention_local_target_bytes_default` + "`" + ` and ` + "`" + `retention_local_target_ms_default` + "`" + ` can be set. The limit that is reached earlier is applied.
2025-04-14 16:44:30.928591799  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.retention_local_target_capacity_bytes:{nullopt}	- The target capacity (in bytes) that log storage will try to use before additional retention rules take over to trim data to meet the target. When no target is specified, storage usage is unbounded. Redpanda Data recommends setting only one of ` + "`" + `retention_local_target_capacity_bytes` + "`" + ` or ` + "`" + `retention_local_target_capacity_percent` + "`" + `. If both are set, the minimum of the two is used as the effective target capacity.
2025-04-14 16:44:30.928594382  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.retention_local_target_capacity_percent:{80}	- The target capacity in percent of unreserved space (` + "`" + `disk_reservation_percent` + "`" + `) that log storage will try to use before additional retention rules will take over to trim data in order to meet the target. When no target is specified storage usage is unbounded. Redpanda Data recommends setting only one of ` + "`" + `retention_local_target_capacity_bytes` + "`" + ` or ` + "`" + `retention_local_target_capacity_percent` + "`" + `. If both are set, the minimum of the two is used as the effective target capacity.
2025-04-14 16:44:30.928596424  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.retention_local_target_ms_default:86400000	- Local retention time target for partitions of topics with object storage write enabled. This property can be overridden on a per-topic basis by setting ` + "`" + `retention.local.target.ms` + "`" + ` in each topic enabled for Tiered Storage. Both ` + "`" + `retention_local_target_bytes_default` + "`" + ` and ` + "`" + `retention_local_target_ms_default` + "`" + ` can be set. The limit that is reached first is applied.
2025-04-14 16:44:30.928597507  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.retention_local_trim_interval:30000	- The period during which disk usage is checked for disk pressure, and data is optionally trimmed to meet the target.
2025-04-14 16:44:30.928598799  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.retention_local_trim_overage_coeff:2	- The space management control loop reclaims the overage multiplied by this this coefficient to compensate for data that is written during the idle period between control loop invocations.
2025-04-14 16:44:30.928600174  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.rm_sync_timeout_ms:10000	- Resource manager's synchronization timeout. Specifies the maximum time for this node to wait for the internal state machine to catch up with all events written by previous leaders before rejecting a request.
2025-04-14 16:44:30.928600799  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.rm_violation_recovery_policy:	- 
2025-04-14 16:44:30.928601674  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.rpc_client_connections_per_peer:128	- The maximum number of connections a broker will open to each of its peers.
2025-04-14 16:44:30.928602632  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.rpc_server_compress_replies:0	- Enable compression for internal RPC (remote procedure call) server replies.
2025-04-14 16:44:30.928603757  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.rpc_server_listen_backlog:{nullopt}	- Maximum TCP connection queue length for Kafka server and internal RPC server. If ` + "`" + `null` + "`" + ` (the default value), no queue length is set.
2025-04-14 16:44:30.928604757  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.rpc_server_tcp_recv_buf:{nullopt}	- Internal RPC TCP receive buffer size. If ` + "`" + `null` + "`" + ` (the default value), no buffer size is set by Redpanda.
2025-04-14 16:44:30.928605799  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.rpc_server_tcp_send_buf:{nullopt}	- Internal RPC TCP send buffer size. If ` + "`" + `null` + "`" + ` (the default value), then no buffer size is set by Redpanda.
2025-04-14 16:44:30.928606799  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.rpk_path:"/usr/bin/rpk"	- Path to RPK binary
2025-04-14 16:44:30.928607757  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.rps_limit_acls_and_users_operations:1000	- Rate limit for controller ACLs and user's operations.
2025-04-14 16:44:30.928608882  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.rps_limit_configuration_operations:1000	- Rate limit for controller configuration operations.
2025-04-14 16:44:30.928609757  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.rps_limit_move_operations:1000	- Rate limit for controller move operations.
2025-04-14 16:44:30.928610590  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.rps_limit_node_management_operations:1000	- Rate limit for controller node management operations.
2025-04-14 16:44:30.928611340  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.rps_limit_topic_operations:1000	- Rate limit for controller topic operations.
2025-04-14 16:44:30.928612174  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.sasl_kerberos_config:/etc/krb5.conf	- The location of the Kerberos ` + "`" + `krb5.conf` + "`" + ` file for Redpanda.
2025-04-14 16:44:30.928613090  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.sasl_kerberos_keytab:/var/lib/redpanda/redpanda.keytab	- The location of the Kerberos keytab file for Redpanda.
2025-04-14 16:44:30.928613965  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.sasl_kerberos_principal:redpanda	- The primary of the Kerberos Service Principal Name (SPN) for Redpanda.
2025-04-14 16:44:30.928614882  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.sasl_kerberos_principal_mapping:{DEFAULT}	- Rules for mapping Kerberos principal names to Redpanda user principals.
2025-04-14 16:44:30.928615799  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.sasl_mechanisms:{SCRAM}	- A list of supported SASL mechanisms. Accepted values: ` + "`" + `SCRAM` + "`" + `, ` + "`" + `GSSAPI` + "`" + `, ` + "`" + `OAUTHBEARER` + "`" + `.
2025-04-14 16:44:30.928616632  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.schema_registry_normalize_on_startup:0	- Normalize schemas as they are read from the topic on startup.
2025-04-14 16:44:30.928617549  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.schema_registry_protobuf_renderer_v2:0	- Enables experimental protobuf renderer to support normalize=true.
2025-04-14 16:44:30.928618132  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.seed_server_meta_topic_partitions:	- 
2025-04-14 16:44:30.928618965  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.segment_appender_flush_timeout_ms:1000	- Maximum delay until buffered data is written.
2025-04-14 16:44:30.928619674  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.segment_fallocation_step:33554432	- Size for segments fallocation.
2025-04-14 16:44:30.928620257  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.seq_table_min_size:	- 
2025-04-14 16:44:30.928621465  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.space_management_enable:1	- Option to explicitly disable automatic disk space management. If this property was explicitly disabled while using v23.2, it will remain disabled following an upgrade.
2025-04-14 16:44:30.928622465  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.space_management_enable_override:0	- Enable automatic space management. This option is ignored and deprecated in versions >= v23.3.
2025-04-14 16:44:30.928623299  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.space_management_max_log_concurrency:20	- Maximum parallel logs inspected during space management process.
2025-04-14 16:44:30.928624215  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.space_management_max_segment_concurrency:10	- Maximum parallel segments inspected during space management process.
2025-04-14 16:44:30.928625174  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.storage_compaction_index_memory:134217728	- Maximum number of bytes that may be used on each shard by compaction index writers.
2025-04-14 16:44:30.928626424  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.storage_compaction_key_map_memory:134217728	- Maximum number of bytes that may be used on each shard by compaction key-offset maps. Only applies when ` + "`" + `log_compaction_use_sliding_window` + "`" + ` is set to ` + "`" + `true` + "`" + `.
2025-04-14 16:44:30.928628882  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.storage_compaction_key_map_memory_limit_percent:12	- Limit on ` + "`" + `storage_compaction_key_map_memory` + "`" + `, expressed as a percentage of memory per shard, that bounds the amount of memory used by compaction key-offset maps. Memory per shard is computed after ` + "`" + `data_transforms_per_core_memory_reservation` + "`" + `, and only applies when ` + "`" + `log_compaction_use_sliding_window` + "`" + ` is set to ` + "`" + `true` + "`" + `.
2025-04-14 16:44:30.928630465  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.storage_ignore_cstore_hints:0	- When set, cstore hints are ignored and not used for data access (but are otherwise generated).
2025-04-14 16:44:30.932467882  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.storage_ignore_timestamps_in_future_sec:{nullopt}	- The maximum number of seconds that a record's timestamp can be ahead of a Redpanda broker's clock and still be used when deciding whether to clean up the record for data retention. This property makes possible the timely cleanup of records from clients with clocks that are drastically unsynchronized relative to Redpanda. When determining whether to clean up a record with timestamp more than ` + "`" + `storage_ignore_timestamps_in_future_sec` + "`" + ` seconds ahead of the broker, Redpanda ignores the record's timestamp and instead uses a valid timestamp of another record in the same segment, or (if another record's valid timestamp is unavailable) the timestamp of when the segment file was last modified (mtime). By default, ` + "`" + `storage_ignore_timestamps_in_future_sec` + "`" + ` is disabled (null). To figure out whether to set ` + "`" + `storage_ignore_timestamps_in_future_sec` + "`" + ` for your system: . Look for logs with segments that are unexpectedly large and not being cleaned up. . In the logs, search for records with unsynchronized timestamps that are further into the future than tolerable by your data retention and storage settings. For example, timestamps 60 seconds or more into the future can be considered to be too unsynchronized. . If you find unsynchronized timestamps throughout your logs, determine the number of seconds that the timestamps are ahead of their actual time, and set ` + "`" + `storage_ignore_timestamps_in_future_sec` + "`" + ` to that value so data retention can proceed. . If you only find unsynchronized timestamps that are the result of transient behavior, you can disable ` + "`" + `storage_ignore_timestamps_in_future_sec` + "`" + `.
2025-04-14 16:44:30.932469674  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.storage_max_concurrent_replay:1024	- Maximum number of partitions' logs that will be replayed concurrently at startup, or flushed concurrently on shutdown.
2025-04-14 16:44:30.932470549  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.storage_min_free_bytes:5368709120	- Threshold of minimum bytes free space before rejecting producers.
2025-04-14 16:44:30.932471424  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.storage_read_buffer_size:131072	- Size of each read buffer (one per in-flight read, per log segment).
2025-04-14 16:44:30.932472299  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.storage_read_readahead_count:10	- How many additional reads to issue ahead of current read location.
2025-04-14 16:44:30.932474007  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.storage_reserve_min_segments:2	- The number of segments per partition that the system will attempt to reserve disk capacity for. For example, if the maximum segment size is configured to be 100 MB, and the value of this option is 2, then in a system with 10 partitions Redpanda will attempt to reserve at least 2 GB of disk space.
2025-04-14 16:44:30.932474924  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.storage_space_alert_free_threshold_bytes:0	- Threshold of minimum bytes free space before setting storage space alert.
2025-04-14 16:44:30.932475882  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.storage_space_alert_free_threshold_percent:5	- Threshold of minimum percent free space before setting storage space alert.
2025-04-14 16:44:30.932477257  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.storage_strict_data_init:0	- Requires that an empty file named ` + "`" + `.redpanda_data_dir` + "`" + ` be present in the broker configuration ` + "`" + `data_directory` + "`" + `. If set to ` + "`" + `true` + "`" + `, Redpanda will refuse to start if the file is not found in the data directory.
2025-04-14 16:44:30.932478340  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.storage_target_replay_bytes:10737418240	- Target bytes to replay from disk on startup after clean shutdown: controls frequency of snapshots and checkpoints.
2025-04-14 16:44:30.932479007  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.superusers:{}	- List of superuser usernames.
2025-04-14 16:44:30.932479924  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.target_fetch_quota_byte_rate:{nullopt}	- Target fetch size quota byte rate (bytes per second) - disabled default
2025-04-14 16:44:30.932480715  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.target_quota_byte_rate:0	- Target request size quota byte rate (bytes per second)
2025-04-14 16:44:30.932482299  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.tls_enable_renegotiation:0	- TLS client-initiated renegotiation is considered unsafe and is by default disabled.  Only re-enable it if you are experiencing issues with your TLS-enabled client.  This option has no effect on TLSv1.3 connections as client-initiated renegotiation was removed.
2025-04-14 16:44:30.932483590  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.tls_min_version:v1.2	- The minimum TLS version that Redpanda clusters support. This property prevents client applications from negotiating a downgrade to the TLS version when they make a connection to a Redpanda cluster.
2025-04-14 16:44:30.932484674  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.tm_sync_timeout_ms:10000	- Transaction manager's synchronization timeout. Maximum time to wait for internal state machine to catch up before rejecting a request.
2025-04-14 16:44:30.932485299  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.tm_violation_recovery_policy:	- 
2025-04-14 16:44:30.932486674  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.tombstone_retention_ms:{nullopt}	- The retention time for tombstone records in a compacted topic. Cannot be enabled at the same time as any of ` + "`" + `cloud_storage_enabled` + "`" + `, ` + "`" + `cloud_storage_enable_remote_read` + "`" + `, or ` + "`" + `cloud_storage_enable_remote_write` + "`" + `.
2025-04-14 16:44:30.932487507  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.topic_fds_per_partition:{5}	- Required file handles per partition when creating topics.
2025-04-14 16:44:30.932488299  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.topic_memory_per_partition:{4194304}	- Required memory per partition when creating topics.
2025-04-14 16:44:30.932489174  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.topic_partitions_per_shard:1000	- Maximum number of partitions which may be allocated to one shard (CPU core).
2025-04-14 16:44:30.932490340  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.topic_partitions_reserve_shard0:0	- Reserved partition slots on shard (CPU core) 0 on each node.  If this is >= topic_partitions_per_shard, no data partitions will be scheduled on shard 0
2025-04-14 16:44:30.932491465  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.transaction_coordinator_cleanup_policy:delete	- Cleanup policy for a transaction coordinator topic. Accepted Values: ` + "`" + `compact` + "`" + `, ` + "`" + `delete` + "`" + `, ` + "`" + `["compact","delete"]` + "`" + `, ` + "`" + `none` + "`" + `
2025-04-14 16:44:30.932492840  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.transaction_coordinator_delete_retention_ms:604800000	- Delete segments older than this age. To ensure transaction state is retained as long as the longest-running transaction, make sure this is no less than ` + "`" + `transactional_id_expiration_ms` + "`" + `.
2025-04-14 16:44:30.932493674  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.transaction_coordinator_log_segment_size:1073741824	- The size (in bytes) each log segment should be.
2025-04-14 16:44:30.932494507  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.transaction_coordinator_partitions:50	- Number of partitions for transactions coordinator.
2025-04-14 16:44:30.932495132  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.transaction_coordinator_replication:	- 
2025-04-14 16:44:30.932496715  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.transaction_max_timeout_ms:900000	- The maximum allowed timeout for transactions. If a client-requested transaction timeout exceeds this configuration, the broker returns an error during transactional producer initialization. This guardrail prevents hanging transactions from blocking consumer progress.
2025-04-14 16:44:30.932573382  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.transactional_id_expiration_ms:604800000	- Expiration time of producer IDs. Measured starting from the time of the last write until now for a given ID.
2025-04-14 16:44:30.932574507  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.tx_log_stats_interval_s:10000	- How often to log per partition tx stats, works only with debug logging enabled.
2025-04-14 16:44:30.932575090  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.tx_registry_log_capacity:	- 
2025-04-14 16:44:30.932575715  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.tx_registry_sync_timeout_ms:	- 
2025-04-14 16:44:30.932576549  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.tx_timeout_delay_ms:1000	- Delay before scheduling the next check for timed out transactions.
2025-04-14 16:44:30.932578215  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.unsafe_enable_consumer_offsets_delete_retention:0	- Enables delete retention of consumer offsets topic. This is an internal-only configuration and should be enabled only after consulting with Redpanda support.
2025-04-14 16:44:30.932579215  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.usage_disk_persistance_interval_sec:300000	- The interval in which all usage stats are written to disk.
2025-04-14 16:44:30.932579965  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.usage_num_windows:24	- The number of windows to persist in memory and disk.
2025-04-14 16:44:30.932580965  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.usage_window_width_interval_sec:3600000	- The width of a usage window, tracking cloud and kafka ingress/egress traffic each interval.
2025-04-14 16:44:30.932581757  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.use_fetch_scheduler_group:1	- Use a separate scheduler group for fetch processing.
2025-04-14 16:44:30.932582299  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.use_scheduling_groups:	- 
2025-04-14 16:44:30.932583215  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.virtual_cluster_min_producer_ids:18446744073709551615	- Minimum number of active producers per virtual cluster.
2025-04-14 16:44:30.932584007  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.wait_for_leader_timeout_ms:5000	- Timeout to wait for leadership in metadata cache.
2025-04-14 16:44:30.932587132  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.write_caching_default:false	- The default write caching mode to apply to user topics. Write caching acknowledges a message as soon as it is received and acknowledged on a majority of brokers, without waiting for it to be written to disk. With ` + "`" + `acks=all` + "`" + `, this provides lower latency while still ensuring that a majority of brokers acknowledge the write. Fsyncs follow ` + "`" + `raft_replica_max_pending_flush_bytes` + "`" + ` and ` + "`" + `raft_replica_max_flush_delay_ms` + "`" + `, whichever is reached first. The ` + "`" + `write_caching_default` + "`" + ` cluster property can be overridden with the ` + "`" + `write.caching` + "`" + ` topic property. Accepted values: * ` + "`" + `true` + "`" + ` * ` + "`" + `false` + "`" + ` * ` + "`" + `disabled` + "`" + `: This takes precedence over topic overrides and disables write caching for the entire cluster.
2025-04-14 16:44:30.932587924  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:849 - redpanda.zstd_decompress_workspace_bytes:8388608	- Size of the zstd decompression workspace.
2025-04-14 16:44:30.932588507  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:900 - Node configuration properties:
2025-04-14 16:44:30.932589174  INFO  2025-04-14 16:44:30,927 [shard 0:main] main - application.cc:901 - (use ` + "`" + `rpk redpanda config set <cfg> <value>` + "`" + ` to change)
2025-04-14 16:44:30.932589965  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.admin:{{:{host: 0.0.0.0, port: 9644}}}	- Network address for the Admin API[] server.
2025-04-14 16:44:30.932590840  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.admin_api_doc_dir:/usr/share/redpanda/admin-api-doc	- Path to the API specifications for the Admin API.
2025-04-14 16:44:30.932591590  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.admin_api_tls:{}	- Specifies the TLS configuration for the HTTP Admin API.
2025-04-14 16:44:30.932592465  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.advertised_kafka_api:{{:{host: 127.0.0.1, port: 9092}}}	- Address of Kafka API published to the clients
2025-04-14 16:44:30.932593340  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.advertised_rpc_api:{{host: 127.0.0.1, port: 33145}}	- Address of RPC endpoint published to other cluster members
2025-04-14 16:44:30.932594340  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_cache_directory:{nullopt}	- Directory for archival cache. Should be present when ` + "`" + `cloud_storage_enabled` + "`" + ` is present
2025-04-14 16:44:30.932595340  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.cloud_storage_inventory_hash_path_directory:{nullopt}	- Directory to store inventory report hashes for use by cloud storage scrubber
2025-04-14 16:44:30.932595924  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.coproc_supervisor_server:	- 
2025-04-14 16:44:30.932597674  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.crash_loop_limit:{5}	- A limit on the number of consecutive times a broker can crash within one hour before its crash-tracking logic is reset. This limit prevents a broker from getting stuck in an infinite cycle of crashes. For more information see https://docs.redpanda.com/current/reference/properties/broker-properties/#crash_loop_limit.
2025-04-14 16:44:30.932599215  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.crash_loop_sleep_sec:{nullopt}	- The amount of time the broker sleeps before terminating the process when it reaches the number of consecutive times a broker can crash. For more information, see https://docs.redpanda.com/current/reference/properties/broker-properties/#crash_loop_limit.
2025-04-14 16:44:30.932599799  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.dashboard_dir:	- 
2025-04-14 16:44:30.932600715  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.data_directory:{data_directory="/data/redpanda"}	- Path to the directory for storing Redpanda's streaming data files.
2025-04-14 16:44:30.932601549  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.developer_mode:1	- Skips most of the checks performed at startup, not recomended for production use
2025-04-14 16:44:30.932602674  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.emergency_disable_data_transforms:0	- Override the cluster property ` + "`" + `data_transforms_enabled` + "`" + ` and disable Wasm-powered data transforms. This is an emergency shutoff button.
2025-04-14 16:44:30.932604549  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.empty_seed_starts_cluster:1	- Controls how a new cluster is formed. All brokers in a cluster must have the same value. See how the ` + "`" + `empty_seed_starts_cluster` + "`" + ` setting works with the ` + "`" + `seed_servers` + "`" + ` setting to form a cluster. For backward compatibility, ` + "`" + `true` + "`" + ` is the default. Redpanda recommends using ` + "`" + `false` + "`" + ` in production environments to prevent accidental cluster formation.
2025-04-14 16:44:30.932605174  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.enable_central_config:	- 
2025-04-14 16:44:30.932608299  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.fips_mode:disabled	- Controls whether Redpanda starts in FIPS mode.  This property allows for three values: ` + "`" + `disabled` + "`" + ` - Redpanda does not start in FIPS mode. ` + "`" + `permissive` + "`" + ` - Redpanda performs the same check as enabled, but a warning is logged, and Redpanda continues to run. Redpanda loads the OpenSSL FIPS provider into the OpenSSL library. After this completes, Redpanda is operating in FIPS mode, which means that the TLS cipher suites available to users are limited to the TLSv1.2 and TLSv1.3 NIST-approved cryptographic methods. ` + "`" + `enabled` + "`" + ` - Redpanda verifies that the operating system is enabled for FIPS by checking ` + "`" + `/proc/sys/crypto/fips_enabled` + "`" + `. If the file does not exist or does not return ` + "`" + `1` + "`" + `, Redpanda immediately exits.
2025-04-14 16:44:30.932609257  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.kafka_api:{{:{host: 0.0.0.0, port: 9092}:{nullopt}}}	- IP address and port of the Kafka API endpoint that handles requests.
2025-04-14 16:44:30.932610090  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.kafka_api_tls:{}	- Transport Layer Security (TLS) configuration for the Kafka API endpoint.
2025-04-14 16:44:30.932738382  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.memory_allocation_warning_threshold:{131073}	- Threshold for log messages that contain a larger memory allocation than specified.
2025-04-14 16:44:30.932740090  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.node_id:{nullopt}	- A number that uniquely identifies the broker within the cluster. If ` + "`" + `null` + "`" + ` (the default value), Redpanda automatically assigns an ID. If set, it must be non-negative value. The ` + "`" + `node_id` + "`" + ` property must not be changed after a broker joins the cluster.
2025-04-14 16:44:30.932741507  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.node_id_overrides:{}	- List of node ID and UUID overrides to be applied at broker startup. Each entry includes the current UUID and desired ID and UUID. Each entry applies to a given node if and only if 'current' matches that node's current UUID.
2025-04-14 16:44:30.932742465  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.openssl_config_file:{nullopt}	- Path to the configuration file used by OpenSSL to properly load the FIPS-compliant module.
2025-04-14 16:44:30.932743507  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.openssl_module_directory:{nullopt}	- Path to the directory that contains the OpenSSL FIPS-compliant module. The filename that Redpanda looks for is ` + "`" + `fips.so` + "`" + `.
2025-04-14 16:44:30.932745007  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.rack:{nullopt}	- A label that identifies a failure zone. Apply the same label to all brokers in the same failure zone. When ` + "`" + `enable_rack_awareness` + "`" + ` is set to ` + "`" + `true` + "`" + ` at the cluster level, the system uses the rack labels to spread partition replicas across different failure zones.
2025-04-14 16:44:30.932746049  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.recovery_mode_enabled:0	- If ` + "`" + `true` + "`" + `, start Redpanda in recovery mode, where user partitions are not loaded and only administrative operations are allowed.
2025-04-14 16:44:30.932746924  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.rpc_server:{host: 0.0.0.0, port: 33145}	- IP address and port for the Remote Procedure Call (RPC) server.
2025-04-14 16:44:30.932747965  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.rpc_server_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} crl file: {nullopt} client_auth_required: 0 }	- TLS configuration for the RPC server.
2025-04-14 16:44:30.932753340  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.seed_servers:{}	- List of the seed servers used to join current cluster. If the ` + "`" + `seed_servers` + "`" + ` list is empty the node will be a cluster root and it will form a new cluster. When ` + "`" + `empty_seed_starts_cluster` + "`" + ` is ` + "`" + `true` + "`" + `, Redpanda enables one broker with an empty ` + "`" + `seed_servers` + "`" + ` list to initiate a new cluster. The broker with an empty ` + "`" + `seed_servers` + "`" + ` becomes the cluster root, to which other brokers must connect to join the cluster.  Brokers looking to join the cluster should have their ` + "`" + `seed_servers` + "`" + ` populated with the cluster root's address, facilitating their connection to the cluster. Only one broker, the designated cluster root, should have an empty ` + "`" + `seed_servers` + "`" + ` list during the initial cluster bootstrapping. This ensures a single initiation point for cluster formation. When ` + "`" + `empty_seed_starts_cluster` + "`" + ` is ` + "`" + `false` + "`" + `, Redpanda requires all brokers to start with a known set of brokers listed in ` + "`" + `seed_servers` + "`" + `. The ` + "`" + `seed_servers` + "`" + ` list must not be empty and should be identical across these initial seed brokers, containing the addresses of all seed brokers. Brokers not included in the ` + "`" + `seed_servers` + "`" + ` list use it to discover and join the cluster, allowing for expansion beyond the foundational members. The ` + "`" + `seed_servers` + "`" + ` list must be consistent across all seed brokers to prevent cluster fragmentation and ensure stable cluster formation.
2025-04-14 16:44:30.932754299  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.storage_failure_injection_config_path:{nullopt}	- Path to the configuration file used for low level storage failure injection.
2025-04-14 16:44:30.932755299  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.storage_failure_injection_enabled:0	- If ` + "`" + `true` + "`" + `, inject low level storage failures on the write path. Do _not_ use for production instances.
2025-04-14 16:44:30.932756299  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.upgrade_override_checks:0	- Whether to violate safety checks when starting a Redpanda version newer than the cluster's consensus version.
2025-04-14 16:44:30.932757715  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - redpanda.verbose_logging_timeout_sec_max:{nullopt}	- Maximum duration in seconds for verbose (` + "`" + `TRACE` + "`" + ` or ` + "`" + `DEBUG` + "`" + `) logging. Values configured above this will be clamped. If null (the default) there is no limit. Can be overridden in the Admin API on a per-request basis.
2025-04-14 16:44:30.932758590  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy.advertised_pandaproxy_api:{}	- Network address for the HTTP Proxy API server to publish to clients.
2025-04-14 16:44:30.932759424  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy.api_doc_dir:/usr/share/redpanda/proxy-api-doc	- Path to the API specifications for the HTTP Proxy API.
2025-04-14 16:44:30.932761049  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy.client_cache_max_size:10	- The maximum number of Kafka client connections that Redpanda can cache in the LRU (least recently used) cache. The LRU cache helps optimize resource utilization by keeping the most recently used clients in memory, facilitating quicker reconnections for frequent clients while limiting memory usage.
2025-04-14 16:44:30.932762007  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy.client_keep_alive:300000	- Time, in milliseconds, that an idle client connection may remain open to the HTTP Proxy API.
2025-04-14 16:44:30.932763090  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy.consumer_instance_timeout_ms:300000	- How long to wait for an idle consumer before removing it. A consumer is considered idle when it's not making requests or heartbeats.
2025-04-14 16:44:30.932763965  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy.pandaproxy_api:{{:{host: 0.0.0.0, port: 8082}:<nullopt>}}	- Rest API listen address and port
2025-04-14 16:44:30.932764674  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy.pandaproxy_api_tls:{}	- TLS configuration for Pandaproxy api.
2025-04-14 16:44:30.932765965  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.broker_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} crl file: {nullopt} client_auth_required: 0 }	- TLS configuration for the Kafka API servers to which the HTTP Proxy client should connect.
2025-04-14 16:44:30.932766924  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.brokers:{{host: 0.0.0.0, port: 9092}}	- Network addresses of the Kafka API servers to which the HTTP Proxy client should connect.
2025-04-14 16:44:30.932768132  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.client_identifier:{pandaproxy_client}	- Custom identifier to include in the Kafka request header for the HTTP Proxy client. This identifier can help debug or monitor client activities.
2025-04-14 16:44:30.932768965  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.consumer_heartbeat_interval_ms:500	- Interval (in milliseconds) for consumer heartbeats.
2025-04-14 16:44:30.932769799  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.consumer_rebalance_timeout_ms:2000	- Timeout (in milliseconds) for consumer rebalance.
2025-04-14 16:44:30.932770590  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.consumer_request_max_bytes:1048576	- Maximum bytes to fetch per request.
2025-04-14 16:44:30.932771340  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.consumer_request_min_bytes:1	- Minimum bytes to fetch per request.
2025-04-14 16:44:30.932772215  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.consumer_request_timeout_ms:100	- Interval (in milliseconds) for consumer request timeout.
2025-04-14 16:44:30.932773049  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.consumer_session_timeout_ms:300000	- Timeout (in milliseconds) for consumer session.
2025-04-14 16:44:30.932845549  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.produce_ack_level:-1	- Number of acknowledgments the producer requires the leader to have received before considering a request complete.
2025-04-14 16:44:30.932846549  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.produce_batch_delay_ms:100	- Delay (in milliseconds) to wait before sending batch.
2025-04-14 16:44:30.932847340  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.produce_batch_record_count:1000	- Number of records to batch before sending to broker.
2025-04-14 16:44:30.932848174  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.produce_batch_size_bytes:1048576	- Number of bytes to batch before sending to broker.
2025-04-14 16:44:30.932849299  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.produce_compression_type:none	- Enable or disable compression by the Kafka client. Specify ` + "`" + `none` + "`" + ` to disable compression or one of the supported types [gzip, snappy, lz4, zstd].
2025-04-14 16:44:30.932850215  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.produce_shutdown_delay_ms:0	- Delay (in milliseconds) to allow for final flush of buffers before shutting down.
2025-04-14 16:44:30.932850965  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.retries:5	- Number of times to retry a request to a broker.
2025-04-14 16:44:30.932851757  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.retry_base_backoff_ms:100	- Delay (in milliseconds) for initial retry backoff.
2025-04-14 16:44:30.932852507  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.sasl_mechanism:	- The SASL mechanism to use when connecting.
2025-04-14 16:44:30.932853257  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.scram_password:	- Password to use for SCRAM authentication mechanisms.
2025-04-14 16:44:30.932854007  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - pandaproxy_client.scram_username:	- Username to use for SCRAM authentication mechanisms.
2025-04-14 16:44:30.932854757  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry.api_doc_dir:/usr/share/redpanda/proxy-api-doc	- API doc directory
2025-04-14 16:44:30.932856340  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry.mode_mutability:1	- Enable modifications to the read-only ` + "`" + `mode` + "`" + ` of the Schema Registry.When set to ` + "`" + `true` + "`" + `, the entire Schema Registry or its subjects can be switched to ` + "`" + `READONLY` + "`" + ` or ` + "`" + `READWRITE` + "`" + `. This property is useful for preventing unwanted changes to the entire Schema Registry or specific subjects.
2025-04-14 16:44:30.932857257  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry.schema_registry_api:{{:{host: 0.0.0.0, port: 8081}:<nullopt>}}	- Schema Registry API listener address and port
2025-04-14 16:44:30.932858007  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry.schema_registry_api_tls:{}	- TLS configuration for Schema Registry API.
2025-04-14 16:44:30.932859049  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry.schema_registry_replication_factor:{nullopt}	- Replication factor for internal ` + "`" + `_schemas` + "`" + ` topic.  If unset, defaults to ` + "`" + `default_topic_replication` + "`" + `.
2025-04-14 16:44:30.932860299  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.broker_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} crl file: {nullopt} client_auth_required: 0 }	- TLS configuration for the Kafka API servers to which the HTTP Proxy client should connect.
2025-04-14 16:44:30.932861340  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.brokers:{{host: 0.0.0.0, port: 9092}}	- Network addresses of the Kafka API servers to which the HTTP Proxy client should connect.
2025-04-14 16:44:30.932862549  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.client_identifier:{schema_registry_client}	- Custom identifier to include in the Kafka request header for the HTTP Proxy client. This identifier can help debug or monitor client activities.
2025-04-14 16:44:30.932863382  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.consumer_heartbeat_interval_ms:500	- Interval (in milliseconds) for consumer heartbeats.
2025-04-14 16:44:30.932864215  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.consumer_rebalance_timeout_ms:2000	- Timeout (in milliseconds) for consumer rebalance.
2025-04-14 16:44:30.932865007  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.consumer_request_max_bytes:1048576	- Maximum bytes to fetch per request.
2025-04-14 16:44:30.932865757  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.consumer_request_min_bytes:1	- Minimum bytes to fetch per request.
2025-04-14 16:44:30.932866632  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.consumer_request_timeout_ms:100	- Interval (in milliseconds) for consumer request timeout.
2025-04-14 16:44:30.932867424  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.consumer_session_timeout_ms:10000	- Timeout (in milliseconds) for consumer session.
2025-04-14 16:44:30.932868465  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.produce_ack_level:-1	- Number of acknowledgments the producer requires the leader to have received before considering a request complete.
2025-04-14 16:44:30.932869632  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.produce_batch_delay_ms:0	- Delay (in milliseconds) to wait before sending batch.
2025-04-14 16:44:30.932870590  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.produce_batch_record_count:0	- Number of records to batch before sending to broker.
2025-04-14 16:44:30.932871382  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.produce_batch_size_bytes:0	- Number of bytes to batch before sending to broker.
2025-04-14 16:44:30.932872549  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.produce_compression_type:none	- Enable or disable compression by the Kafka client. Specify ` + "`" + `none` + "`" + ` to disable compression or one of the supported types [gzip, snappy, lz4, zstd].
2025-04-14 16:44:30.932873465  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.produce_shutdown_delay_ms:0	- Delay (in milliseconds) to allow for final flush of buffers before shutting down.
2025-04-14 16:44:30.932874215  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.retries:5	- Number of times to retry a request to a broker.
2025-04-14 16:44:30.932875007  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.retry_base_backoff_ms:100	- Delay (in milliseconds) for initial retry backoff.
2025-04-14 16:44:30.932875757  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.sasl_mechanism:	- The SASL mechanism to use when connecting.
2025-04-14 16:44:30.932876549  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.scram_password:	- Password to use for SCRAM authentication mechanisms.
2025-04-14 16:44:30.932877340  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - schema_registry_client.scram_username:	- Username to use for SCRAM authentication mechanisms.
2025-04-14 16:44:30.932878590  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.broker_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} crl file: {nullopt} client_auth_required: 0 }	- TLS configuration for the Kafka API servers to which the HTTP Proxy client should connect.
2025-04-14 16:44:30.932879882  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.brokers:{{host: 0.0.0.0, port: 9092}}	- Network addresses of the Kafka API servers to which the HTTP Proxy client should connect.
2025-04-14 16:44:30.932881174  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.client_identifier:{audit_log_client}	- Custom identifier to include in the Kafka request header for the HTTP Proxy client. This identifier can help debug or monitor client activities.
2025-04-14 16:44:30.932882340  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.consumer_heartbeat_interval_ms:500	- Interval (in milliseconds) for consumer heartbeats.
2025-04-14 16:44:30.932883299  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.consumer_rebalance_timeout_ms:2000	- Timeout (in milliseconds) for consumer rebalance.
2025-04-14 16:44:30.932993299  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.consumer_request_max_bytes:1048576	- Maximum bytes to fetch per request.
2025-04-14 16:44:30.932994257  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.consumer_request_min_bytes:1	- Minimum bytes to fetch per request.
2025-04-14 16:44:30.932995132  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.consumer_request_timeout_ms:100	- Interval (in milliseconds) for consumer request timeout.
2025-04-14 16:44:30.932995965  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.consumer_session_timeout_ms:10000	- Timeout (in milliseconds) for consumer session.
2025-04-14 16:44:30.932997007  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.produce_ack_level:1	- Number of acknowledgments the producer requires the leader to have received before considering a request complete.
2025-04-14 16:44:30.932997840  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.produce_batch_delay_ms:0	- Delay (in milliseconds) to wait before sending batch.
2025-04-14 16:44:30.932998674  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.produce_batch_record_count:0	- Number of records to batch before sending to broker.
2025-04-14 16:44:30.932999507  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.produce_batch_size_bytes:0	- Number of bytes to batch before sending to broker.
2025-04-14 16:44:30.933000674  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.produce_compression_type:zstd	- Enable or disable compression by the Kafka client. Specify ` + "`" + `none` + "`" + ` to disable compression or one of the supported types [gzip, snappy, lz4, zstd].
2025-04-14 16:44:30.933001590  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.produce_shutdown_delay_ms:3000	- Delay (in milliseconds) to allow for final flush of buffers before shutting down.
2025-04-14 16:44:30.933002340  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.retries:5	- Number of times to retry a request to a broker.
2025-04-14 16:44:30.933003174  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.retry_base_backoff_ms:100	- Delay (in milliseconds) for initial retry backoff.
2025-04-14 16:44:30.933003924  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.sasl_mechanism:	- The SASL mechanism to use when connecting.
2025-04-14 16:44:30.933004715  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.scram_password:	- Password to use for SCRAM authentication mechanisms.
2025-04-14 16:44:30.933005507  INFO  2025-04-14 16:44:30,928 [shard 0:main] main - application.cc:849 - audit_log_client.scram_username:	- Username to use for SCRAM authentication mechanisms.
2025-04-14 16:44:30.933006090  INFO  2025-04-14 16:44:30,932 [shard 0:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate
2025-04-14 16:44:30.933006757  INFO  2025-04-14 16:44:30,932 [shard 0:main] main - application.cc:563 - Setting abort_on_allocation_failure (abort on OOM): true
2025-04-14 16:44:30.969191965  INFO  2025-04-14 16:44:30,969 [shard 0:main] syschecks - Writing pid file "/data/redpanda/pid.lock"
2025-04-14 16:44:30.980700090  INFO  2025-04-14 16:44:30,980 [shard 0:main] storage - api.cc:70 - Checking ` + "`" + `/data/redpanda` + "`" + ` for supported filesystems
2025-04-14 16:44:30.981282049  INFO  2025-04-14 16:44:30,981 [shard 0:main] syschecks - Detected file system type is other
2025-04-14 16:44:30.981283965  ERROR 2025-04-14 16:44:30,981 [shard 0:main] syschecks - Path: ` + "`" + `/data/redpanda' uses other filesystem which is not XFS or ext4. This is a unsupported configuration. You may experience poor performance or instability.
2025-04-14 16:44:30.982140132  INFO  2025-04-14 16:44:30,982 [shard 0:main] cloud_storage - cache_service.cc:1945 - Creating cache directory "/data/redpanda/cloud_storage_cache"
2025-04-14 16:44:30.992989382  INFO  2025-04-14 16:44:30,992 [shard 0:main] ossl-library-context-service - ossl_context_service.cc:253 - OpenSSL Context loaded and ready
2025-04-14 16:44:31.060284091  Reactor stalled for 67 ms on shard 0. Backtrace: 0x9539ab7 0x953b3ab 0x79f 0x94958f7 0x949853b 0x948b35f 0x8fbf913 0x8fbffeb 0x9557557 0x9559c2b 0x9557d37 0x946aaab 0x94692b3 0x360e207 0x9abbddf 0x30a1b 0x30afb 0x3607eef
2025-04-14 16:44:31.382291549  Reactor stalled for 244 ms on shard 0. Backtrace: 0x9539ab7 0x953b3ab 0x79f 0x94958f7 0x949853b 0x948b35f 0x8fbf913 0x8fbffeb 0x9557557 0x9559c2b 0x9557d37 0x946aaab 0x94692b3 0x360e207 0x9abbddf 0x30a1b 0x30afb 0x3607eef
2025-04-14 16:44:31.393999007  INFO  2025-04-14 16:44:31,393 [shard 0:main] rpc - server.cc:43 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 402653184, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}
2025-04-14 16:44:31.400326966  INFO  2025-04-14 16:44:31,400 [shard 0:main] features - feature_table.cc:495 - Activating features from bootstrap version 11
2025-04-14 16:44:31.400329299  INFO  2025-04-14 16:44:31,400 [shard 0:main] main - application.cc:2678 - Generated new UUID for node: ad4da55e-dce8-4aa0-b45b-0df44a6c6003
2025-04-14 16:44:31.411253382  INFO  2025-04-14 16:44:31,411 [shard 0:main] storage - segment.cc:817 - Creating new segment /data/redpanda/redpanda/kvstore/0_0/0-0-v1.log
2025-04-14 16:44:31.422162924  INFO  2025-04-14 16:44:31,422 [shard 0:main] main - application.cc:2727 - Started RPC server listening at {host: 0.0.0.0, port: 33145}
2025-04-14 16:44:31.423232466  INFO  2025-04-14 16:44:31,423 [shard 0:main] main - application.cc:2833 - Starting Redpanda with node_id 0, cluster UUID {nullopt}
2025-04-14 16:44:31.423516299  INFO  2025-04-14 16:44:31,423 [shard 0:main] raft - coordinated_recovery_throttle.cc:126 - Starting recovery throttle, rate: 104857600
2025-04-14 16:44:31.423825757  INFO  2025-04-14 16:44:31,423 [shard 0:main] cluster - producer_state_manager.cc:45 - Started producer state manager
2025-04-14 16:44:31.423827549  INFO  2025-04-14 16:44:31,423 [shard 0:main] main - application.cc:1730 - Partition manager started
2025-04-14 16:44:31.426289299  INFO  2025-04-14 16:44:31,426 [shard 0:main] main - application.cc:1818 - Archiver service setup, cloud_storage_enabled: false, legacy_upload_mode_enabled: true
2025-04-14 16:44:31.428033091  INFO  2025-04-14 16:44:31,427 [shard 0:main] resource_mgmt - storage.cc:182 - Setting new target log data size 276.259GiB. Disk size 460.432GiB reservation percent 25 target percent {80} bytes {nullopt}
2025-04-14 16:44:31.428965757  INFO  2025-04-14 16:44:31,428 [shard 0:main] kafka - server.cc:43 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 603979776, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}
2025-04-14 16:44:31.451109841  INFO  2025-04-14 16:44:31,451 [shard 0:main] cluster - controller.cc:1180 - persisted initial configuration invariants: { version: 0, node_id: 0, core_count: 1 }
2025-04-14 16:44:31.451417882  INFO  2025-04-14 16:44:31,451 [shard 0:main] cluster - raft0_utils.h:30 - Current node is a cluster founder
2025-04-14 16:44:31.490668924  INFO  2025-04-14 16:44:31,490 [shard 0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1407 - Starting with voted_for {id: -2147483648, revision: -9223372036854775808} term 0 initial_state true
2025-04-14 16:44:31.521721216  INFO  2025-04-14 16:44:31,521 [shard 0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1451 - Current log offsets: {start_offset:-9223372036854775808, committed_offset:-9223372036854775808, committed_offset_term:-9223372036854775808, dirty_offset:-9223372036854775808, dirty_offset_term:-9223372036854775808}, read bootstrap state: data_seen 0 config_seen 0 eol false commit 0 term 0 prev_idx 0 prev_term 0 config_tracker -9223372036854775808 commit_base_tracker -9223372036854775808 configurations []
2025-04-14 16:44:31.521724882  INFO  2025-04-14 16:44:31,521 [shard 0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1478 - Truncating configurations at -9223372036854775808
2025-04-14 16:44:31.552879799  INFO  2025-04-14 16:44:31,552 [shard 0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:940 - starting pre-vote leader election, current term: 0, leadership transfer: false
2025-04-14 16:44:31.568671549  INFO  2025-04-14 16:44:31,568 [shard 0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1589 - started raft, log offsets: {start_offset:-9223372036854775808, committed_offset:-9223372036854775808, committed_offset_term:-9223372036854775808, dirty_offset:-9223372036854775808, dirty_offset_term:-9223372036854775808}, term: 0, configuration: {current: {voters: {{id: 0, revision: 0}}, learners: {}}, old:{nullopt}, revision: 0, update: {nullopt}, version: 5}}
2025-04-14 16:44:31.570688007  INFO  2025-04-14 16:44:31,570 [shard 0:main] cluster - drain_manager.cc:21 - Drain manager starting
2025-04-14 16:44:31.570929799  INFO  2025-04-14 16:44:31,570 [shard 0:main] cluster - members_manager.cc:98 - starting  members manager with founding brokers: {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 1, mem_available 2147483648, disk_available 460, in_fips_mode disabled}}}
2025-04-14 16:44:31.571186507  INFO  2025-04-14 16:44:31,571 [shard 0:main] cluster - controller.cc:572 - Controller log replay starting (to offset -9223372036854775808)
2025-04-14 16:44:31.571187466  INFO  2025-04-14 16:44:31,571 [shard 0:main] cluster - controller.cc:583 - Controller log replay complete.
2025-04-14 16:44:31.584672632  INFO  2025-04-14 16:44:31,584 [shard 0:main] raft - [group_id:0, {redpanda/controller/0}] vote_stm.cc:421 - becoming the leader term:1
2025-04-14 16:44:31.585232216  INFO  2025-04-14 16:44:31,585 [shard 0:main] storage - segment.cc:817 - Creating new segment /data/redpanda/redpanda/controller/0_0/0-1-v1.log
2025-04-14 16:44:31.614887716  INFO  2025-04-14 16:44:31,614 [shard 0:main] raft - [group_id:0, {redpanda/controller/0}] vote_stm.cc:436 - became the leader term: 1
2025-04-14 16:44:31.673745841  INFO  2025-04-14 16:44:31,673 [shard 0:main] cluster - controller.cc:981 - Creating cluster UUID 95b2b3d8-1f26-4368-80f8-833f6e7cea4b
2025-04-14 16:44:31.677634924  INFO  2025-04-14 16:44:31,677 [shard 0:main] cluster - bootstrap_backend.cc:92 - Applying update to bootstrap_manager
2025-04-14 16:44:31.678015882  INFO  2025-04-14 16:44:31,677 [shard 0:main] cluster - members_manager.cc:855 - initializing cluster state with initial brokers [{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 1, mem_available 2147483648, disk_available 460, in_fips_mode disabled}}], and node UUID map: {{ad4da55e-dce8-4aa0-b45b-0df44a6c6003 -> 0}} at offset: 1
2025-04-14 16:44:31.678245799  INFO  2025-04-14 16:44:31,678 [shard 0:main] cluster - members_table.cc:114 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 1, mem_available 2147483648, disk_available 460, in_fips_mode disabled}}}
2025-04-14 16:44:31.692289424  INFO  2025-04-14 16:44:31,692 [shard 0:main] features - feature_table.cc:495 - Activating features from bootstrap version 14
2025-04-14 16:44:31.692439382  INFO  2025-04-14 16:44:31,692 [shard 0:main] cluster - feature_backend.cc:149 - Saving feature_table_snapshot at version 14...
2025-04-14 16:44:31.722638466  INFO  2025-04-14 16:44:31,722 [shard 0:main] cluster - controller.cc:989 - Cluster UUID created 95b2b3d8-1f26-4368-80f8-833f6e7cea4b
2025-04-14 16:44:31.722934216  INFO  2025-04-14 16:44:31,722 [shard 0:main] cluster - shard_placement_table.cc:293 - enabling table persistence...
2025-04-14 16:44:31.737951507  INFO  2025-04-14 16:44:31,737 [shard 0:main] cluster - shard_balancer.cc:526 - detected core count change, triggering rebalance...
2025-04-14 16:44:31.738228257  INFO  2025-04-14 16:44:31,737 [shard 0:main] cluster - shard_balancer.cc:575 - after balancing 0 ntps were reassigned
2025-04-14 16:44:31.752992508  INFO  2025-04-14 16:44:31,752 [shard 0:main] cluster - controller_backend.cc:833 - Cleaning up orphan topic files. bootstrap_revision: -9223372036854775808
2025-04-14 16:44:31.753280216  INFO  2025-04-14 16:44:31,753 [shard 0:main] cluster - feature_manager.cc:100 - Starting...
2025-04-14 16:44:31.753281466  INFO  2025-04-14 16:44:31,753 [shard 0:main] cluster - feature_manager.cc:625 - Activating features after upgrade...
2025-04-14 16:44:31.753282383  INFO  2025-04-14 16:44:31,753 [shard 0:main] cluster - feature_manager.cc:634 - Activating feature broker_time_based_retention (logical version 14)
2025-04-14 16:44:31.753620258  INFO  2025-04-14 16:44:31,753 [shard 0:main] cluster - metrics_reporter.cc:342 - Waiting to initialize cluster metrics ID...
2025-04-14 16:44:31.753724508  INFO  2025-04-14 16:44:31,753 [shard 0:main] cluster - partition_balancer_backend.cc:108 - partition balancer started
2025-04-14 16:44:31.753725466  INFO  2025-04-14 16:44:31,753 [shard 0:main] data-migrate - data_migration_backend.cc:107 - backend starting
2025-04-14 16:44:31.753726258  INFO  2025-04-14 16:44:31,753 [shard 0:main] data-migrate - data_migration_backend.cc:160 - backend not started as cloud_storage_api is not available
2025-04-14 16:44:31.754444549  INFO  2025-04-14 16:44:31,754 [shard 0:main] cluster - leader_balancer.cc:112 - Leader balancer: controller leadership detected. Starting rebalancer in 30 seconds
2025-04-14 16:44:31.757288549  INFO  2025-04-14 16:44:31,757 [shard 0:main] cluster - config_manager.cc:164 - Importing property auto_create_topics_enabled:1
2025-04-14 16:44:31.758199549  ERROR 2025-04-14 16:44:31,758 [shard 0:main] debug-bundle-service - Current specified RPK location /usr/bin/rpk does not exist!  Debug bundle creation is not available until this is fixed!
2025-04-14 16:44:31.763048424  WARN  2025-04-14 16:44:31,763 [shard 0:main] admin_api_server - server.cc:547 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling ` + "`" + `admin_api_require_auth` + "`" + `
2025-04-14 16:44:31.763050466  INFO  2025-04-14 16:44:31,763 [shard 0:main] admin_api_server - server.cc:353 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}
2025-04-14 16:44:31.763182799  INFO  2025-04-14 16:44:31,763 [shard 0:main] resource_mgmt - storage.cc:73 - Starting disk space manager service (enabled)
2025-04-14 16:44:31.766260674  INFO  2025-04-14 16:44:31,766 [shard 0:main] main - application.cc:2874 - Started Pandaproxy listening at {{:{host: 0.0.0.0, port: 8082}:<nullopt>}}
2025-04-14 16:44:31.769187424  INFO  2025-04-14 16:44:31,769 [shard 0:main] cluster - feature_backend.cc:149 - Saving feature_table_snapshot at version 14...
2025-04-14 16:44:31.769603799  INFO  2025-04-14 16:44:31,769 [shard 0:main] main - application.cc:2882 - Started Schema Registry listening at {{:{host: 0.0.0.0, port: 8081}:<nullopt>}}
2025-04-14 16:44:31.769605424  INFO  2025-04-14 16:44:31,769 [shard 0:main] main - application.cc:3259 - Waiting for cluster membership
2025-04-14 16:44:31.769606633  INFO  2025-04-14 16:44:31,769 [shard 0:main] main - application.cc:3277 - Waiting for Cluster ID to initialize...
2025-04-14 16:44:31.785739633  INFO  2025-04-14 16:44:31,785 [shard 0:main] cluster - metrics_reporter.cc:390 - Generated cluster metrics ID 2656ec5b-85d0-4ef7-a7a8-8b8222d87855
2025-04-14 16:44:31.788660883  INFO  2025-04-14 16:44:31,788 [shard 0:main] cluster - config_manager.cc:136 - Completed bootstrap as leader
2025-04-14 16:44:31.788663383  INFO  2025-04-14 16:44:31,788 [shard 0:main] cluster - config_manager.cc:124 - Bootstrap complete (version 1)
2025-04-14 16:44:31.796778049  INFO  2025-04-14 16:44:31,796 [shard 0:main] main - application.cc:3288 - Started Kafka API server listening at {{:{host: 0.0.0.0, port: 9092}:{nullopt}}}
2025-04-14 16:44:31.796782174  INFO  2025-04-14 16:44:31,796 [shard 0:main] main - application.cc:2925 - Successfully started Redpanda!
2025-04-14 16:44:31.802102216  INFO  2025-04-14 16:44:31,802 [shard 0:main] cluster - metrics_reporter.cc:420 - Initialized cluster_id to 2656ec5b-85d0-4ef7-a7a8-8b8222d87855`
