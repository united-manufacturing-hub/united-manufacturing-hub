name: Test UMH Tiny Deployment

on:
  pull_request:

env:
  CHART_DIR: ./deployment/united-manufacturing-hub
  HELM_TIMEOUT: 10m
  K3D_VERSION: v5.4.6
  K3S_VERSION: v1.24.4-k3s1
  VALUES_FILE: ./test/test-values-tiny.yaml
  UMH: united-manufacturing-hub

jobs:
  data-flow-test:
    name: Data Flow Test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - name: Prepare k3d cluster
        id: prepare-k3d
        uses: AbsaOSS/k3d-action@v2
        with:
          cluster-name: "umh-tiny"
          k3d-version: ${{ env.K3D_VERSION }}
          args: --agents 1 --image rancher/k3s:${{ env.K3S_VERSION }}
      - name: Install UMH
        id: install-umh
        run: |
          kubectl create ns ${{ env.UMH }} || true
          helm install ${{ env.UMH }} ${{ env.CHART_DIR }} -f ${{ env.VALUES_FILE }} -n ${{ env.UMH }} --wait --timeout ${{ env.HELM_TIMEOUT }} --debug
          kubectl get po,svc -n ${{ env.UMH }}
      - name: Debug Install UMH failure
        id: debug-install-umh-failure
        if: ${{ failure() && steps.install-umh.outcome == 'failure' }}
        run: |
          mkdir -p ./install-umh-step
          cd ./install-umh-step
          kubectl logs deployment/${{ env.UMH }}-factoryinsight-deployment -n ${{ env.UMH }} --all-containers --prefix > deployment-factoryinsight.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-iotsensorsmqtt -n ${{ env.UMH }} --all-containers --prefix > deployment-iotsensormqtt.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-grafana -n ${{ env.UMH }} --all-containers --prefix > deployment-grafana.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-mqttkafkabridge -n ${{ env.UMH }} --all-containers --prefix > deployment-mqttkafkabridge.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-console -n ${{ env.UMH }} --all-containers --prefix > deployment-console.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-kafkatopostgresql -n ${{ env.UMH }} --all-containers --prefix > deployment-kafkatopostgresql.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-hivemqce -n ${{ env.UMH }} --all-containers --prefix > statefulset-hivemqce.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-kafka -n ${{ env.UMH }} --all-containers --prefix > statefulset-kafka.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-nodered -n ${{ env.UMH }} --all-containers --prefix > statefulset-nodered.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-redis-node -n ${{ env.UMH }} --all-containers --prefix > statefulset-redis-node.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-timescaledb -n ${{ env.UMH }} --all-containers --prefix > statefulset-timescaledb.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-zookeeper -n ${{ env.UMH }} --all-containers --prefix > statefulset-zookeeper.log 2> /dev/null
          helm status ${{ env.UMH }} -n ${{ env.UMH }} > helm-status.log 2> /dev/null
          kubectl get po,svc -n ${{ env.UMH }} > po-svc.log 2> /dev/null
          kubectl describe po -n ${{ env.UMH }} > po-describe.log 2> /dev/null
          kubectl describe svc -n ${{ env.UMH }} > svc-describe.log 2> /dev/null
          kubectl get events -n ${{ env.UMH }} > events.log 2> /dev/null
          cd ..
      - name: Build test image
        id: build-test-image
        run: |
          docker build -t data-flow-test -f ./test/data-flow-test/Dockerfile ./test/data-flow-test
          k3d image import data-flow-test -c umh-tiny
      - name: Run Data Flow Test Job
        id: run-data-flow-test
        run: |
          kubectl apply -f ./test/data-flow-test/data-flow-test-job.yaml -n ${{ env.UMH }}
          kubectl wait --for=condition=complete job/data-flow-test -n ${{ env.UMH }} --timeout=2m
          echo "Data Flow Test Job succeeded"
          kubectl logs job/data-flow-test -n ${{ env.UMH }} || true
      - name: Debug Data Flow failure
        id: debug-data-flow-failure
        if: ${{ failure() && steps.run-data-flow-test.outcome == 'failure'  }}
        run: |
          mkdir -p ./run-data-flow-test-step
          cd ./run-data-flow-test-step
          kubectl logs job/data-flow-test -n ${{ env.UMH }} > job-data-flow-test.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-factoryinsight-deployment -n ${{ env.UMH }} --all-containers --prefix > deployment-factoryinsight.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-iotsensorsmqtt -n ${{ env.UMH }} --all-containers --prefix > deployment-iotsensormqtt.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-grafana -n ${{ env.UMH }} --all-containers --prefix > deployment-grafana.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-mqttkafkabridge -n ${{ env.UMH }} --all-containers --prefix > deployment-mqttkafkabridge.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-console -n ${{ env.UMH }} --all-containers --prefix > deployment-console.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-kafkatopostgresql -n ${{ env.UMH }} --all-containers --prefix > deployment-kafkatopostgresql.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-hivemqce -n ${{ env.UMH }} --all-containers --prefix > statefulset-hivemqce.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-kafka -n ${{ env.UMH }} --all-containers --prefix > statefulset-kafka.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-nodered -n ${{ env.UMH }} --all-containers --prefix > statefulset-nodered.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-redis-node -n ${{ env.UMH }} --all-containers --prefix > statefulset-redis-node.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-timescaledb -n ${{ env.UMH }} --all-containers --prefix > statefulset-timescaledb.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-zookeeper -n ${{ env.UMH }} --all-containers --prefix > statefulset-zookeeper.log 2> /dev/null
          kubectl get po,svc -n ${{ env.UMH }} > po-svc.log 2> /dev/null
          kubectl describe po -n ${{ env.UMH }} > po-describe.log 2> /dev/null
          kubectl describe svc -n ${{ env.UMH }} > svc-describe.log 2> /dev/null
          kubectl get events -n ${{ env.UMH }} > events.log 2> /dev/null
          cd ..
      - name: Upload failed test logs
        id: upload-failed-test-logs
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: data-flow-test-logs
          path: |
            ./install-umh-step
            ./run-data-flow-test-step
          if-no-files-found: ignore
      - name: Delete k3d cluster
        id: delete-k3d
        if: always()
        run: |
          k3d cluster delete umh-tiny

  upgrade-test:
    name: Upgrade Test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - name: Prepare k3d cluster
        id: prepare-k3d
        uses: AbsaOSS/k3d-action@v2
        with:
          cluster-name: "umh-tiny"
          k3d-version: ${{ env.K3D_VERSION }}
          args: --agents 1 --image rancher/k3s:${{ env.K3S_VERSION }}
      - name: Install UMH latest
        id: install-umh-latest
        run: |
          helm repo add united-manufacturing-hub https://repo.umh.app/
          helm repo update
          helm install ${{ env.UMH }} united-manufacturing-hub/united-manufacturing-hub -f ${{ env.VALUES_FILE }} --namespace ${{ env.UMH }} --create-namespace --wait --timeout ${{ env.HELM_TIMEOUT }} --debug
      - name: Debug Install UMH failure
        id: debug-install-umh-failure
        if: ${{ failure() && steps.install-umh-latest.outcome == 'failure' }}
        run: |
          mkdir -p ./install-umh-latest-step
          cd ./install-umh-latest-step
          kubectl logs deployment/${{ env.UMH }}-factoryinsight-deployment -n ${{ env.UMH }} --all-containers --prefix > deployment-factoryinsight.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-iotsensorsmqtt -n ${{ env.UMH }} --all-containers --prefix > deployment-iotsensormqtt.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-grafana -n ${{ env.UMH }} --all-containers --prefix > deployment-grafana.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-mqttkafkabridge -n ${{ env.UMH }} --all-containers --prefix > deployment-mqttkafkabridge.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-console -n ${{ env.UMH }} --all-containers --prefix > deployment-console.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-kafkatopostgresql -n ${{ env.UMH }} --all-containers --prefix > deployment-kafkatopostgresql.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-hivemqce -n ${{ env.UMH }} --all-containers --prefix > statefulset-hivemqce.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-kafka -n ${{ env.UMH }} --all-containers --prefix > statefulset-kafka.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-nodered -n ${{ env.UMH }} --all-containers --prefix > statefulset-nodered.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-redis-node -n ${{ env.UMH }} --all-containers --prefix > statefulset-redis-node.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-timescaledb -n ${{ env.UMH }} --all-containers --prefix > statefulset-timescaledb.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-zookeeper -n ${{ env.UMH }} --all-containers --prefix > statefulset-zookeeper.log 2> /dev/null
          helm status ${{ env.UMH }} -n ${{ env.UMH }} > helm-status.log 2> /dev/null
          kubectl get po,svc -n ${{ env.UMH }} > po-svc.log 2> /dev/null
          kubectl describe po -n ${{ env.UMH }} > po-describe.log 2> /dev/null
          kubectl describe svc -n ${{ env.UMH }} > svc-describe.log 2> /dev/null
          kubectl get events -n ${{ env.UMH }} > events.log 2> /dev/null
          cd ..
      - name: Upgrade UMH
        id: upgrade-umh
        run: |
          kubectl delete deployment ${{ env.UMH }}-factoryinsight-deployment -n ${{ env.UMH }} || true
          kubectl delete deployment ${{ env.UMH }}-iotsensorsmqtt -n ${{ env.UMH }} || true
          kubectl delete deployment ${{ env.UMH }}-kafkatopostgresql -n ${{ env.UMH }} || true
          kubectl delete deployment ${{ env.UMH }}-mqttkafkabridge -n ${{ env.UMH }} || true
          kubectl delete statefulset ${{ env.UMH }}-nodered -n ${{ env.UMH }} || true
          helm upgrade ${{ env.UMH }} ${{ env.CHART_DIR }} -f ${{ env.VALUES_FILE }} --namespace ${{ env.UMH }} --wait --timeout ${{ env.HELM_TIMEOUT }} --debug
          kubectl get po,svc -n ${{ env.UMH }}
      - name: Debug Upgrade UMH failure
        id: debug-upgrade-umh-failure
        if: ${{ failure() && steps.upgrade-umh.outcome == 'failure' }}
        run: |
          mkdir -p ./upgrade-umh-step
          cd ./upgrade-umh-step
          kubectl logs deployment/${{ env.UMH }}-factoryinsight-deployment -n ${{ env.UMH }} --all-containers --prefix > deployment-factoryinsight.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-iotsensorsmqtt -n ${{ env.UMH }} --all-containers --prefix > deployment-iotsensormqtt.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-grafana -n ${{ env.UMH }} --all-containers --prefix > deployment-grafana.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-mqttkafkabridge -n ${{ env.UMH }} --all-containers --prefix > deployment-mqttkafkabridge.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-console -n ${{ env.UMH }} --all-containers --prefix > deployment-console.log 2> /dev/null
          kubectl logs deployment/${{ env.UMH }}-kafkatopostgresql -n ${{ env.UMH }} --all-containers --prefix > deployment-kafkatopostgresql.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-hivemqce -n ${{ env.UMH }} --all-containers --prefix > statefulset-hivemqce.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-kafka -n ${{ env.UMH }} --all-containers --prefix > statefulset-kafka.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-nodered -n ${{ env.UMH }} --all-containers --prefix > statefulset-nodered.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-redis-node -n ${{ env.UMH }} --all-containers --prefix > statefulset-redis-node.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-timescaledb -n ${{ env.UMH }} --all-containers --prefix > statefulset-timescaledb.log 2> /dev/null
          kubectl logs statefulset/${{ env.UMH }}-zookeeper -n ${{ env.UMH }} --all-containers --prefix > statefulset-zookeeper.log 2> /dev/null
          helm status ${{ env.UMH }} -n ${{ env.UMH }} > helm-status.log 2> /dev/null
          kubectl get po,svc -n ${{ env.UMH }} > po-svc.log 2> /dev/null
          kubectl describe po -n ${{ env.UMH }} > po-describe.log 2> /dev/null
          kubectl describe svc -n ${{ env.UMH }} > svc-describe.log 2> /dev/null
          kubectl get events -n ${{ env.UMH }} > events.log 2> /dev/null
          cd ..
      - name: Build test image
        id: build-test-image
        run: |
          docker build -t data-flow-test -f ./test/data-flow-test/Dockerfile ./test/data-flow-test
          k3d image import data-flow-test -c umh-tiny
      - name: Run Data Flow Test Job
        id: run-data-flow-test
        run: |
          kubectl apply -f ./test/data-flow-test/data-flow-test-job.yaml -n ${{ env.UMH }}
          kubectl wait --for=condition=complete job/data-flow-test -n ${{ env.UMH }} --timeout=2m
          echo "Data Flow Test Job succeeded"
          kubectl logs job/data-flow-test -n ${{ env.UMH }} || true
      - name: Debug Data Flow failure
        id: debug-data-flow-failure
        if: ${{ failure() && steps.run-data-flow-test.outcome == 'failure' }}
        run: |
          mkdir -p ./run-data-flow-test-step
          cd ./run-data-flow-test-step
          kubectl logs job/data-flow-test -n ${{ env.UMH }} > jobs-data-flow-test.log
          kubectl logs deployment/${{ env.UMH }}-factoryinsight-deployment -n ${{ env.UMH }} --all-containers --prefix > deployment-factoryinsight.log
          kubectl logs deployment/${{ env.UMH }}-iotsensorsmqtt -n ${{ env.UMH }} --all-containers --prefix > deployment-iotsensormqtt.log
          kubectl logs deployment/${{ env.UMH }}-grafana -n ${{ env.UMH }} --all-containers --prefix > deployment-grafana.log
          kubectl logs deployment/${{ env.UMH }}-mqttkafkabridge -n ${{ env.UMH }} --all-containers --prefix > deployment-mqttkafkabridge.log
          kubectl logs deployment/${{ env.UMH }}-console -n ${{ env.UMH }} --all-containers --prefix > deployment-console.log
          kubectl logs deployment/${{ env.UMH }}-kafkatopostgresql -n ${{ env.UMH }} --all-containers --prefix > deployment-kafkatopostgresql.log
          kubectl logs statefulset/${{ env.UMH }}-hivemqce -n ${{ env.UMH }} --all-containers --prefix > statefulset-hivemqce.log
          kubectl logs statefulset/${{ env.UMH }}-kafka -n ${{ env.UMH }} --all-containers --prefix > statefulset-kafka.log
          kubectl logs statefulset/${{ env.UMH }}-nodered -n ${{ env.UMH }} --all-containers --prefix > statefulset-nodered.log
          kubectl logs statefulset/${{ env.UMH }}-redis-node -n ${{ env.UMH }} --all-containers --prefix > statefulset-redis-node.log
          kubectl logs statefulset/${{ env.UMH }}-timescaledb -n ${{ env.UMH }} --all-containers --prefix > statefulset-timescaledb.log
          kubectl logs statefulset/${{ env.UMH }}-zookeeper -n ${{ env.UMH }} --all-containers --prefix > statefulset-zookeeper.log
          kubectl get po,svc -n ${{ env.UMH }} > po-svc.log
          kubectl describe po -n ${{ env.UMH }} > po-describe.log
          kubectl describe svc -n ${{ env.UMH }} > svc-describe.log
          kubectl get events -n ${{ env.UMH }} > events.log
          cd ..
      - name: Upload failed test logs
        id: upload-failed-test-logs
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: upgrade-test-logs
          path: |
            ./install-umh-latest-step
            ./upgrade-umh-step
            ./run-data-flow-test-step
          if-no-files-found: ignore
      - name: Delete k3d cluster
        id: delete-k3d
        if: always()
        run: |
          k3d cluster delete umh-tiny
