## Input
#
# This configuration bridges _historian messages from MQTT to Kafka
# According to our _historian schema, these can occur in the following topics:
# umh/v1/+/_historian/# (enterprise)
# umh/v1/+/+/_historian/# (enterprise, site)
# umh/v1/+/+/+/_historian/# (enterprise, site, area)
# umh/v1/+/+/+/+/_historian/# (enterprise, site, area, productionLine)
# umh/v1/+/+/+/+/+/_historian/# (enterprise, site, area, productionLine, workCell)
# umh/v1/+/+/+/+/+/+/_historian/# (enterprise, site, area, productionLine, workCell, originID)
# We will use a HiveMQ shared subscription ($share/<GROUPID>/TOPIC) to allow scaling the number of consumers

input:
  mqtt:
    urls:
      - united-manufacturing-hub-mqtt.united-manufacturing-hub.svc.cluster.local:1883
    client_id: "benthos_mqtt_to_kafka"
    dynamic_client_id_suffix: "nanoid"
    topics:
      - $share/benthos_mqtt_to_kafka/umh/v1/+/_historian/#
      - $share/benthos_mqtt_to_kafka/umh/v1/+/+/_historian/#
      - $share/benthos_mqtt_to_kafka/umh/v1/+/+/+/_historian/#
      - $share/benthos_mqtt_to_kafka/umh/v1/+/+/+/+/_historian/#
      - $share/benthos_mqtt_to_kafka/umh/v1/+/+/+/+/+/_historian/#
      - $share/benthos_mqtt_to_kafka/umh/v1/+/+/+/+/+/+/_historian/#

## Processors
#
# This part does the following:
# 0. We check if a bridged-by header already exists, if not we create it.
# 1. We check if the message was either bridged by us, or the reverse of us (benthos-kafka-to-mqtt-<INSTANCE_NAME>), in any of these cases we discard the message.
# 2. We convert the MQTT topic name into a Kafka topic name (replace / with .)
# 3. We slice the topic name into a topic and a key, by splitting it at the 5th dot (so after umh.v1.enterprise.site.area) [This is the same behaviour as the old databridge]
# 4. Check if the message is a valid JSON object
# 4a. If the message is not a valid JSON, we still bridge it, but we set a kafka header (parsing-failure="invalid-json").
# 4b. If the message is a valid JSON, we bridge it as is.
# 5. In both cases we add a new header to indicate that it was bridged by this benthos instance.
#    This key also needs to include our instance name, so we can bridge on multiple devices (We get that from Helm).
#    The key should be an json array of bridges the data was passed through.
#    bridged-by: ["benthos-mqtt-to-kafka-<INSTANCE_NAME>"]
# 6. We can finally bridge the message to Kafka

# For debugging reason our instance name is achlys

pipeline:
  processors:
    - bloblang: |
        # Step 2
        let topic = meta("mqtt_topic")
        let converted_topic = $topic.replace("/", ".")
        ## Trim leading or trailing dots
        let trimmed_topic = $converted_topic.trim(".")

        # Step 3
        let topic_parts = $trimmed_topic.split(".")
        let kafka_topic = $topic_parts.slice(0, 5).join(".")
        let kafka_key = $topic_parts.slice(5).join(".")

        meta "kafka_topic" = $kafka_topic
        meta "kafka_key" = $kafka_key

        let invalid_json = this.parse_json().catch(this).format_json().catch(false)

        ## If not valid json build a new payload
        let encoded = this.format_json().catch(this).encode("base64")

        root = if $invalid_json == true {
          {
            "parsing_failure": "invalid-json",
            "payload": $encoded
          }
        } else {
          root
        }


        # Step 0
        ## Check if bridged-by header exists, if not create it
        let bridged_by = match {
          this.bridged_by => this.bridged_by
          _ => []
        }

        # Step 1
        ## Check if the message was bridged by us, or the reverse of us
        root = if $bridged_by.contains("benthos-kafka-to-mqtt-achlys") || $bridged_by.contains("benthos-mqtt-to-kafka-achlys") {
          deleted()
        } else{
          root
        }

        # Step 4 (a/b)
        let valid_schema = root.json_schema("""{
          "$schema": "http://json-schema.org/draft-07/schema#",
          "type": "object",
          "required": ["timestamp_ms"],
          "properties": {
            "timestamp_ms": {
              "type": "integer"
            }
          },
          "minProperties": 2,
          "additionalProperties": true
        }
        """).catch(false)

        let parsing_failure = match {
          $valid_schema == false => true
          _ => false
        }

        meta "parsing_failure" = $parsing_failure


        # Step 5

        ## Append our bridge to the bridged-by header
        let bridged_by = $bridged_by.append("benthos-mqtt-to-kafka-achlys")

        ### Transform bridged_by to meta as we are transfering to kafka
        meta "bridged_by" = $bridged_by

output:
  broker:
    pattern: fan_out
    outputs:
      - kafka_franz:
          topic: ${! meta("kafka_topic") }
          seed_brokers:
            - united-manufacturing-hub-kafka.united-manufacturing-hub.svc.cluster.local:9092
          client_id: mqtt_to_kafka_historian_bridge_achlys
          key: ${! meta("kafka_key") }
          metadata:
            include_prefixes:
              - kafka_
              - bridged_
              - parsing_
      - stdout:
          codec: lines